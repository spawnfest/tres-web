ified length.')\n  }\n\n  const hash = varint.encode(hashfn)\n  const len = varint.encode(length)\n  return uint8ArrayConcat([hash, len, digest], hash.length + len.length + digest.length)\n}\n\n/**\n * Converts a hash function name into the matching code.\n * If passed a number it will return the number if it's a valid code.\n *\n * @param {HashName | number} name\n * @returns {number}\n */\nfunction coerceCode (name) {\n  let code = name\n\n  if (typeof name === 'string') {\n    if (names[name] === undefined) {\n      throw new Error(`Unrecognized hash function named: ${name}`)\n    }\n    code = names[name]\n  }\n\n  if (typeof code !== 'number') {\n    throw new Error(`Hash function code should be a number. Got: ${code}`)\n  }\n\n  // @ts-ignore\n  if (codes[code] === undefined && !isAppCode(code)) {\n    throw new Error(`Unrecognized function code: ${code}`)\n  }\n\n  return code\n}\n\n/**\n * Checks if a code is part of the app range\n *\n * @param {number} code\n * @returns {boolean}\n */\nfunction isAppCode (code) {\n  return code > 0 && code < 0x10\n}\n\n/**\n * Checks whether a multihash code is valid.\n *\n * @param {HashCode} code\n * @returns {boolean}\n */\nfunction isValidCode (code) {\n  if (isAppCode(code)) {\n    return true\n  }\n\n  if (codes[code]) {\n    return true\n  }\n\n  return false\n}\n\n/**\n * Check if the given buffer is a valid multihash. Throws an error if it is not valid.\n *\n * @param {Uint8Array} multihash\n * @returns {void}\n * @throws {Error}\n */\nfunction validate (multihash) {\n  decode(multihash) // throws if bad.\n}\n\n/**\n * Returns a prefix from a valid multihash. Throws an error if it is not valid.\n *\n * @param {Uint8Array} multihash\n * @returns {Uint8Array}\n * @throws {Error}\n */\nfunction prefix (multihash) {\n  validate(multihash)\n\n  return multihash.subarray(0, 2)\n}\n\nmodule.exports = {\n  names,\n  codes,\n  toHexString,\n  fromHexString,\n  toB58String,\n  fromB58String,\n  decode,\n  encode,\n  coerceCode,\n  isAppCode,\n  validate,\n  prefix,\n  isValidCode\n}\n\n/**\n * @typedef { import(\"./constants\").HashCode } HashCode\n * @typedef { import(\"./constants\").HashName } HashName\n */\n","'use strict'\n\n/**\n * Returns a new Uint8Array created by concatenating the passed ArrayLikes\n *\n * @param {Array<ArrayLike<number>>} arrays\n * @param {number} [length]\n */\nfunction concat (arrays, length) {\n  if (!length) {\n    length = arrays.reduce((acc, curr) => acc + curr.length, 0)\n  }\n\n  const output = new Uint8Array(length)\n  let offset = 0\n\n  for (const arr of arrays) {\n    output.set(arr, offset)\n    offset += arr.length\n  }\n\n  return output\n}\n\nmodule.exports = concat\n","'use strict'\n\nconst varint = require('varint')\nconst uint8ArrayToString = require('uint8arrays/to-string')\nconst uint8ArrayFromString = require('uint8arrays/from-string')\n\nmodule.exports = {\n  numberToUint8Array,\n  uint8ArrayToNumber,\n  varintUint8ArrayEncode,\n  varintEncode\n}\n\n/**\n * @param {Uint8Array} buf\n */\nfunction uint8ArrayToNumber (buf) {\n  return parseInt(uint8ArrayToString(buf, 'base16'), 16)\n}\n\n/**\n * @param {number} num\n */\nfunction numberToUint8Array (num) {\n  let hexString = num.toString(16)\n  if (hexString.length % 2 === 1) {\n    hexString = '0' + hexString\n  }\n  return uint8ArrayFromString(hexString, 'base16')\n}\n\n/**\n * @param {Uint8Array} input\n */\nfunction varintUint8ArrayEncode (input) {\n  return Uint8Array.from(varint.encode(uint8ArrayToNumber(input)))\n}\n\n/**\n * @param {number} num\n */\nfunction varintEncode (num) {\n  return Uint8Array.from(varint.encode(num))\n}\n","'use strict'\nconst EthAccount = require('ethereumjs-account').default\nconst multicodec = require('multicodec')\nconst { Buffer } = require('buffer')\n\nconst cidFromHash = require('../util/cidFromHash')\nconst createResolver = require('../util/createResolver')\nconst emptyCodeHash = require('../util/emptyCodeHash')\n\nconst deserialize = (serialized) => {\n  const ethObj = new EthAccount(serialized)\n\n  const deserialized = {\n    balance: ethObj.balance,\n    code: emptyCodeHash.equals(ethObj.codeHash)\n      ? Buffer.alloc(0)\n      : cidFromHash(multicodec.RAW, ethObj.codeHash),\n    codeHash: ethObj.codeHash,\n    isEmpty: ethObj.isEmpty(),\n    isContract: ethObj.isContract(),\n    nonce: ethObj.nonce,\n    stateRoot: ethObj.stateRoot,\n    storage: cidFromHash(multicodec.ETH_STORAGE_TRIE, ethObj.stateRoot),\n    _ethObj: ethObj\n  }\n\n  Object.defineProperty(deserialized, '_ethObj', { enumerable: false })\n\n  return deserialized\n}\n\nmodule.exports = createResolver(multicodec.ETH_ACCOUNT_SNAPSHOT, deserialize)\n","module.exports = require('./lib')(require('./lib/elliptic'))\n","'use strict';\n\nvar utils = exports;\n\nfunction toArray(msg, enc) {\n  if (Array.isArray(msg))\n    return msg.slice();\n  if (!msg)\n    return [];\n  var res = [];\n  if (typeof msg !== 'string') {\n    for (var i = 0; i < msg.length; i++)\n      res[i] = msg[i] | 0;\n    return res;\n  }\n  if (enc === 'hex') {\n    msg = msg.replace(/[^a-z0-9]+/ig, '');\n    if (msg.length % 2 !== 0)\n      msg = '0' + msg;\n    for (var i = 0; i < msg.length; i += 2)\n      res.push(parseInt(msg[i] + msg[i + 1], 16));\n  } else {\n    for (var i = 0; i < msg.length; i++) {\n      var c = msg.charCodeAt(i);\n      var hi = c >> 8;\n      var lo = c & 0xff;\n      if (hi)\n        res.push(hi, lo);\n      else\n        res.push(lo);\n    }\n  }\n  return res;\n}\nutils.toArray = toArray;\n\nfunction zero2(word) {\n  if (word.length === 1)\n    return '0' + word;\n  else\n    return word;\n}\nutils.zero2 = zero2;\n\nfunction toHex(msg) {\n  var res = '';\n  for (var i = 0; i < msg.length; i++)\n    res += zero2(msg[i].toString(16));\n  return res;\n}\nutils.toHex = toHex;\n\nutils.encode = function encode(arr, enc) {\n  if (enc === 'hex')\n    return toHex(arr);\n  else\n    return arr;\n};\n","var r;\n\nmodule.exports = function rand(len) {\n  if (!r)\n    r = new Rand(null);\n\n  return r.generate(len);\n};\n\nfunction Rand(rand) {\n  this.rand = rand;\n}\nmodule.exports.Rand = Rand;\n\nRand.prototype.generate = function generate(len) {\n  return this._rand(len);\n};\n\n// Emulate crypto API using randy\nRand.prototype._rand = function _rand(n) {\n  if (this.rand.getBytes)\n    return this.rand.getBytes(n);\n\n  var res = new Uint8Array(n);\n  for (var i = 0; i < res.length; i++)\n    res[i] = this.rand.getByte();\n  return res;\n};\n\nif (typeof self === 'object') {\n  if (self.crypto && self.crypto.getRandomValues) {\n    // Modern browsers\n    Rand.prototype._rand = function _rand(n) {\n      var arr = new Uint8Array(n);\n      self.crypto.getRandomValues(arr);\n      return arr;\n    };\n  } else if (self.msCrypto && self.msCrypto.getRandomValues) {\n    // IE\n    Rand.prototype._rand = function _rand(n) {\n      var arr = new Uint8Array(n);\n      self.msCrypto.getRandomValues(arr);\n      return arr;\n    };\n\n  // Safari's WebWorkers do not have `crypto`\n  } else if (typeof window === 'object') {\n    // Old junk\n    Rand.prototype._rand = function() {\n      throw new Error('Not implemented yet');\n    };\n  }\n} else {\n  // Node.js or Web worker with no crypto support\n  try {\n    var crypto = require('crypto');\n    if (typeof crypto.randomBytes !== 'function')\n      throw new Error('Not supported');\n\n    Rand.prototype._rand = function _rand(n) {\n      return crypto.randomBytes(n);\n    };\n  } catch (e) {\n  }\n}\n","'use strict';\n\nvar curve = exports;\n\ncurve.base = require('./base');\ncurve.short = require('./short');\ncurve.mont = require('./mont');\ncurve.edwards = require('./edwards');\n","'use strict';\n\nvar utils = require('../utils');\nvar rotr32 = utils.rotr32;\n\nfunction ft_1(s, x, y, z) {\n  if (s === 0)\n    return ch32(x, y, z);\n  if (s === 1 || s === 3)\n    return p32(x, y, z);\n  if (s === 2)\n    return maj32(x, y, z);\n}\nexports.ft_1 = ft_1;\n\nfunction ch32(x, y, z) {\n  return (x & y) ^ ((~x) & z);\n}\nexports.ch32 = ch32;\n\nfunction maj32(x, y, z) {\n  return (x & y) ^ (x & z) ^ (y & z);\n}\nexports.maj32 = maj32;\n\nfunction p32(x, y, z) {\n  return x ^ y ^ z;\n}\nexports.p32 = p32;\n\nfunction s0_256(x) {\n  return rotr32(x, 2) ^ rotr32(x, 13) ^ rotr32(x, 22);\n}\nexports.s0_256 = s0_256;\n\nfunction s1_256(x) {\n  return rotr32(x, 6) ^ rotr32(x, 11) ^ rotr32(x, 25);\n}\nexports.s1_256 = s1_256;\n\nfunction g0_256(x) {\n  return rotr32(x, 7) ^ rotr32(x, 18) ^ (x >>> 3);\n}\nexports.g0_256 = g0_256;\n\nfunction g1_256(x) {\n  return rotr32(x, 17) ^ rotr32(x, 19) ^ (x >>> 10);\n}\nexports.g1_256 = g1_256;\n","'use strict';\n\nvar utils = require('../utils');\nvar common = require('../common');\nvar shaCommon = require('./common');\nvar assert = require('minimalistic-assert');\n\nvar sum32 = utils.sum32;\nvar sum32_4 = utils.sum32_4;\nvar sum32_5 = utils.sum32_5;\nvar ch32 = shaCommon.ch32;\nvar maj32 = shaCommon.maj32;\nvar s0_256 = shaCommon.s0_256;\nvar s1_256 = shaCommon.s1_256;\nvar g0_256 = shaCommon.g0_256;\nvar g1_256 = shaCommon.g1_256;\n\nvar BlockHash = common.BlockHash;\n\nvar sha256_K = [\n  0x428a2f98, 0x71374491, 0xb5c0fbcf, 0xe9b5dba5,\n  0x3956c25b, 0x59f111f1, 0x923f82a4, 0xab1c5ed5,\n  0xd807aa98, 0x12835b01, 0x243185be, 0x550c7dc3,\n  0x72be5d74, 0x80deb1fe, 0x9bdc06a7, 0xc19bf174,\n  0xe49b69c1, 0xefbe4786, 0x0fc19dc6, 0x240ca1cc,\n  0x2de92c6f, 0x4a7484aa, 0x5cb0a9dc, 0x76f988da,\n  0x983e5152, 0xa831c66d, 0xb00327c8, 0xbf597fc7,\n  0xc6e00bf3, 0xd5a79147, 0x06ca6351, 0x14292967,\n  0x27b70a85, 0x2e1b2138, 0x4d2c6dfc, 0x53380d13,\n  0x650a7354, 0x766a0abb, 0x81c2c92e, 0x92722c85,\n  0xa2bfe8a1, 0xa81a664b, 0xc24b8b70, 0xc76c51a3,\n  0xd192e819, 0xd6990624, 0xf40e3585, 0x106aa070,\n  0x19a4c116, 0x1e376c08, 0x2748774c, 0x34b0bcb5,\n  0x391c0cb3, 0x4ed8aa4a, 0x5b9cca4f, 0x682e6ff3,\n  0x748f82ee, 0x78a5636f, 0x84c87814, 0x8cc70208,\n  0x90befffa, 0xa4506ceb, 0xbef9a3f7, 0xc67178f2\n];\n\nfunction SHA256() {\n  if (!(this instanceof SHA256))\n    return new SHA256();\n\n  BlockHash.call(this);\n  this.h = [\n    0x6a09e667, 0xbb67ae85, 0x3c6ef372, 0xa54ff53a,\n    0x510e527f, 0x9b05688c, 0x1f83d9ab, 0x5be0cd19\n  ];\n  this.k = sha256_K;\n  this.W = new Array(64);\n}\nutils.inherits(SHA256, BlockHash);\nmodule.exports = SHA256;\n\nSHA256.blockSize = 512;\nSHA256.outSize = 256;\nSHA256.hmacStrength = 192;\nSHA256.padLength = 64;\n\nSHA256.prototype._update = function _update(msg, start) {\n  var W = this.W;\n\n  for (var i = 0; i < 16; i++)\n    W[i] = msg[start + i];\n  for (; i < W.length; i++)\n    W[i] = sum32_4(g1_256(W[i - 2]), W[i - 7], g0_256(W[i - 15]), W[i - 16]);\n\n  var a = this.h[0];\n  var b = this.h[1];\n  var c = this.h[2];\n  var d = this.h[3];\n  var e = this.h[4];\n  var f = this.h[5];\n  var g = this.h[6];\n  var h = this.h[7];\n\n  assert(this.k.length === W.length);\n  for (i = 0; i < W.length; i++) {\n    var T1 = sum32_5(h, s1_256(e), ch32(e, f, g), this.k[i], W[i]);\n    var T2 = sum32(s0_256(a), maj32(a, b, c));\n    h = g;\n    g = f;\n    f = e;\n    e = sum32(d, T1);\n    d = c;\n    c = b;\n    b = a;\n    a = sum32(T1, T2);\n  }\n\n  this.h[0] = sum32(this.h[0], a);\n  this.h[1] = sum32(this.h[1], b);\n  this.h[2] = sum32(this.h[2], c);\n  this.h[3] = sum32(this.h[3], d);\n  this.h[4] = sum32(this.h[4], e);\n  this.h[5] = sum32(this.h[5], f);\n  this.h[6] = sum32(this.h[6], g);\n  this.h[7] = sum32(this.h[7], h);\n};\n\nSHA256.prototype._digest = function digest(enc) {\n  if (enc === 'hex')\n    return utils.toHex32(this.h, 'big');\n  else\n    return utils.split32(this.h, 'big');\n};\n","'use strict';\n\nvar utils = require('../utils');\nvar common = require('../common');\nvar assert = require('minimalistic-assert');\n\nvar rotr64_hi = utils.rotr64_hi;\nvar rotr64_lo = utils.rotr64_lo;\nvar shr64_hi = utils.shr64_hi;\nvar shr64_lo = utils.shr64_lo;\nvar sum64 = utils.sum64;\nvar sum64_hi = utils.sum64_hi;\nvar sum64_lo = utils.sum64_lo;\nvar sum64_4_hi = utils.sum64_4_hi;\nvar sum64_4_lo = utils.sum64_4_lo;\nvar sum64_5_hi = utils.sum64_5_hi;\nvar sum64_5_lo = utils.sum64_5_lo;\n\nvar BlockHash = common.BlockHash;\n\nvar sha512_K = [\n  0x428a2f98, 0xd728ae22, 0x71374491, 0x23ef65cd,\n  0xb5c0fbcf, 0xec4d3b2f, 0xe9b5dba5, 0x8189dbbc,\n  0x3956c25b, 0xf348b538, 0x59f111f1, 0xb605d019,\n  0x923f82a4, 0xaf194f9b, 0xab1c5ed5, 0xda6d8118,\n  0xd807aa98, 0xa3030242, 0x12835b01, 0x45706fbe,\n  0x243185be, 0x4ee4b28c, 0x550c7dc3, 0xd5ffb4e2,\n  0x72be5d74, 0xf27b896f, 0x80deb1fe, 0x3b1696b1,\n  0x9bdc06a7, 0x25c71235, 0xc19bf174, 0xcf692694,\n  0xe49b69c1, 0x9ef14ad2, 0xefbe4786, 0x384f25e3,\n  0x0fc19dc6, 0x8b8cd5b5, 0x240ca1cc, 0x77ac9c65,\n  0x2de92c6f, 0x592b0275, 0x4a7484aa, 0x6ea6e483,\n  0x5cb0a9dc, 0xbd41fbd4, 0x76f988da, 0x831153b5,\n  0x983e5152, 0xee66dfab, 0xa831c66d, 0x2db43210,\n  0xb00327c8, 0x98fb213f, 0xbf597fc7, 0xbeef0ee4,\n  0xc6e00bf3, 0x3da88fc2, 0xd5a79147, 0x930aa725,\n  0x06ca6351, 0xe003826f, 0x14292967, 0x0a0e6e70,\n  0x27b70a85, 0x46d22ffc, 0x2e1b2138, 0x5c26c926,\n  0x4d2c6dfc, 0x5ac42aed, 0x53380d13, 0x9d95b3df,\n  0x650a7354, 0x8baf63de, 0x766a0abb, 0x3c77b2a8,\n  0x81c2c92e, 0x47edaee6, 0x92722c85, 0x1482353b,\n  0xa2bfe8a1, 0x4cf10364, 0xa81a664b, 0xbc423001,\n  0xc24b8b70, 0xd0f89791, 0xc76c51a3, 0x0654be30,\n  0xd192e819, 0xd6ef5218, 0xd6990624, 0x5565a910,\n  0xf40e3585, 0x5771202a, 0x106aa070, 0x32bbd1b8,\n  0x19a4c116, 0xb8d2d0c8, 0x1e376c08, 0x5141ab53,\n  0x2748774c, 0xdf8eeb99, 0x34b0bcb5, 0xe19b48a8,\n  0x391c0cb3, 0xc5c95a63, 0x4ed8aa4a, 0xe3418acb,\n  0x5b9cca4f, 0x7763e373, 0x682e6ff3, 0xd6b2b8a3,\n  0x748f82ee, 0x5defb2fc, 0x78a5636f, 0x43172f60,\n  0x84c87814, 0xa1f0ab72, 0x8cc70208, 0x1a6439ec,\n  0x90befffa, 0x23631e28, 0xa4506ceb, 0xde82bde9,\n  0xbef9a3f7, 0xb2c67915, 0xc67178f2, 0xe372532b,\n  0xca273ece, 0xea26619c, 0xd186b8c7, 0x21c0c207,\n  0xeada7dd6, 0xcde0eb1e, 0xf57d4f7f, 0xee6ed178,\n  0x06f067aa, 0x72176fba, 0x0a637dc5, 0xa2c898a6,\n  0x113f9804, 0xbef90dae, 0x1b710b35, 0x131c471b,\n  0x28db77f5, 0x23047d84, 0x32caab7b, 0x40c72493,\n  0x3c9ebe0a, 0x15c9bebc, 0x431d67c4, 0x9c100d4c,\n  0x4cc5d4be, 0xcb3e42b6, 0x597f299c, 0xfc657e2a,\n  0x5fcb6fab, 0x3ad6faec, 0x6c44198c, 0x4a475817\n];\n\nfunction SHA512() {\n  if (!(this instanceof SHA512))\n    return new SHA512();\n\n  BlockHash.call(this);\n  this.h = [\n    0x6a09e667, 0xf3bcc908,\n    0xbb67ae85, 0x84caa73b,\n    0x3c6ef372, 0xfe94f82b,\n    0xa54ff53a, 0x5f1d36f1,\n    0x510e527f, 0xade682d1,\n    0x9b05688c, 0x2b3e6c1f,\n    0x1f83d9ab, 0xfb41bd6b,\n    0x5be0cd19, 0x137e2179 ];\n  this.k = sha512_K;\n  this.W = new Array(160);\n}\nutils.inherits(SHA512, BlockHash);\nmodule.exports = SHA512;\n\nSHA512.blockSize = 1024;\nSHA512.outSize = 512;\nSHA512.hmacStrength = 192;\nSHA512.padLength = 128;\n\nSHA512.prototype._prepareBlock = function _prepareBlock(msg, start) {\n  var W = this.W;\n\n  // 32 x 32bit words\n  for (var i = 0; i < 32; i++)\n    W[i] = msg[start + i];\n  for (; i < W.length; i += 2) {\n    var c0_hi = g1_512_hi(W[i - 4], W[i - 3]);  // i - 2\n    var c0_lo = g1_512_lo(W[i - 4], W[i - 3]);\n    var c1_hi = W[i - 14];  // i - 7\n    var c1_lo = W[i - 13];\n    var c2_hi = g0_512_hi(W[i - 30], W[i - 29]);  // i - 15\n    var c2_lo = g0_512_lo(W[i - 30], W[i - 29]);\n    var c3_hi = W[i - 32];  // i - 16\n    var c3_lo = W[i - 31];\n\n    W[i] = sum64_4_hi(\n      c0_hi, c0_lo,\n      c1_hi, c1_lo,\n      c2_hi, c2_lo,\n      c3_hi, c3_lo);\n    W[i + 1] = sum64_4_lo(\n      c0_hi, c0_lo,\n      c1_hi, c1_lo,\n      c2_hi, c2_lo,\n      c3_hi, c3_lo);\n  }\n};\n\nSHA512.prototype._update = function _update(msg, start) {\n  this._prepareBlock(msg, start);\n\n  var W = this.W;\n\n  var ah = this.h[0];\n  var al = this.h[1];\n  var bh = this.h[2];\n  var bl = this.h[3];\n  var ch = this.h[4];\n  var cl = this.h[5];\n  var dh = this.h[6];\n  var dl = this.h[7];\n  var eh = this.h[8];\n  var el = this.h[9];\n  var fh = this.h[10];\n  var fl = this.h[11];\n  var gh = this.h[12];\n  var gl = this.h[13];\n  var hh = this.h[14];\n  var hl = this.h[15];\n\n  assert(this.k.length === W.length);\n  for (var i = 0; i < W.length; i += 2) {\n    var c0_hi = hh;\n    var c0_lo = hl;\n    var c1_hi = s1_512_hi(eh, el);\n    var c1_lo = s1_512_lo(eh, el);\n    var c2_hi = ch64_hi(eh, el, fh, fl, gh, gl);\n    var c2_lo = ch64_lo(eh, el, fh, fl, gh, gl);\n    var c3_hi = this.k[i];\n    var c3_lo = this.k[i + 1];\n    var c4_hi = W[i];\n    var c4_lo = W[i + 1];\n\n    var T1_hi = sum64_5_hi(\n      c0_hi, c0_lo,\n      c1_hi, c1_lo,\n      c2_hi, c2_lo,\n      c3_hi, c3_lo,\n      c4_hi, c4_lo);\n    var T1_lo = sum64_5_lo(\n      c0_hi, c0_lo,\n      c1_hi, c1_lo,\n      c2_hi, c2_lo,\n      c3_hi, c3_lo,\n      c4_hi, c4_lo);\n\n    c0_hi = s0_512_hi(ah, al);\n    c0_lo = s0_512_lo(ah, al);\n    c1_hi = maj64_hi(ah, al, bh, bl, ch, cl);\n    c1_lo = maj64_lo(ah, al, bh, bl, ch, cl);\n\n    var T2_hi = sum64_hi(c0_hi, c0_lo, c1_hi, c1_lo);\n    var T2_lo = sum64_lo(c0_hi, c0_lo, c1_hi, c1_lo);\n\n    hh = gh;\n    hl = gl;\n\n    gh = fh;\n    gl = fl;\n\n    fh = eh;\n    fl = el;\n\n    eh = sum64_hi(dh, dl, T1_hi, T1_lo);\n    el = sum64_lo(dl, dl, T1_hi, T1_lo);\n\n    dh = ch;\n    dl = cl;\n\n    ch = bh;\n    cl = bl;\n\n    bh = ah;\n    bl = al;\n\n    ah = sum64_hi(T1_hi, T1_lo, T2_hi, T2_lo);\n    al = sum64_lo(T1_hi, T1_lo, T2_hi, T2_lo);\n  }\n\n  sum64(this.h, 0, ah, al);\n  sum64(this.h, 2, bh, bl);\n  sum64(this.h, 4, ch, cl);\n  sum64(this.h, 6, dh, dl);\n  sum64(this.h, 8, eh, el);\n  sum64(this.h, 10, fh, fl);\n  sum64(this.h, 12, gh, gl);\n  sum64(this.h, 14, hh, hl);\n};\n\nSHA512.prototype._digest = function digest(enc) {\n  if (enc === 'hex')\n    return utils.toHex32(this.h, 'big');\n  else\n    return utils.split32(this.h, 'big');\n};\n\nfunction ch64_hi(xh, xl, yh, yl, zh) {\n  var r = (xh & yh) ^ ((~xh) & zh);\n  if (r < 0)\n    r += 0x100000000;\n  return r;\n}\n\nfunction ch64_lo(xh, xl, yh, yl, zh, zl) {\n  var r = (xl & yl) ^ ((~xl) & zl);\n  if (r < 0)\n    r += 0x100000000;\n  return r;\n}\n\nfunction maj64_hi(xh, xl, yh, yl, zh) {\n  var r = (xh & yh) ^ (xh & zh) ^ (yh & zh);\n  if (r < 0)\n    r += 0x100000000;\n  return r;\n}\n\nfunction maj64_lo(xh, xl, yh, yl, zh, zl) {\n  var r = (xl & yl) ^ (xl & zl) ^ (yl & zl);\n  if (r < 0)\n    r += 0x100000000;\n  return r;\n}\n\nfunction s0_512_hi(xh, xl) {\n  var c0_hi = rotr64_hi(xh, xl, 28);\n  var c1_hi = rotr64_hi(xl, xh, 2);  // 34\n  var c2_hi = rotr64_hi(xl, xh, 7);  // 39\n\n  var r = c0_hi ^ c1_hi ^ c2_hi;\n  if (r < 0)\n    r += 0x100000000;\n  return r;\n}\n\nfunction s0_512_lo(xh, xl) {\n  var c0_lo = rotr64_lo(xh, xl, 28);\n  var c1_lo = rotr64_lo(xl, xh, 2);  // 34\n  var c2_lo = rotr64_lo(xl, xh, 7);  // 39\n\n  var r = c0_lo ^ c1_lo ^ c2_lo;\n  if (r < 0)\n    r += 0x100000000;\n  return r;\n}\n\nfunction s1_512_hi(xh, xl) {\n  var c0_hi = rotr64_hi(xh, xl, 14);\n  var c1_hi = rotr64_hi(xh, xl, 18);\n  var c2_hi = rotr64_hi(xl, xh, 9);  // 41\n\n  var r = c0_hi ^ c1_hi ^ c2_hi;\n  if (r < 0)\n    r += 0x100000000;\n  return r;\n}\n\nfunction s1_512_lo(xh, xl) {\n  var c0_lo = rotr64_lo(xh, xl, 14);\n  var c1_lo = rotr64_lo(xh, xl, 18);\n  var c2_lo = rotr64_lo(xl, xh, 9);  // 41\n\n  var r = c0_lo ^ c1_lo ^ c2_lo;\n  if (r < 0)\n    r += 0x100000000;\n  return r;\n}\n\nfunction g0_512_hi(xh, xl) {\n  var c0_hi = rotr64_hi(xh, xl, 1);\n  var c1_hi = rotr64_hi(xh, xl, 8);\n  var c2_hi = shr64_hi(xh, xl, 7);\n\n  var r = c0_hi ^ c1_hi ^ c2_hi;\n  if (r < 0)\n    r += 0x100000000;\n  return r;\n}\n\nfunction g0_512_lo(xh, xl) {\n  var c0_lo = rotr64_lo(xh, xl, 1);\n  var c1_lo = rotr64_lo(xh, xl, 8);\n  var c2_lo = shr64_lo(xh, xl, 7);\n\n  var r = c0_lo ^ c1_lo ^ c2_lo;\n  if (r < 0)\n    r += 0x100000000;\n  return r;\n}\n\nfunction g1_512_hi(xh, xl) {\n  var c0_hi = rotr64_hi(xh, xl, 19);\n  var c1_hi = rotr64_hi(xl, xh, 29);  // 61\n  var c2_hi = shr64_hi(xh, xl, 6);\n\n  var r = c0_hi ^ c1_hi ^ c2_hi;\n  if (r < 0)\n    r += 0x100000000;\n  return r;\n}\n\nfunction g1_512_lo(xh, xl) {\n  var c0_lo = rotr64_lo(xh, xl, 19);\n  var c1_lo = rotr64_lo(xl, xh, 29);  // 61\n  var c2_lo = shr64_lo(xh, xl, 6);\n\n  var r = c0_lo ^ c1_lo ^ c2_lo;\n  if (r < 0)\n    r += 0x100000000;\n  return r;\n}\n","/**\n * Returns a `Boolean` on whether or not the a `String` starts with '0x'\n * @param {String} str the string input value\n * @return {Boolean} a boolean if it is or is not hex prefixed\n * @throws if the str input is not a string\n */\nmodule.exports = function isHexPrefixed(str) {\n  if (typeof str !== 'string') {\n    throw new Error(\"[is-hex-prefixed] value must be type 'string', is currently type \" + (typeof str) + \", while checking isHexPrefixed.\");\n  }\n\n  return str.slice(0, 2) === '0x';\n}\n","// Copyright Joyent, Inc. and other Node contributors.\n//\n// Permission is hereby granted, free of charge, to any person obtaining a\n// copy of this software and associated documentation files (the\n// \"Software\"), to deal in the Software without restriction, including\n// without limitation the rights to use, copy, modify, merge, publish,\n// distribute, sublicense, and/or sell copies of the Software, and to permit\n// persons to whom the Software is furnished to do so, subject to the\n// following conditions:\n//\n// The above copyright notice and this permission notice shall be included\n// in all copies or substantial portions of the Software.\n//\n// THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS\n// OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n// MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN\n// NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n// DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR\n// OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE\n// USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n'use strict';\n\n/*<replacement>*/\n\nvar pna = require('process-nextick-args');\n/*</replacement>*/\n\nmodule.exports = Readable;\n\n/*<replacement>*/\nvar isArray = require('isarray');\n/*</replacement>*/\n\n/*<replacement>*/\nvar Duplex;\n/*</replacement>*/\n\nReadable.ReadableState = ReadableState;\n\n/*<replacement>*/\nvar EE = require('events').EventEmitter;\n\nvar EElistenerCount = function (emitter, type) {\n  return emitter.listeners(type).length;\n};\n/*</replacement>*/\n\n/*<replacement>*/\nvar Stream = require('./internal/streams/stream');\n/*</replacement>*/\n\n/*<replacement>*/\n\nvar Buffer = require('safe-buffer').Buffer;\nvar OurUint8Array = global.Uint8Array || function () {};\nfunction _uint8ArrayToBuffer(chunk) {\n  return Buffer.from(chunk);\n}\nfunction _isUint8Array(obj) {\n  return Buffer.isBuffer(obj) || obj instanceof OurUint8Array;\n}\n\n/*</replacement>*/\n\n/*<replacement>*/\nvar util = Object.create(require('core-util-is'));\nutil.inherits = require('inherits');\n/*</replacement>*/\n\n/*<replacement>*/\nvar debugUtil = require('util');\nvar debug = void 0;\nif (debugUtil && debugUtil.debuglog) {\n  debug = debugUtil.debuglog('stream');\n} else {\n  debug = function () {};\n}\n/*</replacement>*/\n\nvar BufferList = require('./internal/streams/BufferList');\nvar destroyImpl = require('./internal/streams/destroy');\nvar StringDecoder;\n\nutil.inherits(Readable, Stream);\n\nvar kProxyEvents = ['error', 'close', 'destroy', 'pause', 'resume'];\n\nfunction prependListener(emitter, event, fn) {\n  // Sadly this is not cacheable as some libraries bundle their own\n  // event emitter implementation with them.\n  if (typeof emitter.prependListener === 'function') return emitter.prependListener(event, fn);\n\n  // This is a hack to make sure that our error handler is attached before any\n  // userland ones.  NEVER DO THIS. This is here only because this code needs\n  // to continue to work with older versions of Node.js that do not include\n  // the prependListener() method. The goal is to eventually remove this hack.\n  if (!emitter._events || !emitter._events[event]) emitter.on(event, fn);else if (isArray(emitter._events[event])) emitter._events[event].unshift(fn);else emitter._events[event] = [fn, emitter._events[event]];\n}\n\nfunction ReadableState(options, stream) {\n  Duplex = Duplex || require('./_stream_duplex');\n\n  options = options || {};\n\n  // Duplex streams are both readable and writable, but share\n  // the same options object.\n  // However, some cases require setting options to different\n  // values for the readable and the writable sides of the duplex stream.\n  // These options can be provided separately as readableXXX and writableXXX.\n  var isDuplex = stream instanceof Duplex;\n\n  // object stream flag. Used to make read(n) ignore n and to\n  // make all the buffer merging and length checks go away\n  this.objectMode = !!options.objectMode;\n\n  if (isDuplex) this.objectMode = this.objectMode || !!options.readableObjectMode;\n\n  // the point at which it stops calling _read() to fill the buffer\n  // Note: 0 is a valid value, means \"don't call _read preemptively ever\"\n  var hwm = options.highWaterMark;\n  var readableHwm = options.readableHighWaterMark;\n  var defaultHwm = this.objectMode ? 16 : 16 * 1024;\n\n  if (hwm || hwm === 0) this.highWaterMark = hwm;else if (isDuplex && (readableHwm || readableHwm === 0)) this.highWaterMark = readableHwm;else this.highWaterMark = defaultHwm;\n\n  // cast to ints.\n  this.highWaterMark = Math.floor(this.highWaterMark);\n\n  // A linked list is used to store data chunks instead of an array because the\n  // linked list can remove elements from the beginning faster than\n  // array.shift()\n  this.buffer = new BufferList();\n  this.length = 0;\n  this.pipes = null;\n  this.pipesCount = 0;\n  this.flowing = null;\n  this.ended = false;\n  this.endEmitted = false;\n  this.reading = false;\n\n  // a flag to be able to tell if the event 'readable'/'data' is emitted\n  // immediately, or on a later tick.  We set this to true at first, because\n  // any actions that shouldn't happen until \"later\" should generally also\n  // not happen before the first read call.\n  this.sync = true;\n\n  // whenever we return null, then we set a flag to say\n  // that we're awaiting a 'readable' event emission.\n  this.needReadable = false;\n  this.emittedReadable = false;\n  this.readableListening = false;\n  this.resumeScheduled = false;\n\n  // has it been destroyed\n  this.destroyed = false;\n\n  // Crypto is kind of old and crusty.  Historically, its default string\n  // encoding is 'binary' so we have to make this configurable.\n  // Everything else in the universe uses 'utf8', though.\n  this.defaultEncoding = options.defaultEncoding || 'utf8';\n\n  // the number of writers that are awaiting a drain event in .pipe()s\n  this.awaitDrain = 0;\n\n  // if true, a maybeReadMore has been scheduled\n  this.readingMore = false;\n\n  this.decoder = null;\n  this.encoding = null;\n  if (options.encoding) {\n    if (!StringDecoder) StringDecoder = require('string_decoder/').StringDecoder;\n    this.decoder = new StringDecoder(options.encoding);\n    this.encoding = options.encoding;\n  }\n}\n\nfunction Readable(options) {\n  Duplex = Duplex || require('./_stream_duplex');\n\n  if (!(this instanceof Readable)) return new Readable(options);\n\n  this._readableState = new ReadableState(options, this);\n\n  // legacy\n  this.readable = true;\n\n  if (options) {\n    if (typeof options.read === 'function') this._read = options.read;\n\n    if (typeof options.destroy === 'function') this._destroy = options.destroy;\n  }\n\n  Stream.call(this);\n}\n\nObject.defineProperty(Readable.prototype, 'destroyed', {\n  get: function () {\n    if (this._readableState === undefined) {\n      return false;\n    }\n    return this._readableState.destroyed;\n  },\n  set: function (value) {\n    // we ignore the value if the stream\n    // has not been initialized yet\n    if (!this._readableState) {\n      return;\n    }\n\n    // backward compatibility, the user is explicitly\n    // managing destroyed\n    this._readableState.destroyed = value;\n  }\n});\n\nReadable.prototype.destroy = destroyImpl.destroy;\nReadable.prototype._undestroy = destroyImpl.undestroy;\nReadable.prototype._destroy = function (err, cb) {\n  this.push(null);\n  cb(err);\n};\n\n// Manually shove something into the read() buffer.\n// This returns true if the highWaterMark has not been hit yet,\n// similar to how Writable.write() returns true if you should\n// write() some more.\nReadable.prototype.push = function (chunk, encoding) {\n  var state = this._readableState;\n  var skipChunkCheck;\n\n  if (!state.objectMode) {\n    if (typeof chunk === 'string') {\n      encoding = encoding || state.defaultEncoding;\n      if (encoding !== state.encoding) {\n        chunk = Buffer.from(chunk, encoding);\n        encoding = '';\n      }\n      skipChunkCheck = true;\n    }\n  } else {\n    skipChunkCheck = true;\n  }\n\n  return readableAddChunk(this, chunk, encoding, false, skipChunkCheck);\n};\n\n// Unshift should *always* be something directly out of read()\nReadable.prototype.unshift = function (chunk) {\n  return readableAddChunk(this, chunk, null, true, false);\n};\n\nfunction readableAddChunk(stream, chunk, encoding, addToFront, skipChunkCheck) {\n  var state = stream._readableState;\n  if (chunk === null) {\n    state.reading = false;\n    onEofChunk(stream, state);\n  } else {\n    var er;\n    if (!skipChunkCheck) er = chunkInvalid(state, chunk);\n    if (er) {\n      stream.emit('error', er);\n    } else if (state.objectMode || chunk && chunk.length > 0) {\n      if (typeof chunk !== 'string' && !state.objectMode && Object.getPrototypeOf(chunk) !== Buffer.prototype) {\n        chunk = _uint8ArrayToBuffer(chunk);\n      }\n\n      if (addToFront) {\n        if (state.endEmitted) stream.emit('error', new Error('stream.unshift() after end event'));else addChunk(stream, state, chunk, true);\n      } else if (state.ended) {\n        stream.emit('error', new Error('stream.push() after EOF'));\n      } else {\n        state.reading = false;\n        if (state.decoder && !encoding) {\n          chunk = state.decoder.write(chunk);\n          if (state.objectMode || chunk.length !== 0) addChunk(stream, state, chunk, false);else maybeReadMore(stream, state);\n        } else {\n          addChunk(stream, state, chunk, false);\n        }\n      }\n    } else if (!addToFront) {\n      state.reading = false;\n    }\n  }\n\n  return needMoreData(state);\n}\n\nfunction addChunk(stream, state, chunk, addToFront) {\n  if (state.flowing && state.length === 0 && !state.sync) {\n    stream.emit('data', chunk);\n    stream.read(0);\n  } else {\n    // update the buffer info.\n    state.length += state.objectMode ? 1 : chunk.length;\n    if (addToFront) state.buffer.unshift(chunk);else state.buffer.push(chunk);\n\n    if (state.needReadable) emitReadable(stream);\n  }\n  maybeReadMore(stream, state);\n}\n\nfunction chunkInvalid(state, chunk) {\n  var er;\n  if (!_isUint8Array(chunk) && typeof chunk !== 'string' && chunk !== undefined && !state.objectMode) {\n    er = new TypeError('Invalid non-string/buffer chunk');\n  }\n  return er;\n}\n\n// if it's past the high water mark, we can push in some more.\n// Also, if we have no data yet, we can stand some\n// more bytes.  This is to work around cases where hwm=0,\n// such as the repl.  Also, if the push() triggered a\n// readable event, and the user called read(largeNumber) such that\n// needReadable was set, then we ought to push more, so that another\n// 'readable' event will be triggered.\nfunction needMoreData(state) {\n  return !state.ended && (state.needReadable || state.length < state.highWaterMark || state.length === 0);\n}\n\nReadable.prototype.isPaused = function () {\n  return this._readableState.flowing === false;\n};\n\n// backwards compatibility.\nReadable.prototype.setEncoding = function (enc) {\n  if (!StringDecoder) StringDecoder = require('string_decoder/').StringDecoder;\n  this._readableState.decoder = new StringDecoder(enc);\n  this._readableState.encoding = enc;\n  return this;\n};\n\n// Don't raise the hwm > 8MB\nvar MAX_HWM = 0x800000;\nfunction computeNewHighWaterMark(n) {\n  if (n >= MAX_HWM) {\n    n = MAX_HWM;\n  } else {\n    // Get the next highest power of 2 to prevent increasing hwm excessively in\n    // tiny amounts\n    n--;\n    n |= n >>> 1;\n    n |= n >>> 2;\n    n |= n >>> 4;\n    n |= n >>> 8;\n    n |= n >>> 16;\n    n++;\n  }\n  return n;\n}\n\n// This function is designed to be inlinable, so please take care when making\n// changes to the function body.\nfunction howMuchToRead(n, state) {\n  if (n <= 0 || state.length === 0 && state.ended) return 0;\n  if (state.objectMode) return 1;\n  if (n !== n) {\n    // Only flow one buffer at a time\n    if (state.flowing && state.length) return state.buffer.head.data.length;else return state.length;\n  }\n  // If we're asking for more than the current hwm, then raise the hwm.\n  if (n > state.highWaterMark) state.highWaterMark = computeNewHighWaterMark(n);\n  if (n <= state.length) return n;\n  // Don't have enough\n  if (!state.ended) {\n    state.needReadable = true;\n    return 0;\n  }\n  return state.length;\n}\n\n// you can override either this method, or the async _read(n) below.\nReadable.prototype.read = function (n) {\n  debug('read', n);\n  n = parseInt(n, 10);\n  var state = this._readableState;\n  var nOrig = n;\n\n  if (n !== 0) state.emittedReadable = false;\n\n  // if we're doing read(0) to trigger a readable event, but we\n  // already have a bunch of data in the buffer, then just trigger\n  // the 'readable' event and move on.\n  if (n === 0 && state.needReadable && (state.length >= state.highWaterMark || state.ended)) {\n    debug('read: emitReadable', state.length, state.ended);\n    if (state.length === 0 && state.ended) endReadable(this);else emitReadable(this);\n    return null;\n  }\n\n  n = howMuchToRead(n, state);\n\n  // if we've ended, and we're now clear, then finish it up.\n  if (n === 0 && state.ended) {\n    if (state.length === 0) endReadable(this);\n    return null;\n  }\n\n  // All the actual chunk generation logic needs to be\n  // *below* the call to _read.  The reason is that in certain\n  // synthetic stream cases, such as passthrough streams, _read\n  // may be a completely synchronous operation which may change\n  // the state of the read buffer, providing enough data when\n  // before there was *not* enough.\n  //\n  // So, the steps are:\n  // 1. Figure out what the state of things will be after we do\n  // a read from the buffer.\n  //\n  // 2. If that resulting state will trigger a _read, then call _read.\n  // Note that this may be asynchronous, or synchronous.  Yes, it is\n  // deeply ugly to write APIs this way, but that still doesn't mean\n  // that the Readable class should behave improperly, as streams are\n  // designed to be sync/async agnostic.\n  // Take note if the _read call is sync or async (ie, if the read call\n  // has returned yet), so that we know whether or not it's safe to emit\n  // 'readable' etc.\n  //\n  // 3. Actually pull the requested chunks out of the buffer and return.\n\n  // if we need a readable event, then we need to do some reading.\n  var doRead = state.needReadable;\n  debug('need readable', doRead);\n\n  // if we currently have less than the highWaterMark, then also read some\n  if (state.length === 0 || state.length - n < state.highWaterMark) {\n    doRead = true;\n    debug('length less than watermark', doRead);\n  }\n\n  // however, if we've ended, then there's no point, and if we're already\n  // reading, then it's unnecessary.\n  if (state.ended || state.reading) {\n    doRead = false;\n    debug('reading or ended', doRead);\n  } else if (doRead) {\n    debug('do read');\n    state.reading = true;\n    state.sync = true;\n    // if the length is currently zero, then we *need* a readable event.\n    if (state.length === 0) state.needReadable = true;\n    // call internal read method\n    this._read(state.highWaterMark);\n    state.sync = false;\n    // If _read pushed data synchronously, then `reading` will be false,\n    // and we need to re-evaluate how much data we can return to the user.\n    if (!state.reading) n = howMuchToRead(nOrig, state);\n  }\n\n  var ret;\n  if (n > 0) ret = fromList(n, state);else ret = null;\n\n  if (ret === null) {\n    state.needReadable = true;\n    n = 0;\n  } else {\n    state.length -= n;\n  }\n\n  if (state.length === 0) {\n    // If we have nothing in the buffer, then we want to know\n    // as soon as we *do* get something into the buffer.\n    if (!state.ended) state.needReadable = true;\n\n    // If we tried to read() past the EOF, then emit end on the next tick.\n    if (nOrig !== n && state.ended) endReadable(this);\n  }\n\n  if (ret !== null) this.emit('data', ret);\n\n  return ret;\n};\n\nfunction onEofChunk(stream, state) {\n  if (state.ended) return;\n  if (state.decoder) {\n    var chunk = state.decoder.end();\n    if (chunk && chunk.length) {\n      state.buffer.push(chunk);\n      state.length += state.objectMode ? 1 : chunk.length;\n    }\n  }\n  state.ended = true;\n\n  // emit 'readable' now to make sure it gets picked up.\n  emitReadable(stream);\n}\n\n// Don't emit readable right away in sync mode, because this can trigger\n// another read() call => stack overflow.  This way, it might trigger\n// a nextTick recursion warning, but that's not so bad.\nfunction emitReadable(stream) {\n  var state = stream._readableState;\n  state.needReadable = false;\n  if (!state.emittedReadable) {\n    debug('emitReadable', state.flowing);\n    state.emittedReadable = true;\n    if (state.sync) pna.nextTick(emitReadable_, stream);else emitReadable_(stream);\n  }\n}\n\nfunction emitReadable_(stream) {\n  debug('emit readable');\n  stream.emit('readable');\n  flow(stream);\n}\n\n// at this point, the user has presumably seen the 'readable' event,\n// and called read() to consume some data.  that may have triggered\n// in turn another _read(n) call, in which case reading = true if\n// it's in progress.\n// However, if we're not ended, or reading, and the length < hwm,\n// then go ahead and try to read some more preemptively.\nfunction maybeReadMore(stream, state) {\n  if (!state.readingMore) {\n    state.readingMore = true;\n    pna.nextTick(maybeReadMore_, stream, state);\n  }\n}\n\nfunction maybeReadMore_(stream, state) {\n  var len = state.length;\n  while (!state.reading && !state.flowing && !state.ended && state.length < state.highWaterMark) {\n    debug('maybeReadMore read 0');\n    stream.read(0);\n    if (len === state.length)\n      // didn't get any data, stop spinning.\n      break;else len = state.length;\n  }\n  state.readingMore = false;\n}\n\n// abstract method.  to be overridden in specific implementation classes.\n// call cb(er, data) where data is <= n in length.\n// for virtual (non-string, non-buffer) streams, \"length\" is somewhat\n// arbitrary, and perhaps not very meaningful.\nReadable.prototype._read = function (n) {\n  this.emit('error', new Error('_read() is not implemented'));\n};\n\nReadable.prototype.pipe = function (dest, pipeOpts) {\n  var src = this;\n  var state = this._readableState;\n\n  switch (state.pipesCount) {\n    case 0:\n      state.pipes = dest;\n      break;\n    case 1:\n      state.pipes = [state.pipes, dest];\n      break;\n    default:\n      state.pipes.push(dest);\n      break;\n  }\n  state.pipesCount += 1;\n  debug('pipe count=%d opts=%j', state.pipesCount, pipeOpts);\n\n  var doEnd = (!pipeOpts || pipeOpts.end !== false) && dest !== process.stdout && dest !== process.stderr;\n\n  var endFn = doEnd ? onend : unpipe;\n  if (state.endEmitted) pna.nextTick(endFn);else src.once('end', endFn);\n\n  dest.on('unpipe', onunpipe);\n  function onunpipe(readable, unpipeInfo) {\n    debug('onunpipe');\n    if (readable === src) {\n      if (unpipeInfo && unpipeInfo.hasUnpiped === false) {\n        unpipeInfo.hasUnpiped = true;\n        cleanup();\n      }\n    }\n  }\n\n  function onend() {\n    debug('onend');\n    dest.end();\n  }\n\n  // when the dest drains, it reduces the awaitDrain counter\n  // on the source.  This would be more elegant with a .once()\n  // handler in flow(), but adding and removing repeatedly is\n  // too slow.\n  var ondrain = pipeOnDrain(src);\n  dest.on('drain', ondrain);\n\n  var cleanedUp = false;\n  function cleanup() {\n    debug('cleanup');\n    // cleanup event handlers once the pipe is broken\n    dest.removeListener('close', onclose);\n    dest.removeListener('finish', onfinish);\n    dest.removeListener('drain', ondrain);\n    dest.removeListener('error', onerror);\n    dest.removeListener('unpipe', onunpipe);\n    src.removeListener('end', onend);\n    src.removeListener('end', unpipe);\n    src.removeListener('data', ondata);\n\n    cleanedUp = true;\n\n    // if the reader is waiting for a drain event from this\n    // specific writer, then it would cause it to never start\n    // flowing again.\n    // So, if this is awaiting a drain, then we just call it now.\n    // If we don't know, then assume that we are waiting for one.\n    if (state.awaitDrain && (!dest._writableState || dest._writableState.needDrain)) ondrain();\n  }\n\n  // If the user pushes more data while we're writing to dest then we'll end up\n  // in ondata again. However, we only want to increase awaitDrain once because\n  // dest will only emit one 'drain' event for the multiple writes.\n  // => Introduce a guard on increasing awaitDrain.\n  var increasedAwaitDrain = false;\n  src.on('data', ondata);\n  function ondata(chunk) {\n    debug('ondata');\n    increasedAwaitDrain = false;\n    var ret = dest.write(chunk);\n    if (false === ret && !increasedAwaitDrain) {\n      // If the user unpiped during `dest.write()`, it is possible\n      // to get stuck in a permanently paused state if that write\n      // also returned false.\n      // => Check whether `dest` is still a piping destination.\n      if ((state.pipesCount === 1 && state.pipes === dest || state.pipesCount > 1 && indexOf(state.pipes, dest) !== -1) && !cleanedUp) {\n        debug('false write response, pause', src._readableState.awaitDrain);\n        src._readableState.awaitDrain++;\n        increasedAwaitDrain = true;\n      }\n      src.pause();\n    }\n  }\n\n  // if the dest has an error, then stop piping into it.\n  // however, don't suppress the throwing behavior for this.\n  function onerror(er) {\n    debug('onerror', er);\n    unpipe();\n    dest.removeListener('error', onerror);\n    if (EElistenerCount(dest, 'error') === 0) dest.emit('error', er);\n  }\n\n  // Make sure our error handler is attached before userland ones.\n  prependListener(dest, 'error', onerror);\n\n  // Both close and finish should trigger unpipe, but only once.\n  function onclose() {\n    dest.removeListener('finish', onfinish);\n    unpipe();\n  }\n  dest.once('close', onclose);\n  function onfinish() {\n    debug('onfinish');\n    dest.removeListener('close', onclose);\n    unpipe();\n  }\n  dest.once('finish', onfinish);\n\n  function unpipe() {\n    debug('unpipe');\n    src.unpipe(dest);\n  }\n\n  // tell the dest that it's being piped to\n  dest.emit('pipe', src);\n\n  // start the flow if it hasn't been started already.\n  if (!state.flowing) {\n    debug('pipe resume');\n    src.resume();\n  }\n\n  return dest;\n};\n\nfunction pipeOnDrain(src) {\n  return function () {\n    var state = src._readableState;\n    debug('pipeOnDrain', state.awaitDrain);\n    if (state.awaitDrain) state.awaitDrain--;\n    if (state.awaitDrain === 0 && EElistenerCount(src, 'data')) {\n      state.flowing = true;\n      flow(src);\n    }\n  };\n}\n\nReadable.prototype.unpipe = function (dest) {\n  var state = this._readableState;\n  var unpipeInfo = { hasUnpiped: false };\n\n  // if we're not piping anywhere, then do nothing.\n  if (state.pipesCount === 0) return this;\n\n  // just one destination.  most common case.\n  if (state.pipesCount === 1) {\n    // passed in one, but it's not the right one.\n    if (dest && dest !== state.pipes) return this;\n\n    if (!dest) dest = state.pipes;\n\n    // got a match.\n    state.pipes = null;\n    state.pipesCount = 0;\n    state.flowing = false;\n    if (dest) dest.emit('unpipe', this, unpipeInfo);\n    return this;\n  }\n\n  // slow case. multiple pipe destinations.\n\n  if (!dest) {\n    // remove all.\n    var dests = state.pipes;\n    var len = state.pipesCount;\n    state.pipes = null;\n    state.pipesCount = 0;\n    state.flowing = false;\n\n    for (var i = 0; i < len; i++) {\n      dests[i].emit('unpipe', this, unpipeInfo);\n    }return this;\n  }\n\n  // try to find the right one.\n  var index = indexOf(state.pipes, dest);\n  if (index === -1) return this;\n\n  state.pipes.splice(index, 1);\n  state.pipesCount -= 1;\n  if (state.pipesCount === 1) state.pipes = state.pipes[0];\n\n  dest.emit('unpipe', this, unpipeInfo);\n\n  return this;\n};\n\n// set up data events if they are asked for\n// Ensure readable listeners eventually get something\nReadable.prototype.on = function (ev, fn) {\n  var res = Stream.prototype.on.call(this, ev, fn);\n\n  if (ev === 'data') {\n    // Start flowing on next tick if stream isn't explicitly paused\n    if (this._readableState.flowing !== false) this.resume();\n  } else if (ev === 'readable') {\n    var state = this._readableState;\n    if (!state.endEmitted && !state.readableListening) {\n      state.readableListening = state.needReadable = true;\n      state.emittedReadable = false;\n      if (!state.reading) {\n        pna.nextTick(nReadingNextTick, this);\n      } else if (state.length) {\n        emitReadable(this);\n      }\n    }\n  }\n\n  return res;\n};\nReadable.prototype.addListener = Readable.prototype.on;\n\nfunction nReadingNextTick(self) {\n  debug('readable nexttick read 0');\n  self.read(0);\n}\n\n// pause() and resume() are remnants of the legacy readable stream API\n// If the user uses them, then switch into old mode.\nReadable.prototype.resume = function () {\n  var state = this._readableState;\n  if (!state.flowing) {\n    debug('resume');\n    state.flowing = true;\n    resume(this, state);\n  }\n  return this;\n};\n\nfunction resume(stream, state) {\n  if (!state.resumeScheduled) {\n    state.resumeScheduled = true;\n    pna.nextTick(resume_, stream, state);\n  }\n}\n\nfunction resume_(stream, state) {\n  if (!state.reading) {\n    debug('resume read 0');\n    stream.read(0);\n  }\n\n  state.resumeScheduled = false;\n  state.awaitDrain = 0;\n  stream.emit('resume');\n  flow(stream);\n  if (state.flowing && !state.reading) stream.read(0);\n}\n\nReadable.prototype.pause = function () {\n  debug('call pause flowing=%j', this._readableState.flowing);\n  if (false !== this._readableState.flowing) {\n    debug('pause');\n    this._readableState.flowing = false;\n    this.emit('pause');\n  }\n  return this;\n};\n\nfunction flow(stream) {\n  var state = stream._readableState;\n  debug('flow', state.flowing);\n  while (state.flowing && stream.read() !== null) {}\n}\n\n// wrap an old-style stream as the async data source.\n// This is *not* part of the readable stream interface.\n// It is an ugly unfortunate mess of history.\nReadable.prototype.wrap = function (stream) {\n  var _this = this;\n\n  var state = this._readableState;\n  var paused = false;\n\n  stream.on('end', function () {\n    debug('wrapped end');\n    if (state.decoder && !state.ended) {\n      var chunk = state.decoder.end();\n      if (chunk && chunk.length) _this.push(chunk);\n    }\n\n    _this.push(null);\n  });\n\n  stream.on('data', function (chunk) {\n    debug('wrapped data');\n    if (state.decoder) chunk = state.decoder.write(chunk);\n\n    // don't skip over falsy values in objectMode\n    if (state.objectMode && (chunk === null || chunk === undefined)) return;else if (!state.objectMode && (!chunk || !chunk.length)) return;\n\n    var ret = _this.push(chunk);\n    if (!ret) {\n      paused = true;\n      stream.pause();\n    }\n  });\n\n  // proxy all the other methods.\n  // important when wrapping filters and duplexes.\n  for (var i in stream) {\n    if (this[i] === undefined && typeof stream[i] === 'function') {\n      this[i] = function (method) {\n        return function () {\n          return stream[method].apply(stream, arguments);\n        };\n      }(i);\n    }\n  }\n\n  // proxy certain important events.\n  for (var n = 0; n < kProxyEvents.length; n++) {\n    stream.on(kProxyEvents[n], this.emit.bind(this, kProxyEvents[n]));\n  }\n\n  // when we try to consume some more bytes, simply unpause the\n  // underlying stream.\n  this._read = function (n) {\n    debug('wrapped _read', n);\n    if (paused) {\n      paused = false;\n      stream.resume();\n    }\n  };\n\n  return this;\n};\n\nObject.defineProperty(Readable.prototype, 'readableHighWaterMark', {\n  // making it explicit this property is not enumerable\n  // because otherwise some prototype manipulation in\n  // userland will fail\n  enumerable: false,\n  get: function () {\n    return this._readableState.highWaterMark;\n  }\n});\n\n// exposed for testing purposes only.\nReadable._fromList = fromList;\n\n// Pluck off n bytes from an array of buffers.\n// Length is the combined lengths of all the buffers in the list.\n// This function is designed to be inlinable, so please take care when making\n// changes to the function body.\nfunction fromList(n, state) {\n  // nothing buffered\n  if (state.length === 0) return null;\n\n  var ret;\n  if (state.objectMode) ret = state.buffer.shift();else if (!n || n >= state.length) {\n    // read it all, truncate the list\n    if (state.decoder) ret = state.buffer.join('');else if (state.buffer.length === 1) ret = state.buffer.head.data;else ret = state.buffer.concat(state.length);\n    state.buffer.clear();\n  } else {\n    // read part of list\n    ret = fromListPartial(n, state.buffer, state.decoder);\n  }\n\n  return ret;\n}\n\n// Extracts only enough buffered data to satisfy the amount requested.\n// This function is designed to be inlinable, so please take care when making\n// changes to the function body.\nfunction fromListPartial(n, list, hasStrings) {\n  var ret;\n  if (n < list.head.data.length) {\n    // slice is the same for buffers and strings\n    ret = list.head.data.slice(0, n);\n    list.head.data = list.head.data.slice(n);\n  } else if (n === list.head.data.length) {\n    // first chunk is a perfect match\n    ret = list.shift();\n  } else {\n    // result spans more than one buffer\n    ret = hasStrings ? copyFromBufferString(n, list) : copyFromBuffer(n, list);\n  }\n  return ret;\n}\n\n// Copies a specified amount of characters from the list of buffered data\n// chunks.\n// This function is designed to be inlinable, so please take care when making\n// changes to the function body.\nfunction copyFromBufferString(n, list) {\n  var p = list.head;\n  var c = 1;\n  var ret = p.data;\n  n -= ret.length;\n  while (p = p.next) {\n    var str = p.data;\n    var nb = n > str.length ? str.length : n;\n    if (nb === str.length) ret += str;else ret += str.slice(0, n);\n    n -= nb;\n    if (n === 0) {\n      if (nb === str.length) {\n        ++c;\n        if (p.next) list.head = p.next;else list.head = list.tail = null;\n      } else {\n        list.head = p;\n        p.data = str.slice(nb);\n      }\n      break;\n    }\n    ++c;\n  }\n  list.length -= c;\n  return ret;\n}\n\n// Copies a specified amount of bytes from the list of buffered data chunks.\n// This function is designed to be inlinable, so please take care when making\n// changes to the function body.\nfunction copyFromBuffer(n, list) {\n  var ret = Buffer.allocUnsafe(n);\n  var p = list.head;\n  var c = 1;\n  p.data.copy(ret);\n  n -= p.data.length;\n  while (p = p.next) {\n    var buf = p.data;\n    var nb = n > buf.length ? buf.length : n;\n    buf.copy(ret, ret.length - n, 0, nb);\n    n -= nb;\n    if (n === 0) {\n      if (nb === buf.length) {\n        ++c;\n        if (p.next) list.head = p.next;else list.head = list.tail = null;\n      } else {\n        list.head = p;\n        p.data = buf.slice(nb);\n      }\n      break;\n    }\n    ++c;\n  }\n  list.length -= c;\n  return ret;\n}\n\nfunction endReadable(stream) {\n  var state = stream._readableState;\n\n  // If we get here before consuming all the bytes, then that is a\n  // bug in node.  Should never happen.\n  if (state.length > 0) throw new Error('\"endReadable()\" called on non-empty stream');\n\n  if (!state.endEmitted) {\n    state.ended = true;\n    pna.nextTick(endReadableNT, state, stream);\n  }\n}\n\nfunction endReadableNT(state, stream) {\n  // Check that we didn't get one last unshift.\n  if (!state.endEmitted && state.length === 0) {\n    state.endEmitted = true;\n    stream.readable = false;\n    stream.emit('end');\n  }\n}\n\nfunction indexOf(xs, x) {\n  for (var i = 0, l = xs.length; i < l; i++) {\n    if (xs[i] === x) return i;\n  }\n  return -1;\n}","module.exports = require('events').EventEmitter;\n","'use strict';\n\n/*<replacement>*/\n\nvar pna = require('process-nextick-args');\n/*</replacement>*/\n\n// undocumented cb() API, needed for core, not for public API\nfunction destroy(err, cb) {\n  var _this = this;\n\n  var readableDestroyed = this._readableState && this._readableState.destroyed;\n  var writableDestroyed = this._writableState && this._writableState.destroyed;\n\n  if (readableDestroyed || writableDestroyed) {\n    if (cb) {\n      cb(err);\n    } else if (err && (!this._writableState || !this._writableState.errorEmitted)) {\n      pna.nextTick(emitErrorNT, this, err);\n    }\n    return this;\n  }\n\n  // we set destroyed to true before firing error callbacks in order\n  // to make it re-entrance safe in case destroy() is called within callbacks\n\n  if (this._readableState) {\n    this._readableState.destroyed = true;\n  }\n\n  // if this is a duplex stream mark the writable part as destroyed as well\n  if (this._writableState) {\n    this._writableState.destroyed = true;\n  }\n\n  this._destroy(err || null, function (err) {\n    if (!cb && err) {\n      pna.nextTick(emitErrorNT, _this, err);\n      if (_this._writableState) {\n        _this._writableState.errorEmitted = true;\n      }\n    } else if (cb) {\n      cb(err);\n    }\n  });\n\n  return this;\n}\n\nfunction undestroy() {\n  if (this._readableState) {\n    this._readableState.destroyed = false;\n    this._readableState.reading = false;\n    this._readableState.ended = false;\n    this._readableState.endEmitted = false;\n  }\n\n  if (this._writableState) {\n    this._writableState.destroyed = false;\n    this._writableState.ended = false;\n    this._writableState.ending = false;\n    this._writableState.finished = false;\n    this._writableState.errorEmitted = false;\n  }\n}\n\nfunction emitErrorNT(self, err) {\n  self.emit('error', err);\n}\n\nmodule.exports = {\n  destroy: destroy,\n  undestroy: undestroy\n};","\n/**\n * Module exports.\n */\n\nmodule.exports = deprecate;\n\n/**\n * Mark that a method should not be used.\n * Returns a modified function which warns once by default.\n *\n * If `localStorage.noDeprecation = true` is set, then it is a no-op.\n *\n * If `localStorage.throwDeprecation = true` is set, then deprecated functions\n * will throw an Error when invoked.\n *\n * If `localStorage.traceDeprecation = true` is set, then deprecated functions\n * will invoke `console.trace()` instead of `console.error()`.\n *\n * @param {Function} fn - the function to deprecate\n * @param {String} msg - the string to print to the console when `fn` is invoked\n * @returns {Function} a new \"deprecated\" version of `fn`\n * @api public\n */\n\nfunction deprecate (fn, msg) {\n  if (config('noDeprecation')) {\n    return fn;\n  }\n\n  var warned = false;\n  function deprecated() {\n    if (!warned) {\n      if (config('throwDeprecation')) {\n        throw new Error(msg);\n      } else if (config('traceDeprecation')) {\n        console.trace(msg);\n      } else {\n        console.warn(msg);\n      }\n      warned = true;\n    }\n    return fn.apply(this, arguments);\n  }\n\n  return deprecated;\n}\n\n/**\n * Checks `localStorage` for boolean values for the given `name`.\n *\n * @param {String} name\n * @returns {Boolean}\n * @api private\n */\n\nfunction config (name) {\n  // accessing global.localStorage can trigger a DOMException in sandboxed iframes\n  try {\n    if (!global.localStorage) return false;\n  } catch (_) {\n    return false;\n  }\n  var val = global.localStorage[name];\n  if (null == val) return false;\n  return String(val).toLowerCase() === 'true';\n}\n","// Copyright Joyent, Inc. and other Node contributors.\n//\n// Permission is hereby granted, free of charge, to any person obtaining a\n// copy of this software and associated documentation files (the\n// \"Software\"), to deal in the Software without restriction, including\n// without limitation the rights to use, copy, modify, merge, publish,\n// distribute, sublicense, and/or sell copies of the Software, and to permit\n// persons to whom the Software is furnished to do so, subject to the\n// following conditions:\n//\n// The above copyright notice and this permission notice shall be included\n// in all copies or substantial portions of the Software.\n//\n// THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS\n// OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n// MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN\n// NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n// DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR\n// OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE\n// USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n// a transform stream is a readable/writable stream where you do\n// something with the data.  Sometimes it's called a \"filter\",\n// but that's not a great name for it, since that implies a thing where\n// some bits pass through, and others are simply ignored.  (That would\n// be a valid example of a transform, of course.)\n//\n// While the output is causally related to the input, it's not a\n// necessarily symmetric or synchronous transformation.  For example,\n// a zlib stream might take multiple plain-text writes(), and then\n// emit a single compressed chunk some time in the future.\n//\n// Here's how this works:\n//\n// The Transform stream has all the aspects of the readable and writable\n// stream classes.  When you write(chunk), that calls _write(chunk,cb)\n// internally, and returns false if there's a lot of pending writes\n// buffered up.  When you call read(), that calls _read(n) until\n// there's enough pending readable data buffered up.\n//\n// In a transform stream, the written data is placed in a buffer.  When\n// _read(n) is called, it transforms the queued up data, calling the\n// buffered _write cb's as it consumes chunks.  If consuming a single\n// written chunk would result in multiple output chunks, then the first\n// outputted bit calls the readcb, and subsequent chunks just go into\n// the read buffer, and will cause it to emit 'readable' if necessary.\n//\n// This way, back-pressure is actually determined by the reading side,\n// since _read has to be called to start processing a new chunk.  However,\n// a pathological inflate type of transform can cause excessive buffering\n// here.  For example, imagine a stream where every byte of input is\n// interpreted as an integer from 0-255, and then results in that many\n// bytes of output.  Writing the 4 bytes {ff,ff,ff,ff} would result in\n// 1kb of data being output.  In this case, you could write a very small\n// amount of input, and end up with a very large amount of output.  In\n// such a pathological inflating mechanism, there'd be no way to tell\n// the system to stop doing the transform.  A single 4MB write could\n// cause the system to run out of memory.\n//\n// However, even in such a pathological case, only a single written chunk\n// would be consumed, and then the rest would wait (un-transformed) until\n// the results of the previous transformed chunk were consumed.\n\n'use strict';\n\nmodule.exports = Transform;\n\nvar Duplex = require('./_stream_duplex');\n\n/*<replacement>*/\nvar util = Object.create(require('core-util-is'));\nutil.inherits = require('inherits');\n/*</replacement>*/\n\nutil.inherits(Transform, Duplex);\n\nfunction afterTransform(er, data) {\n  var ts = this._transformState;\n  ts.transforming = false;\n\n  var cb = ts.writecb;\n\n  if (!cb) {\n    return this.emit('error', new Error('write callback called multiple times'));\n  }\n\n  ts.writechunk = null;\n  ts.writecb = null;\n\n  if (data != null) // single equals check for both `null` and `undefined`\n    this.push(data);\n\n  cb(er);\n\n  var rs = this._readableState;\n  rs.reading = false;\n  if (rs.needReadable || rs.length < rs.highWaterMark) {\n    this._read(rs.highWaterMark);\n  }\n}\n\nfunction Transform(options) {\n  if (!(this instanceof Transform)) return new Transform(options);\n\n  Duplex.call(this, options);\n\n  this._transformState = {\n    afterTransform: afterTransform.bind(this),\n    needTransform: false,\n    transforming: false,\n    writecb: null,\n    writechunk: null,\n    writeencoding: null\n  };\n\n  // start out asking for a readable event once data is transformed.\n  this._readableState.needReadable = true;\n\n  // we have implemented the _read method, and done the other things\n  // that Readable wants before the first _read call, so unset the\n  // sync guard flag.\n  this._readableState.sync = false;\n\n  if (options) {\n    if (typeof options.transform === 'function') this._transform = options.transform;\n\n    if (typeof options.flush === 'function') this._flush = options.flush;\n  }\n\n  // When the writable side finishes, then flush out anything remaining.\n  this.on('prefinish', prefinish);\n}\n\nfunction prefinish() {\n  var _this = this;\n\n  if (typeof this._flush === 'function') {\n    this._flush(function (er, data) {\n      done(_this, er, data);\n    });\n  } else {\n    done(this, null, null);\n  }\n}\n\nTransform.prototype.push = function (chunk, encoding) {\n  this._transformState.needTransform = false;\n  return Duplex.prototype.push.call(this, chunk, encoding);\n};\n\n// This is the part where you do stuff!\n// override this function in implementation classes.\n// 'chunk' is an input chunk.\n//\n// Call `push(newChunk)` to pass along transformed output\n// to the readable side.  You may call 'push' zero or more times.\n//\n// Call `cb(err)` when you are done with this chunk.  If you pass\n// an error, then that'll put the hurt on the whole operation.  If you\n// never call cb(), then you'll never get another chunk.\nTransform.prototype._transform = function (chunk, encoding, cb) {\n  throw new Error('_transform() is not implemented');\n};\n\nTransform.prototype._write = function (chunk, encoding, cb) {\n  var ts = this._transformState;\n  ts.writecb = cb;\n  ts.writechunk = chunk;\n  ts.writeencoding = encoding;\n  if (!ts.transforming) {\n    var rs = this._readableState;\n    if (ts.needTransform || rs.needReadable || rs.length < rs.highWaterMark) this._read(rs.highWaterMark);\n  }\n};\n\n// Doesn't matter what the args are here.\n// _transform does all the work.\n// That we got here means that the readable side wants more data.\nTransform.prototype._read = function (n) {\n  var ts = this._transformState;\n\n  if (ts.writechunk !== null && ts.writecb && !ts.transforming) {\n    ts.transforming = true;\n    this._transform(ts.writechunk, ts.writeencoding, ts.afterTransform);\n  } else {\n    // mark that we need a transform, so that any data that comes in\n    // will get processed, now that we've asked for it.\n    ts.needTransform = true;\n  }\n};\n\nTransform.prototype._destroy = function (err, cb) {\n  var _this2 = this;\n\n  Duplex.prototype._destroy.call(this, err, function (err2) {\n    cb(err2);\n    _this2.emit('close');\n  });\n};\n\nfunction done(stream, er, data) {\n  if (er) return stream.emit('error', er);\n\n  if (data != null) // single equals check for both `null` and `undefined`\n    stream.push(data);\n\n  // if there's nothing in the write buffer, then that means\n  // that nothing more will ever be provided\n  if (stream._writableState.length) throw new Error('Calling transform done when ws.length != 0');\n\n  if (stream._transformState.transforming) throw new Error('Calling transform done when still transforming');\n\n  return stream.push(null);\n}","'use strict'\nvar Buffer = require('safe-buffer').Buffer\nvar Transform = require('readable-stream').Transform\nvar inherits = require('inherits')\n\nfunction throwIfNotStringOrBuffer (val, prefix) {\n  if (!Buffer.isBuffer(val) && typeof val !== 'string') {\n    throw new TypeError(prefix + ' must be a string or a buffer')\n  }\n}\n\nfunction HashBase (blockSize) {\n  Transform.call(this)\n\n  this._block = Buffer.allocUnsafe(blockSize)\n  this._blockSize = blockSize\n  this._blockOffset = 0\n  this._length = [0, 0, 0, 0]\n\n  this._finalized = false\n}\n\ninherits(HashBase, Transform)\n\nHashBase.prototype._transform = function (chunk, encoding, callback) {\n  var error = null\n  try {\n    this.update(chunk, encoding)\n  } catch (err) {\n    error = err\n  }\n\n  callback(error)\n}\n\nHashBase.prototype._flush = function (callback) {\n  var error = null\n  try {\n    this.push(this.digest())\n  } catch (err) {\n    error = err\n  }\n\n  callback(error)\n}\n\nHashBase.prototype.update = function (data, encoding) {\n  throwIfNotStringOrBuffer(data, 'Data')\n  if (this._finalized) throw new Error('Digest already called')\n  if (!Buffer.isBuffer(data)) data = Buffer.from(data, encoding)\n\n  // consume data\n  var block = this._block\n  var offset = 0\n  while (this._blockOffset + data.length - offset >= this._blockSize) {\n    for (var i = this._blockOffset; i < this._blockSize;) block[i++] = data[offset++]\n    this._update()\n    this._blockOffset = 0\n  }\n  while (offset < data.length) block[this._blockOffset++] = data[offset++]\n\n  // update length\n  for (var j = 0, carry = data.length * 8; carry > 0; ++j) {\n    this._length[j] += carry\n    carry = (this._length[j] / 0x0100000000) | 0\n    if (carry > 0) this._length[j] -= 0x0100000000 * carry\n  }\n\n  return this\n}\n\nHashBase.prototype._update = function () {\n  throw new Error('_update is not implemented')\n}\n\nHashBase.prototype.digest = function (encoding) {\n  if (this._finalized) throw new Error('Digest already called')\n  this._finalized = true\n\n  var digest = this._digest()\n  if (encoding !== undefined) digest = digest.toString(encoding)\n\n  // reset state\n  this._block.fill(0)\n  this._blockOffset = 0\n  for (var i = 0; i < 4; ++i) this._length[i] = 0\n\n  return digest\n}\n\nHashBase.prototype._digest = function () {\n  throw new Error('_digest is not implemented')\n}\n\nmodule.exports = HashBase\n","// Copyright Joyent, Inc. and other Node contributors.\n//\n// Permission is hereby granted, free of charge, to any person obtaining a\n// copy of this software and associated documentation files (the\n// \"Software\"), to deal in the Software without restriction, including\n// without limitation the rights to use, copy, modify, merge, publish,\n// distribute, sublicense, and/or sell copies of the Software, and to permit\n// persons to whom the Software is furnished to do so, subject to the\n// following conditions:\n//\n// The above copyright notice and this permission notice shall be included\n// in all copies or substantial portions of the Software.\n//\n// THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS\n// OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n// MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN\n// NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n// DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR\n// OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE\n// USE OR OTHER DEALINGS IN THE SOFTWARE.\n'use strict';\n\nmodule.exports = Readable;\n/*<replacement>*/\n\nvar Duplex;\n/*</replacement>*/\n\nReadable.ReadableState = ReadableState;\n/*<replacement>*/\n\nvar EE = require('events').EventEmitter;\n\nvar EElistenerCount = function EElistenerCount(emitter, type) {\n  return emitter.listeners(type).length;\n};\n/*</replacement>*/\n\n/*<replacement>*/\n\n\nvar Stream = require('./internal/streams/stream');\n/*</replacement>*/\n\n\nvar Buffer = require('buffer').Buffer;\n\nvar OurUint8Array = global.Uint8Array || function () {};\n\nfunction _uint8ArrayToBuffer(chunk) {\n  return Buffer.from(chunk);\n}\n\nfunction _isUint8Array(obj) {\n  return Buffer.isBuffer(obj) || obj instanceof OurUint8Array;\n}\n/*<replacement>*/\n\n\nvar debugUtil = require('util');\n\nvar debug;\n\nif (debugUtil && debugUtil.debuglog) {\n  debug = debugUtil.debuglog('stream');\n} else {\n  debug = function debug() {};\n}\n/*</replacement>*/\n\n\nvar BufferList = require('./internal/streams/buffer_list');\n\nvar destroyImpl = require('./internal/streams/destroy');\n\nvar _require = require('./internal/streams/state'),\n    getHighWaterMark = _require.getHighWaterMark;\n\nvar _require$codes = require('../errors').codes,\n    ERR_INVALID_ARG_TYPE = _require$codes.ERR_INVALID_ARG_TYPE,\n    ERR_STREAM_PUSH_AFTER_EOF = _require$codes.ERR_STREAM_PUSH_AFTER_EOF,\n    ERR_METHOD_NOT_IMPLEMENTED = _require$codes.ERR_METHOD_NOT_IMPLEMENTED,\n    ERR_STREAM_UNSHIFT_AFTER_END_EVENT = _require$codes.ERR_STREAM_UNSHIFT_AFTER_END_EVENT; // Lazy loaded to improve the startup performance.\n\n\nvar StringDecoder;\nvar createReadableStreamAsyncIterator;\nvar from;\n\nrequire('inherits')(Readable, Stream);\n\nvar errorOrDestroy = destroyImpl.errorOrDestroy;\nvar kProxyEvents = ['error', 'close', 'destroy', 'pause', 'resume'];\n\nfunction prependListener(emitter, event, fn) {\n  // Sadly this is not cacheable as some libraries bundle their own\n  // event emitter implementation with them.\n  if (typeof emitter.prependListener === 'function') return emitter.prependListener(event, fn); // This is a hack to make sure that our error handler is attached before any\n  // userland ones.  NEVER DO THIS. This is here only because this code needs\n  // to continue to work with older versions of Node.js that do not include\n  // the prependListener() method. The goal is to eventually remove this hack.\n\n  if (!emitter._events || !emitter._events[event]) emitter.on(event, fn);else if (Array.isArray(emitter._events[event])) emitter._events[event].unshift(fn);else emitter._events[event] = [fn, emitter._events[event]];\n}\n\nfunction ReadableState(options, stream, isDuplex) {\n  Duplex = Duplex || require('./_stream_duplex');\n  options = options || {}; // Duplex streams are both readable and writable, but share\n  // the same options object.\n  // However, some cases require setting options to different\n  // values for the readable and the writable sides of the duplex stream.\n  // These options can be provided separately as readableXXX and writableXXX.\n\n  if (typeof isDuplex !== 'boolean') isDuplex = stream instanceof Duplex; // object stream flag. Used to make read(n) ignore n and to\n  // make all the buffer merging and length checks go away\n\n  this.objectMode = !!options.objectMode;\n  if (isDuplex) this.objectMode = this.objectMode || !!options.readableObjectMode; // the point at which it stops calling _read() to fill the buffer\n  // Note: 0 is a valid value, means \"don't call _read preemptively ever\"\n\n  this.highWaterMark = getHighWaterMark(this, options, 'readableHighWaterMark', isDuplex); // A linked list is used to store data chunks instead of an array because the\n  // linked list can remove elements from the beginning faster than\n  // array.shift()\n\n  this.buffer = new BufferList();\n  this.length = 0;\n  this.pipes = null;\n  this.pipesCount = 0;\n  this.flowing = null;\n  this.ended = false;\n  this.endEmitted = false;\n  this.reading = false; // a flag to be able to tell if the event 'readable'/'data' is emitted\n  // immediately, or on a later tick.  We set this to true at first, because\n  // any actions that shouldn't happen until \"later\" should generally also\n  // not happen before the first read call.\n\n  this.sync = true; // whenever we return null, then we set a flag to say\n  // that we're awaiting a 'readable' event emission.\n\n  this.needReadable = false;\n  this.emittedReadable = false;\n  this.readableListening = false;\n  this.resumeScheduled = false;\n  this.paused = true; // Should close be emitted on destroy. Defaults to true.\n\n  this.emitClose = options.emitClose !== false; // Should .destroy() be called after 'end' (and potentially 'finish')\n\n  this.autoDestroy = !!options.autoDestroy; // has it been destroyed\n\n  this.destroyed = false; // Crypto is kind of old and crusty.  Historically, its default string\n  // encoding is 'binary' so we have to make this configurable.\n  // Everything else in the universe uses 'utf8', though.\n\n  this.defaultEncoding = options.defaultEncoding || 'utf8'; // the number of writers that are awaiting a drain event in .pipe()s\n\n  this.awaitDrain = 0; // if true, a maybeReadMore has been scheduled\n\n  this.readingMore = false;\n  this.decoder = null;\n  this.encoding = null;\n\n  if (options.encoding) {\n    if (!StringDecoder) StringDecoder = require('string_decoder/').StringDecoder;\n    this.decoder = new StringDecoder(options.encoding);\n    this.encoding = options.encoding;\n  }\n}\n\nfunction Readable(options) {\n  Duplex = Duplex || require('./_stream_duplex');\n  if (!(this instanceof Readable)) return new Readable(options); // Checking for a Stream.Duplex instance is faster here instead of inside\n  // the ReadableState constructor, at least with V8 6.5\n\n  var isDuplex = this instanceof Duplex;\n  this._readableState = new ReadableState(options, this, isDuplex); // legacy\n\n  this.readable = true;\n\n  if (options) {\n    if (typeof options.read === 'function') this._read = options.read;\n    if (typeof options.destroy === 'function') this._destroy = options.destroy;\n  }\n\n  Stream.call(this);\n}\n\nObject.defineProperty(Readable.prototype, 'destroyed', {\n  // making it explicit this property is not enumerable\n  // because otherwise some prototype manipulation in\n  // userland will fail\n  enumerable: false,\n  get: function get() {\n    if (this._readableState === undefined) {\n      return false;\n    }\n\n    return this._readableState.destroyed;\n  },\n  set: function set(value) {\n    // we ignore the value if the stream\n    // has not been initialized yet\n    if (!this._readableState) {\n      return;\n    } // backward compatibility, the user is explicitly\n    // managing destroyed\n\n\n    this._readableState.destroyed = value;\n  }\n});\nReadable.prototype.destroy = destroyImpl.destroy;\nReadable.prototype._undestroy = destroyImpl.undestroy;\n\nReadable.prototype._destroy = function (err, cb) {\n  cb(err);\n}; // Manually shove something into the read() buffer.\n// This returns true if the highWaterMark has not been hit yet,\n// similar to how Writable.write() returns true if you should\n// write() some more.\n\n\nReadable.prototype.push = function (chunk, encoding) {\n  var state = this._readableState;\n  var skipChunkCheck;\n\n  if (!state.objectMode) {\n    if (typeof chunk === 'string') {\n      encoding = encoding || state.defaultEncoding;\n\n      if (encoding !== state.encoding) {\n        chunk = Buffer.from(chunk, encoding);\n        encoding = '';\n      }\n\n      skipChunkCheck = true;\n    }\n  } else {\n    skipChunkCheck = true;\n  }\n\n  return readableAddChunk(this, chunk, encoding, false, skipChunkCheck);\n}; // Unshift should *always* be something directly out of read()\n\n\nReadable.prototype.unshift = function (chunk) {\n  return readableAddChunk(this, chunk, null, true, false);\n};\n\nfunction readableAddChunk(stream, chunk, encoding, addToFront, skipChunkCheck) {\n  debug('readableAddChunk', chunk);\n  var state = stream._readableState;\n\n  if (chunk === null) {\n    state.reading = false;\n    onEofChunk(stream, state);\n  } else {\n    var er;\n    if (!skipChunkCheck) er = chunkInvalid(state, chunk);\n\n    if (er) {\n      errorOrDestroy(stream, er);\n    } else if (state.objectMode || chunk && chunk.length > 0) {\n      if (typeof chunk !== 'string' && !state.objectMode && Object.getPrototypeOf(chunk) !== Buffer.prototype) {\n        chunk = _uint8ArrayToBuffer(chunk);\n      }\n\n      if (addToFront) {\n        if (state.endEmitted) errorOrDestroy(stream, new ERR_STREAM_UNSHIFT_AFTER_END_EVENT());else addChunk(stream, state, chunk, true);\n      } else if (state.ended) {\n        errorOrDestroy(stream, new ERR_STREAM_PUSH_AFTER_EOF());\n      } else if (state.destroyed) {\n        return false;\n      } else {\n        state.reading = false;\n\n        if (state.decoder && !encoding) {\n          chunk = state.decoder.write(chunk);\n          if (state.objectMode || chunk.length !== 0) addChunk(stream, state, chunk, false);else maybeReadMore(stream, state);\n        } else {\n          addChunk(stream, state, chunk, false);\n        }\n      }\n    } else if (!addToFront) {\n      state.reading = false;\n      maybeReadMore(stream, state);\n    }\n  } // We can push more data if we are below the highWaterMark.\n  // Also, if we have no data yet, we can stand some more bytes.\n  // This is to work around cases where hwm=0, such as the repl.\n\n\n  return !state.ended && (state.length < state.highWaterMark || state.length === 0);\n}\n\nfunction addChunk(stream, state, chunk, addToFront) {\n  if (state.flowing && state.length === 0 && !state.sync) {\n    state.awaitDrain = 0;\n    stream.emit('data', chunk);\n  } else {\n    // update the buffer info.\n    state.length += state.objectMode ? 1 : chunk.length;\n    if (addToFront) state.buffer.unshift(chunk);else state.buffer.push(chunk);\n    if (state.needReadable) emitReadable(stream);\n  }\n\n  maybeReadMore(stream, state);\n}\n\nfunction chunkInvalid(state, chunk) {\n  var er;\n\n  if (!_isUint8Array(chunk) && typeof chunk !== 'string' && chunk !== undefined && !state.objectMode) {\n    er = new ERR_INVALID_ARG_TYPE('chunk', ['string', 'Buffer', 'Uint8Array'], chunk);\n  }\n\n  return er;\n}\n\nReadable.prototype.isPaused = function () {\n  return this._readableState.flowing === false;\n}; // backwards compatibility.\n\n\nReadable.prototype.setEncoding = function (enc) {\n  if (!StringDecoder) StringDecoder = require('string_decoder/').StringDecoder;\n  var decoder = new StringDecoder(enc);\n  this._readableState.decoder = decoder; // If setEncoding(null), decoder.encoding equals utf8\n\n  this._readableState.encoding = this._readableState.decoder.encoding; // Iterate over current buffer to convert already stored Buffers:\n\n  var p = this._readableState.buffer.head;\n  var content = '';\n\n  while (p !== null) {\n    content += decoder.write(p.data);\n    p = p.next;\n  }\n\n  this._readableState.buffer.clear();\n\n  if (content !== '') this._readableState.buffer.push(content);\n  this._readableState.length = content.length;\n  return this;\n}; // Don't raise the hwm > 1GB\n\n\nvar MAX_HWM = 0x40000000;\n\nfunction computeNewHighWaterMark(n) {\n  if (n >= MAX_HWM) {\n    // TODO(ronag): Throw ERR_VALUE_OUT_OF_RANGE.\n    n = MAX_HWM;\n  } else {\n    // Get the next highest power of 2 to prevent increasing hwm excessively in\n    // tiny amounts\n    n--;\n    n |= n >>> 1;\n    n |= n >>> 2;\n    n |= n >>> 4;\n    n |= n >>> 8;\n    n |= n >>> 16;\n    n++;\n  }\n\n  return n;\n} // This function is designed to be inlinable, so please take care when making\n// changes to the function body.\n\n\nfunction howMuchToRead(n, state) {\n  if (n <= 0 || state.length === 0 && state.ended) return 0;\n  if (state.objectMode) return 1;\n\n  if (n !== n) {\n    // Only flow one buffer at a time\n    if (state.flowing && state.length) return state.buffer.head.data.length;else return state.length;\n  } // If we're asking for more than the current hwm, then raise the hwm.\n\n\n  if (n > state.highWaterMark) state.highWaterMark = computeNewHighWaterMark(n);\n  if (n <= state.length) return n; // Don't have enough\n\n  if (!state.ended) {\n    state.needReadable = true;\n    return 0;\n  }\n\n  return state.length;\n} // you can override either this method, or the async _read(n) below.\n\n\nReadable.prototype.read = function (n) {\n  debug('read', n);\n  n = parseInt(n, 10);\n  var state = this._readableState;\n  var nOrig = n;\n  if (n !== 0) state.emittedReadable = false; // if we're doing read(0) to trigger a readable event, but we\n  // already have a bunch of data in the buffer, then just trigger\n  // the 'readable' event and move on.\n\n  if (n === 0 && state.needReadable && ((state.highWaterMark !== 0 ? state.length >= state.highWaterMark : state.length > 0) || state.ended)) {\n    debug('read: emitReadable', state.length, state.ended);\n    if (state.length === 0 && state.ended) endReadable(this);else emitReadable(this);\n    return null;\n  }\n\n  n = howMuchToRead(n, state); // if we've ended, and we're now clear, then finish it up.\n\n  if (n === 0 && state.ended) {\n    if (state.length === 0) endReadable(this);\n    return null;\n  } // All the actual chunk generation logic needs to be\n  // *below* the call to _read.  The reason is that in certain\n  // synthetic stream cases, such as passthrough streams, _read\n  // may be a completely synchronous operation which may change\n  // the state of the read buffer, providing enough data when\n  // before there was *not* enough.\n  //\n  // So, the steps are:\n  // 1. Figure out what the state of things will be after we do\n  // a read from the buffer.\n  //\n  // 2. If that resulting state will trigger a _read, then call _read.\n  // Note that this may be asynchronous, or synchronous.  Yes, it is\n  // deeply ugly to write APIs this way, but that still doesn't mean\n  // that the Readable class should behave improperly, as streams are\n  // designed to be sync/async agnostic.\n  // Take note if the _read call is sync or async (ie, if the read call\n  // has returned yet), so that we know whether or not it's safe to emit\n  // 'readable' etc.\n  //\n  // 3. Actually pull the requested chunks out of the buffer and return.\n  // if we need a readable event, then we need to do some reading.\n\n\n  var doRead = state.needReadable;\n  debug('need readable', doRead); // if we currently have less than the highWaterMark, then also read some\n\n  if (state.length === 0 || state.length - n < state.highWaterMark) {\n    doRead = true;\n    debug('length less than watermark', doRead);\n  } // however, if we've ended, then there's no point, and if we're already\n  // reading, then it's unnecessary.\n\n\n  if (state.ended || state.reading) {\n    doRead = false;\n    debug('reading or ended', doRead);\n  } else if (doRead) {\n    debug('do read');\n    state.reading = true;\n    state.sync = true; // if the length is currently zero, then we *need* a readable event.\n\n    if (state.length === 0) state.needReadable = true; // call internal read method\n\n    this._read(state.highWaterMark);\n\n    state.sync = false; // If _read pushed data synchronously, then `reading` will be false,\n    // and we need to re-evaluate how much data we can return to the user.\n\n    if (!state.reading) n = howMuchToRead(nOrig, state);\n  }\n\n  var ret;\n  if (n > 0) ret = fromList(n, state);else ret = null;\n\n  if (ret === null) {\n    state.needReadable = state.length <= state.highWaterMark;\n    n = 0;\n  } else {\n    state.length -= n;\n    state.awaitDrain = 0;\n  }\n\n  if (state.length === 0) {\n    // If we have nothing in the buffer, then we want to know\n    // as soon as we *do* get something into the buffer.\n    if (!state.ended) state.needReadable = true; // If we tried to read() past the EOF, then emit end on the next tick.\n\n    if (nOrig !== n && state.ended) endReadable(this);\n  }\n\n  if (ret !== null) this.emit('data', ret);\n  return ret;\n};\n\nfunction onEofChunk(stream, state) {\n  debug('onEofChunk');\n  if (state.ended) return;\n\n  if (state.decoder) {\n    var chunk = state.decoder.end();\n\n    if (chunk && chunk.length) {\n      state.buffer.push(chunk);\n      state.length += state.objectMode ? 1 : chunk.length;\n    }\n  }\n\n  state.ended = true;\n\n  if (state.sync) {\n    // if we are sync, wait until next tick to emit the data.\n    // Otherwise we risk emitting data in the flow()\n    // the readable code triggers during a read() call\n    emitReadable(stream);\n  } else {\n    // emit 'readable' now to make sure it gets picked up.\n    state.needReadable = false;\n\n    if (!state.emittedReadable) {\n      state.emittedReadable = true;\n      emitReadable_(stream);\n    }\n  }\n} // Don't emit readable right away in sync mode, because this can trigger\n// another read() call => stack overflow.  This way, it might trigger\n// a nextTick recursion warning, but that's not so bad.\n\n\nfunction emitReadable(stream) {\n  var state = stream._readableState;\n  debug('emitReadable', state.needReadable, state.emittedReadable);\n  state.needReadable = false;\n\n  if (!state.emittedReadable) {\n    debug('emitReadable', state.flowing);\n    state.emittedReadable = true;\n    process.nextTick(emitReadable_, stream);\n  }\n}\n\nfunction emitReadable_(stream) {\n  var state = stream._readableState;\n  debug('emitReadable_', state.destroyed, state.length, state.ended);\n\n  if (!state.destroyed && (state.length || state.ended)) {\n    stream.emit('readable');\n    state.emittedReadable = false;\n  } // The stream needs another readable event if\n  // 1. It is not flowing, as the flow mechanism will take\n  //    care of it.\n  // 2. It is not ended.\n  // 3. It is below the highWaterMark, so we can schedule\n  //    another readable later.\n\n\n  state.needReadable = !state.flowing && !state.ended && state.length <= state.highWaterMark;\n  flow(stream);\n} // at this point, the user has presumably seen the 'readable' event,\n// and called read() to consume some data.  that may have triggered\n// in turn another _read(n) call, in which case reading = true if\n// it's in progress.\n// However, if we're not ended, or reading, and the length < hwm,\n// then go ahead and try to read some more preemptively.\n\n\nfunction maybeReadMore(stream, state) {\n  if (!state.readingMore) {\n    state.readingMore = true;\n    process.nextTick(maybeReadMore_, stream, state);\n  }\n}\n\nfunction maybeReadMore_(stream, state) {\n  // Attempt to read more data if we should.\n  //\n  // The conditions for reading more data are (one of):\n  // - Not enough data buffered (state.length < state.highWaterMark). The loop\n  //   is responsible for filling the buffer with enough data if such data\n  //   is available. If highWaterMark is 0 and we are not in the flowing mode\n  //   we should _not_ attempt to buffer any extra data. We'll get more data\n  //   when the stream consumer calls read() instead.\n  // - No data in the buffer, and the stream is in flowing mode. In this mode\n  //   the loop below is responsible for ensuring read() is called. Failing to\n  //   call read here would abort the flow and there's no other mechanism for\n  //   continuing the flow if the stream consumer has just subscribed to the\n  //   'data' event.\n  //\n  // In addition to the above conditions to keep reading data, the following\n  // conditions prevent the data from being read:\n  // - The stream has ended (state.ended).\n  // - There is already a pending 'read' operation (state.reading). This is a\n  //   case where the the stream has called the implementation defined _read()\n  //   method, but they are processing the call asynchronously and have _not_\n  //   called push() with new data. In this case we skip performing more\n  //   read()s. The execution ends in this method again after the _read() ends\n  //   up calling push() with more data.\n  while (!state.reading && !state.ended && (state.length < state.highWaterMark || state.flowing && state.length === 0)) {\n    var len = state.length;\n    debug('maybeReadMore read 0');\n    stream.read(0);\n    if (len === state.length) // didn't get any data, stop spinning.\n      break;\n  }\n\n  state.readingMore = false;\n} // abstract method.  to be overridden in specific implementation classes.\n// call cb(er, data) where data is <= n in length.\n// for virtual (non-string, non-buffer) streams, \"length\" is somewhat\n// arbitrary, and perhaps not very meaningful.\n\n\nReadable.prototype._read = function (n) {\n  errorOrDestroy(this, new ERR_METHOD_NOT_IMPLEMENTED('_read()'));\n};\n\nReadable.prototype.pipe = function (dest, pipeOpts) {\n  var src = this;\n  var state = this._readableState;\n\n  switch (state.pipesCount) {\n    case 0:\n      state.pipes = dest;\n      break;\n\n    case 1:\n      state.pipes = [state.pipes, dest];\n      break;\n\n    default:\n      state.pipes.push(dest);\n      break;\n  }\n\n  state.pipesCount += 1;\n  debug('pipe count=%d opts=%j', state.pipesCount, pipeOpts);\n  var doEnd = (!pipeOpts || pipeOpts.end !== false) && dest !== process.stdout && dest !== process.stderr;\n  var endFn = doEnd ? onend : unpipe;\n  if (state.endEmitted) process.nextTick(endFn);else src.once('end', endFn);\n  dest.on('unpipe', onunpipe);\n\n  function onunpipe(readable, unpipeInfo) {\n    debug('onunpipe');\n\n    if (readable === src) {\n      if (unpipeInfo && unpipeInfo.hasUnpiped === false) {\n        unpipeInfo.hasUnpiped = true;\n        cleanup();\n      }\n    }\n  }\n\n  function onend() {\n    debug('onend');\n    dest.end();\n  } // when the dest drains, it reduces the awaitDrain counter\n  // on the source.  This would be more elegant with a .once()\n  // handler in flow(), but adding and removing repeatedly is\n  // too slow.\n\n\n  var ondrain = pipeOnDrain(src);\n  dest.on('drain', ondrain);\n  var cleanedUp = false;\n\n  function cleanup() {\n    debug('cleanup'); // cleanup event handlers once the pipe is broken\n\n    dest.removeListener('close', onclose);\n    dest.removeListener('finish', onfinish);\n    dest.removeListener('drain', ondrain);\n    dest.removeListener('error', onerror);\n    dest.removeListener('unpipe', onunpipe);\n    src.removeListener('end', onend);\n    src.removeListener('end', unpipe);\n    src.removeListener('data', ondata);\n    cleanedUp = true; // if the reader is waiting for a drain event from this\n    // specific writer, then it would cause it to never start\n    // flowing again.\n    // So, if this is awaiting a drain, then we just call it now.\n    // If we don't know, then assume that we are waiting for one.\n\n    if (state.awaitDrain && (!dest._writableState || dest._writableState.needDrain)) ondrain();\n  }\n\n  src.on('data', ondata);\n\n  function ondata(chunk) {\n    debug('ondata');\n    var ret = dest.write(chunk);\n    debug('dest.write', ret);\n\n    if (ret === false) {\n      // If the user unpiped during `dest.write()`, it is possible\n      // to get stuck in a permanently paused state if that write\n      // also returned false.\n      // => Check whether `dest` is still a piping destination.\n      if ((state.pipesCount === 1 && state.pipes === dest || state.pipesCount > 1 && indexOf(state.pipes, dest) !== -1) && !cleanedUp) {\n        debug('false write response, pause', state.awaitDrain);\n        state.awaitDrain++;\n      }\n\n      src.pause();\n    }\n  } // if the dest has an error, then stop piping into it.\n  // however, don't suppress the throwing behavior for this.\n\n\n  function onerror(er) {\n    debug('onerror', er);\n    unpipe();\n    dest.removeListener('error', onerror);\n    if (EElistenerCount(dest, 'error') === 0) errorOrDestroy(dest, er);\n  } // Make sure our error handler is attached before userland ones.\n\n\n  prependListener(dest, 'error', onerror); // Both close and finish should trigger unpipe, but only once.\n\n  function onclose() {\n    dest.removeListener('finish', onfinish);\n    unpipe();\n  }\n\n  dest.once('close', onclose);\n\n  function onfinish() {\n    debug('onfinish');\n    dest.removeListener('close', onclose);\n    unpipe();\n  }\n\n  dest.once('finish', onfinish);\n\n  function unpipe() {\n    debug('unpipe');\n    src.unpipe(dest);\n  } // tell the dest that it's being piped to\n\n\n  dest.emit('pipe', src); // start the flow if it hasn't been started already.\n\n  if (!state.flowing) {\n    debug('pipe resume');\n    src.resume();\n  }\n\n  return dest;\n};\n\nfunction pipeOnDrain(src) {\n  return function pipeOnDrainFunctionResult() {\n    var state = src._readableState;\n    debug('pipeOnDrain', state.awaitDrain);\n    if (state.awaitDrain) state.awaitDrain--;\n\n    if (state.awaitDrain === 0 && EElistenerCount(src, 'data')) {\n      state.flowing = true;\n      flow(src);\n    }\n  };\n}\n\nReadable.prototype.unpipe = function (dest) {\n  var state = this._readableState;\n  var unpipeInfo = {\n    hasUnpiped: false\n  }; // if we're not piping anywhere, then do nothing.\n\n  if (state.pipesCount === 0) return this; // just one destination.  most common case.\n\n  if (state.pipesCount === 1) {\n    // passed in one, but it's not the right one.\n    if (dest && dest !== state.pipes) return this;\n    if (!dest) dest = state.pipes; // got a match.\n\n    state.pipes = null;\n    state.pipesCount = 0;\n    state.flowing = false;\n    if (dest) dest.emit('unpipe', this, unpipeInfo);\n    return this;\n  } // slow case. multiple pipe destinations.\n\n\n  if (!dest) {\n    // remove all.\n    var dests = state.pipes;\n    var len = state.pipesCount;\n    state.pipes = null;\n    state.pipesCount = 0;\n    state.flowing = false;\n\n    for (var i = 0; i < len; i++) {\n      dests[i].emit('unpipe', this, {\n        hasUnpiped: false\n      });\n    }\n\n    return this;\n  } // try to find the right one.\n\n\n  var index = indexOf(state.pipes, dest);\n  if (index === -1) return this;\n  state.pipes.splice(index, 1);\n  state.pipesCount -= 1;\n  if (state.pipesCount === 1) state.pipes = state.pipes[0];\n  dest.emit('unpipe', this, unpipeInfo);\n  return this;\n}; // set up data events if they are asked for\n// Ensure readable listeners eventually get something\n\n\nReadable.prototype.on = function (ev, fn) {\n  var res = Stream.prototype.on.call(this, ev, fn);\n  var state = this._readableState;\n\n  if (ev === 'data') {\n    // update readableListening so that resume() may be a no-op\n    // a few lines down. This is needed to support once('readable').\n    state.readableListening = this.listenerCount('readable') > 0; // Try start flowing on next tick if stream isn't explicitly paused\n\n    if (state.flowing !== false) this.resume();\n  } else if (ev === 'readable') {\n    if (!state.endEmitted && !state.readableListening) {\n      state.readableListening = state.needReadable = true;\n      state.flowing = false;\n      state.emittedReadable = false;\n      debug('on readable', state.length, state.reading);\n\n      if (state.length) {\n        emitReadable(this);\n      } else if (!state.reading) {\n        process.nextTick(nReadingNextTick, this);\n      }\n    }\n  }\n\n  return res;\n};\n\nReadable.prototype.addListener = Readable.prototype.on;\n\nReadable.prototype.removeListener = function (ev, fn) {\n  var res = Stream.prototype.removeListener.call(this, ev, fn);\n\n  if (ev === 'readable') {\n    // We need to check if there is someone still listening to\n    // readable and reset the state. However this needs to happen\n    // after readable has been emitted but before I/O (nextTick) to\n    // support once('readable', fn) cycles. This means that calling\n    // resume within the same tick will have no\n    // effect.\n    process.nextTick(updateReadableListening, this);\n  }\n\n  return res;\n};\n\nReadable.prototype.removeAllListeners = function (ev) {\n  var res = Stream.prototype.removeAllListeners.apply(this, arguments);\n\n  if (ev === 'readable' || ev === undefined) {\n    // We need to check if there is someone still listening to\n    // readable and reset the state. However this needs to happen\n    // after readable has been emitted but before I/O (nextTick) to\n    // support once('readable', fn) cycles. This means that calling\n    // resume within the same tick will have no\n    // effect.\n    process.nextTick(updateReadableListening, this);\n  }\n\n  return res;\n};\n\nfunction updateReadableListening(self) {\n  var state = self._readableState;\n  state.readableListening = self.listenerCount('readable') > 0;\n\n  if (state.resumeScheduled && !state.paused) {\n    // flowing needs to be set to true now, otherwise\n    // the upcoming resume will not flow.\n    state.flowing = true; // crude way to check if we should resume\n  } else if (self.listenerCount('data') > 0) {\n    self.resume();\n  }\n}\n\nfunction nReadingNextTick(self) {\n  debug('readable nexttick read 0');\n  self.read(0);\n} // pause() and resume() are remnants of the legacy readable stream API\n// If the user uses them, then switch into old mode.\n\n\nReadable.prototype.resume = function () {\n  var state = this._readableState;\n\n  if (!state.flowing) {\n    debug('resume'); // we flow only if there is no one listening\n    // for readable, but we still have to call\n    // resume()\n\n    state.flowing = !state.readableListening;\n    resume(this, state);\n  }\n\n  state.paused = false;\n  return this;\n};\n\nfunction resume(stream, state) {\n  if (!state.resumeScheduled) {\n    state.resumeScheduled = true;\n    process.nextTick(resume_, stream, state);\n  }\n}\n\nfunction resume_(stream, state) {\n  debug('resume', state.reading);\n\n  if (!state.reading) {\n    stream.read(0);\n  }\n\n  state.resumeScheduled = false;\n  stream.emit('resume');\n  flow(stream);\n  if (state.flowing && !state.reading) stream.read(0);\n}\n\nReadable.prototype.pause = function () {\n  debug('call pause flowing=%j', this._readableState.flowing);\n\n  if (this._readableState.flowing !== false) {\n    debug('pause');\n    this._readableState.flowing = false;\n    this.emit('pause');\n  }\n\n  this._readableState.paused = true;\n  return this;\n};\n\nfunction flow(stream) {\n  var state = stream._readableState;\n  debug('flow', state.flowing);\n\n  while (state.flowing && stream.read() !== null) {\n    ;\n  }\n} // wrap an old-style stream as the async data source.\n// This is *not* part of the readable stream interface.\n// It is an ugly unfortunate mess of history.\n\n\nReadable.prototype.wrap = function (stream) {\n  var _this = this;\n\n  var state = this._readableState;\n  var paused = false;\n  stream.on('end', function () {\n    debug('wrapped end');\n\n    if (state.decoder && !state.ended) {\n      var chunk = state.decoder.end();\n      if (chunk && chunk.length) _this.push(chunk);\n    }\n\n    _this.push(null);\n  });\n  stream.on('data', function (chunk) {\n    debug('wrapped data');\n    if (state.decoder) chunk = state.decoder.write(chunk); // don't skip over falsy values in objectMode\n\n    if (state.objectMode && (chunk === null || chunk === undefined)) return;else if (!state.objectMode && (!chunk || !chunk.length)) return;\n\n    var ret = _this.push(chunk);\n\n    if (!ret) {\n      paused = true;\n      stream.pause();\n    }\n  }); // proxy all the other methods.\n  // important when wrapping filters and duplexes.\n\n  for (var i in stream) {\n    if (this[i] === undefined && typeof stream[i] === 'function') {\n      this[i] = function methodWrap(method) {\n        return function methodWrapReturnFunction() {\n          return stream[method].apply(stream, arguments);\n        };\n      }(i);\n    }\n  } // proxy certain important events.\n\n\n  for (var n = 0; n < kProxyEvents.length; n++) {\n    stream.on(kProxyEvents[n], this.emit.bind(this, kProxyEvents[n]));\n  } // when we try to consume some more bytes, simply unpause the\n  // underlying stream.\n\n\n  this._read = function (n) {\n    debug('wrapped _read', n);\n\n    if (paused) {\n      paused = false;\n      stream.resume();\n    }\n  };\n\n  return this;\n};\n\nif (typeof Symbol === 'function') {\n  Readable.prototype[Symbol.asyncIterator] = function () {\n    if (createReadableStreamAsyncIterator === undefined) {\n      createReadableStreamAsyncIterator = require('./internal/streams/async_iterator');\n    }\n\n    return createReadableStreamAsyncIterator(this);\n  };\n}\n\nObject.defineProperty(Readable.prototype, 'readableHighWaterMark', {\n  // making it explicit this property is not enumerable\n  // because otherwise some prototype manipulation in\n  // userland will fail\n  enumerable: false,\n  get: function get() {\n    return this._readableState.highWaterMark;\n  }\n});\nObject.defineProperty(Readable.prototype, 'readableBuffer', {\n  // making it explicit this property is not enumerable\n  // because otherwise some prototype manipulation in\n  // userland will fail\n  enumerable: false,\n  get: function get() {\n    return this._readableState && this._readableState.buffer;\n  }\n});\nObject.defineProperty(Readable.prototype, 'readableFlowing', {\n  // making it explicit this property is not enumerable\n  // because otherwise some prototype manipulation in\n  // userland will fail\n  enumerable: false,\n  get: function get() {\n    return this._readableState.flowing;\n  },\n  set: function set(state) {\n    if (this._readableState) {\n      this._readableState.flowing = state;\n    }\n  }\n}); // exposed for testing purposes only.\n\nReadable._fromList = fromList;\nObject.defineProperty(Readable.prototype, 'readableLength', {\n  // making it explicit this property is not enumerable\n  // because otherwise some prototype manipulation in\n  // userland will fail\n  enumerable: false,\n  get: function get() {\n    return this._readableState.length;\n  }\n}); // Pluck off n bytes from an array of buffers.\n// Length is the combined lengths of all the buffers in the list.\n// This function is designed to be inlinable, so please take care when making\n// changes to the function body.\n\nfunction fromList(n, state) {\n  // nothing buffered\n  if (state.length === 0) return null;\n  var ret;\n  if (state.objectMode) ret = state.buffer.shift();else if (!n || n >= state.length) {\n    // read it all, truncate the list\n    if (state.decoder) ret = state.buffer.join('');else if (state.buffer.length === 1) ret = state.buffer.first();else ret = state.buffer.concat(state.length);\n    state.buffer.clear();\n  } else {\n    // read part of list\n    ret = state.buffer.consume(n, state.decoder);\n  }\n  return ret;\n}\n\nfunction endReadable(stream) {\n  var state = stream._readableState;\n  debug('endReadable', state.endEmitted);\n\n  if (!state.endEmitted) {\n    state.ended = true;\n    process.nextTick(endReadableNT, state, stream);\n  }\n}\n\nfunction endReadableNT(state, stream) {\n  debug('endReadableNT', state.endEmitted, state.length); // Check that we didn't get one last unshift.\n\n  if (!state.endEmitted && state.length === 0) {\n    state.endEmitted = true;\n    stream.readable = false;\n    stream.emit('end');\n\n    if (state.autoDestroy) {\n      // In case of duplex streams we need a way to detect\n      // if the writable side is ready for autoDestroy as well\n      var wState = stream._writableState;\n\n      if (!wState || wState.autoDestroy && wState.finished) {\n        stream.destroy();\n      }\n    }\n  }\n}\n\nif (typeof Symbol === 'function') {\n  Readable.from = function (iterable, opts) {\n    if (from === undefined) {\n      from = require('./internal/streams/from');\n    }\n\n    return from(Readable, iterable, opts);\n  };\n}\n\nfunction indexOf(xs, x) {\n  for (var i = 0, l = xs.length; i < l; i++) {\n    if (xs[i] === x) return i;\n  }\n\n  return -1;\n}","module.exports = require('events').EventEmitter;\n","'use strict'; // undocumented cb() API, needed for core, not for public API\n\nfunction destroy(err, cb) {\n  var _this = this;\n\n  var readableDestroyed = this._readableState && this._readableState.destroyed;\n  var writableDestroyed = this._writableState && this._writableState.destroyed;\n\n  if (readableDestroyed || writableDestroyed) {\n    if (cb) {\n      cb(err);\n    } else if (err) {\n      if (!this._writableState) {\n        process.nextTick(emitErrorNT, this, err);\n      } else if (!this._writableState.errorEmitted) {\n        this._writableState.errorEmitted = true;\n        process.nextTick(emitErrorNT, this, err);\n      }\n    }\n\n    return this;\n  } // we set destroyed to true before firing error callbacks in order\n  // to make it re-entrance safe in case destroy() is called within callbacks\n\n\n  if (this._readableState) {\n    this._readableState.destroyed = true;\n  } // if this is a duplex stream mark the writable part as destroyed as well\n\n\n  if (this._writableState) {\n    this._writableState.destroyed = true;\n  }\n\n  this._destroy(err || null, function (err) {\n    if (!cb && err) {\n      if (!_this._writableState) {\n        process.nextTick(emitErrorAndCloseNT, _this, err);\n      } else if (!_this._writableState.errorEmitted) {\n        _this._writableState.errorEmitted = true;\n        process.nextTick(emitErrorAndCloseNT, _this, err);\n      } else {\n        process.nextTick(emitCloseNT, _this);\n      }\n    } else if (cb) {\n      process.nextTick(emitCloseNT, _this);\n      cb(err);\n    } else {\n      process.nextTick(emitCloseNT, _this);\n    }\n  });\n\n  return this;\n}\n\nfunction emitErrorAndCloseNT(self, err) {\n  emitErrorNT(self, err);\n  emitCloseNT(self);\n}\n\nfunction emitCloseNT(self) {\n  if (self._writableState && !self._writableState.emitClose) return;\n  if (self._readableState && !self._readableState.emitClose) return;\n  self.emit('close');\n}\n\nfunction undestroy() {\n  if (this._readableState) {\n    this._readableState.destroyed = false;\n    this._readableState.reading = false;\n    this._readableState.ended = false;\n    this._readableState.endEmitted = false;\n  }\n\n  if (this._writableState) {\n    this._writableState.destroyed = false;\n    this._writableState.ended = false;\n    this._writableState.ending = false;\n    this._writableState.finalCalled = false;\n    this._writableState.prefinished = false;\n    this._writableState.finished = false;\n    this._writableState.errorEmitted = false;\n  }\n}\n\nfunction emitErrorNT(self, err) {\n  self.emit('error', err);\n}\n\nfunction errorOrDestroy(stream, err) {\n  // We have tests that rely on errors being emitted\n  // in the same tick, so changing this is semver major.\n  // For now when you opt-in to autoDestroy we allow\n  // the error to be emitted nextTick. In a future\n  // semver major update we should change the default to this.\n  var rState = stream._readableState;\n  var wState = stream._writableState;\n  if (rState && rState.autoDestroy || wState && wState.autoDestroy) stream.destroy(err);else stream.emit('error', err);\n}\n\nmodule.exports = {\n  destroy: destroy,\n  undestroy: undestroy,\n  errorOrDestroy: errorOrDestroy\n};","'use strict';\n\nvar ERR_INVALID_OPT_VALUE = require('../../../errors').codes.ERR_INVALID_OPT_VALUE;\n\nfunction highWaterMarkFrom(options, isDuplex, duplexKey) {\n  return options.highWaterMark != null ? options.highWaterMark : isDuplex ? options[duplexKey] : null;\n}\n\nfunction getHighWaterMark(state, options, duplexKey, isDuplex) {\n  var hwm = highWaterMarkFrom(options, isDuplex, duplexKey);\n\n  if (hwm != null) {\n    if (!(isFinite(hwm) && Math.floor(hwm) === hwm) || hwm < 0) {\n      var name = isDuplex ? duplexKey : 'highWaterMark';\n      throw new ERR_INVALID_OPT_VALUE(name, hwm);\n    }\n\n    return Math.floor(hwm);\n  } // Default value\n\n\n  return state.objectMode ? 16 : 16 * 1024;\n}\n\nmodule.exports = {\n  getHighWaterMark: getHighWaterMark\n};","// Copyright Joyent, Inc. and other Node contributors.\n//\n// Permission is hereby granted, free of charge, to any person obtaining a\n// copy of this software and associated documentation files (the\n// \"Software\"), to deal in the Software without restriction, including\n// without limitation the rights to use, copy, modify, merge, publish,\n// distribute, sublicense, and/or sell copies of the Software, and to permit\n// persons to whom the Software is furnished to do so, subject to the\n// following conditions:\n//\n// The above copyright notice and this permission notice shall be included\n// in all copies or substantial portions of the Software.\n//\n// THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS\n// OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n// MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN\n// NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n// DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR\n// OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE\n// USE OR OTHER DEALINGS IN THE SOFTWARE.\n// A bit simpler than readable streams.\n// Implement an async ._write(chunk, encoding, cb), and it'll handle all\n// the drain event emission and buffering.\n'use strict';\n\nmodule.exports = Writable;\n/* <replacement> */\n\nfunction WriteReq(chunk, encoding, cb) {\n  this.chunk = chunk;\n  this.encoding = encoding;\n  this.callback = cb;\n  this.next = null;\n} // It seems a linked list but it is not\n// there will be only 2 of these for each stream\n\n\nfunction CorkedRequest(state) {\n  var _this = this;\n\n  this.next = null;\n  this.entry = null;\n\n  this.finish = function () {\n    onCorkedFinish(_this, state);\n  };\n}\n/* </replacement> */\n\n/*<replacement>*/\n\n\nvar Duplex;\n/*</replacement>*/\n\nWritable.WritableState = WritableState;\n/*<replacement>*/\n\nvar internalUtil = {\n  deprecate: require('util-deprecate')\n};\n/*</replacement>*/\n\n/*<replacement>*/\n\nvar Stream = require('./internal/streams/stream');\n/*</replacement>*/\n\n\nvar Buffer = require('buffer').Buffer;\n\nvar OurUint8Array = global.Uint8Array || function () {};\n\nfunction _uint8ArrayToBuffer(chunk) {\n  return Buffer.from(chunk);\n}\n\nfunction _isUint8Array(obj) {\n  return Buffer.isBuffer(obj) || obj instanceof OurUint8Array;\n}\n\nvar destroyImpl = require('./internal/streams/destroy');\n\nvar _require = require('./internal/streams/state'),\n    getHighWaterMark = _require.getHighWaterMark;\n\nvar _require$codes = require('../errors').codes,\n    ERR_INVALID_ARG_TYPE = _require$codes.ERR_INVALID_ARG_TYPE,\n    ERR_METHOD_NOT_IMPLEMENTED = _require$codes.ERR_METHOD_NOT_IMPLEMENTED,\n    ERR_MULTIPLE_CALLBACK = _require$codes.ERR_MULTIPLE_CALLBACK,\n    ERR_STREAM_CANNOT_PIPE = _require$codes.ERR_STREAM_CANNOT_PIPE,\n    ERR_STREAM_DESTROYED = _require$codes.ERR_STREAM_DESTROYED,\n    ERR_STREAM_NULL_VALUES = _require$codes.ERR_STREAM_NULL_VALUES,\n    ERR_STREAM_WRITE_AFTER_END = _require$codes.ERR_STREAM_WRITE_AFTER_END,\n    ERR_UNKNOWN_ENCODING = _require$codes.ERR_UNKNOWN_ENCODING;\n\nvar errorOrDestroy = destroyImpl.errorOrDestroy;\n\nrequire('inherits')(Writable, Stream);\n\nfunction nop() {}\n\nfunction WritableState(options, stream, isDuplex) {\n  Duplex = Duplex || require('./_stream_duplex');\n  options = options || {}; // Duplex streams are both readable and writable, but share\n  // the same options object.\n  // However, some cases require setting options to different\n  // values for the readable and the writable sides of the duplex stream,\n  // e.g. options.readableObjectMode vs. options.writableObjectMode, etc.\n\n  if (typeof isDuplex !== 'boolean') isDuplex = stream instanceof Duplex; // object stream flag to indicate whether or not this stream\n  // contains buffers or objects.\n\n  this.objectMode = !!options.objectMode;\n  if (isDuplex) this.objectMode = this.objectMode || !!options.writableObjectMode; // the point at which write() starts returning false\n  // Note: 0 is a valid value, means that we always return false if\n  // the entire buffer is not flushed immediately on write()\n\n  this.highWaterMark = getHighWaterMark(this, options, 'writableHighWaterMark', isDuplex); // if _final has been called\n\n  this.finalCalled = false; // drain event flag.\n\n  this.needDrain = false; // at the start of calling end()\n\n  this.ending = false; // when end() has been called, and returned\n\n  this.ended = false; // when 'finish' is emitted\n\n  this.finished = false; // has it been destroyed\n\n  this.destroyed = false; // should we decode strings into buffers before passing to _write?\n  // this is here so that some node-core streams can optimize string\n  // handling at a lower level.\n\n  var noDecode = options.decodeStrings === false;\n  this.decodeStrings = !noDecode; // Crypto is kind of old and crusty.  Historically, its default string\n  // encoding is 'binary' so we have to make this configurable.\n  // Everything else in the universe uses 'utf8', though.\n\n  this.defaultEncoding = options.defaultEncoding || 'utf8'; // not an actual buffer we keep track of, but a measurement\n  // of how much we're waiting to get pushed to some underlying\n  // socket or file.\n\n  this.length = 0; // a flag to see when we're in the middle of a write.\n\n  this.writing = false; // when true all writes will be buffered until .uncork() call\n\n  this.corked = 0; // a flag to be able to tell if the onwrite cb is called immediately,\n  // or on a later tick.  We set this to true at first, because any\n  // actions that shouldn't happen until \"later\" should generally also\n  // not happen before the first write call.\n\n  this.sync = true; // a flag to know if we're processing previously buffered items, which\n  // may call the _write() callback in the same tick, so that we don't\n  // end up in an overlapped onwrite situation.\n\n  this.bufferProcessing = false; // the callback that's passed to _write(chunk,cb)\n\n  this.onwrite = function (er) {\n    onwrite(stream, er);\n  }; // the callback that the user supplies to write(chunk,encoding,cb)\n\n\n  this.writecb = null; // the amount that is being written when _write is called.\n\n  this.writelen = 0;\n  this.bufferedRequest = null;\n  this.lastBufferedRequest = null; // number of pending user-supplied write callbacks\n  // this must be 0 before 'finish' can be emitted\n\n  this.pendingcb = 0; // emit prefinish if the only thing we're waiting for is _write cbs\n  // This is relevant for synchronous Transform streams\n\n  this.prefinished = false; // True if the error was already emitted and should not be thrown again\n\n  this.errorEmitted = false; // Should close be emitted on destroy. Defaults to true.\n\n  this.emitClose = options.emitClose !== false; // Should .destroy() be called after 'finish' (and potentially 'end')\n\n  this.autoDestroy = !!options.autoDestroy; // count buffered requests\n\n  this.bufferedRequestCount = 0; // allocate the first CorkedRequest, there is always\n  // one allocated and free to use, and we maintain at most two\n\n  this.corkedRequestsFree = new CorkedRequest(this);\n}\n\nWritableState.prototype.getBuffer = function getBuffer() {\n  var current = this.bufferedRequest;\n  var out = [];\n\n  while (current) {\n    out.push(current);\n    current = current.next;\n  }\n\n  return out;\n};\n\n(function () {\n  try {\n    Object.defineProperty(WritableState.prototype, 'buffer', {\n      get: internalUtil.deprecate(function writableStateBufferGetter() {\n        return this.getBuffer();\n      }, '_writableState.buffer is deprecated. Use _writableState.getBuffer ' + 'instead.', 'DEP0003')\n    });\n  } catch (_) {}\n})(); // Test _writableState for inheritance to account for Duplex streams,\n// whose prototype chain only points to Readable.\n\n\nvar realHasInstance;\n\nif (typeof Symbol === 'function' && Symbol.hasInstance && typeof Function.prototype[Symbol.hasInstance] === 'function') {\n  realHasInstance = Function.prototype[Symbol.hasInstance];\n  Object.defineProperty(Writable, Symbol.hasInstance, {\n    value: function value(object) {\n      if (realHasInstance.call(this, object)) return true;\n      if (this !== Writable) return false;\n      return object && object._writableState instanceof WritableState;\n    }\n  });\n} else {\n  realHasInstance = function realHasInstance(object) {\n    return object instanceof this;\n  };\n}\n\nfunction Writable(options) {\n  Duplex = Duplex || require('./_stream_duplex'); // Writable ctor is applied to Duplexes, too.\n  // `realHasInstance` is necessary because using plain `instanceof`\n  // would return false, as no `_writableState` property is attached.\n  // Trying to use the custom `instanceof` for Writable here will also break the\n  // Node.js LazyTransform implementation, which has a non-trivial getter for\n  // `_writableState` that would lead to infinite recursion.\n  // Checking for a Stream.Duplex instance is faster here instead of inside\n  // the WritableState constructor, at least with V8 6.5\n\n  var isDuplex = this instanceof Duplex;\n  if (!isDuplex && !realHasInstance.call(Writable, this)) return new Writable(options);\n  this._writableState = new WritableState(options, this, isDuplex); // legacy.\n\n  this.writable = true;\n\n  if (options) {\n    if (typeof options.write === 'function') this._write = options.write;\n    if (typeof options.writev === 'function') this._writev = options.writev;\n    if (typeof options.destroy === 'function') this._destroy = options.destroy;\n    if (typeof options.final === 'function') this._final = options.final;\n  }\n\n  Stream.call(this);\n} // Otherwise people can pipe Writable streams, which is just wrong.\n\n\nWritable.prototype.pipe = function () {\n  errorOrDestroy(this, new ERR_STREAM_CANNOT_PIPE());\n};\n\nfunction writeAfterEnd(stream, cb) {\n  var er = new ERR_STREAM_WRITE_AFTER_END(); // TODO: defer error events consistently everywhere, not just the cb\n\n  errorOrDestroy(stream, er);\n  process.nextTick(cb, er);\n} // Checks that a user-supplied chunk is valid, especially for the particular\n// mode the stream is in. Currently this means that `null` is never accepted\n// and undefined/non-string values are only allowed in object mode.\n\n\nfunction validChunk(stream, state, chunk, cb) {\n  var er;\n\n  if (chunk === null) {\n    er = new ERR_STREAM_NULL_VALUES();\n  } else if (typeof chunk !== 'string' && !state.objectMode) {\n    er = new ERR_INVALID_ARG_TYPE('chunk', ['string', 'Buffer'], chunk);\n  }\n\n  if (er) {\n    errorOrDestroy(stream, er);\n    process.nextTick(cb, er);\n    return false;\n  }\n\n  return true;\n}\n\nWritable.prototype.write = function (chunk, encoding, cb) {\n  var state = this._writableState;\n  var ret = false;\n\n  var isBuf = !state.objectMode && _isUint8Array(chunk);\n\n  if (isBuf && !Buffer.isBuffer(chunk)) {\n    chunk = _uint8ArrayToBuffer(chunk);\n  }\n\n  if (typeof encoding === 'function') {\n    cb = encoding;\n    encoding = null;\n  }\n\n  if (isBuf) encoding = 'buffer';else if (!encoding) encoding = state.defaultEncoding;\n  if (typeof cb !== 'function') cb = nop;\n  if (state.ending) writeAfterEnd(this, cb);else if (isBuf || validChunk(this, state, chunk, cb)) {\n    state.pendingcb++;\n    ret = writeOrBuffer(this, state, isBuf, chunk, encoding, cb);\n  }\n  return ret;\n};\n\nWritable.prototype.cork = function () {\n  this._writableState.corked++;\n};\n\nWritable.prototype.uncork = function () {\n  var state = this._writableState;\n\n  if (state.corked) {\n    state.corked--;\n    if (!state.writing && !state.corked && !state.bufferProcessing && state.bufferedRequest) clearBuffer(this, state);\n  }\n};\n\nWritable.prototype.setDefaultEncoding = function setDefaultEncoding(encoding) {\n  // node::ParseEncoding() requires lower case.\n  if (typeof encoding === 'string') encoding = encoding.toLowerCase();\n  if (!(['hex', 'utf8', 'utf-8', 'ascii', 'binary', 'base64', 'ucs2', 'ucs-2', 'utf16le', 'utf-16le', 'raw'].indexOf((encoding + '').toLowerCase()) > -1)) throw new ERR_UNKNOWN_ENCODING(encoding);\n  this._writableState.defaultEncoding = encoding;\n  return this;\n};\n\nObject.defineProperty(Writable.prototype, 'writableBuffer', {\n  // making it explicit this property is not enumerable\n  // because otherwise some prototype manipulation in\n  // userland will fail\n  enumerable: false,\n  get: function get() {\n    return this._writableState && this._writableState.getBuffer();\n  }\n});\n\nfunction decodeChunk(state, chunk, encoding) {\n  if (!state.objectMode && state.decodeStrings !== false && typeof chunk === 'string') {\n    chunk = Buffer.from(chunk, encoding);\n  }\n\n  return chunk;\n}\n\nObject.defineProperty(Writable.prototype, 'writableHighWaterMark', {\n  // making it explicit this property is not enumerable\n  // because otherwise some prototype manipulation in\n  // userland will fail\n  enumerable: false,\n  get: function get() {\n    return this._writableState.highWaterMark;\n  }\n}); // if we're already writing something, then just put this\n// in the queue, and wait our turn.  Otherwise, call _write\n// If we return false, then we need a drain event, so set that flag.\n\nfunction writeOrBuffer(stream, state, isBuf, chunk, encoding, cb) {\n  if (!isBuf) {\n    var newChunk = decodeChunk(state, chunk, encoding);\n\n    if (chunk !== newChunk) {\n      isBuf = true;\n      encoding = 'buffer';\n      chunk = newChunk;\n    }\n  }\n\n  var len = state.objectMode ? 1 : chunk.length;\n  state.length += len;\n  var ret = state.length < state.highWaterMark; // we must ensure that previous needDrain will not be reset to false.\n\n  if (!ret) state.needDrain = true;\n\n  if (state.writing || state.corked) {\n    var last = state.lastBufferedRequest;\n    state.lastBufferedRequest = {\n      chunk: chunk,\n      encoding: encoding,\n      isBuf: isBuf,\n      callback: cb,\n      next: null\n    };\n\n    if (last) {\n      last.next = state.lastBufferedRequest;\n    } else {\n      state.bufferedRequest = state.lastBufferedRequest;\n    }\n\n    state.bufferedRequestCount += 1;\n  } else {\n    doWrite(stream, state, false, len, chunk, encoding, cb);\n  }\n\n  return ret;\n}\n\nfunction doWrite(stream, state, writev, len, chunk, encoding, cb) {\n  state.writelen = len;\n  state.writecb = cb;\n  state.writing = true;\n  state.sync = true;\n  if (state.destroyed) state.onwrite(new ERR_STREAM_DESTROYED('write'));else if (writev) stream._writev(chunk, state.onwrite);else stream._write(chunk, encoding, state.onwrite);\n  state.sync = false;\n}\n\nfunction onwriteError(stream, state, sync, er, cb) {\n  --state.pendingcb;\n\n  if (sync) {\n    // defer the callback if we are being called synchronously\n    // to avoid piling up things on the stack\n    process.nextTick(cb, er); // this can emit finish, and it will always happen\n    // after error\n\n    process.nextTick(finishMaybe, stream, state);\n    stream._writableState.errorEmitted = true;\n    errorOrDestroy(stream, er);\n  } else {\n    // the caller expect this to happen before if\n    // it is async\n    cb(er);\n    stream._writableState.errorEmitted = true;\n    errorOrDestroy(stream, er); // this can emit finish, but finish must\n    // always follow error\n\n    finishMaybe(stream, state);\n  }\n}\n\nfunction onwriteStateUpdate(state) {\n  state.writing = false;\n  state.writecb = null;\n  state.length -= state.writelen;\n  state.writelen = 0;\n}\n\nfunction onwrite(stream, er) {\n  var state = stream._writableState;\n  var sync = state.sync;\n  var cb = state.writecb;\n  if (typeof cb !== 'function') throw new ERR_MULTIPLE_CALLBACK();\n  onwriteStateUpdate(state);\n  if (er) onwriteError(stream, state, sync, er, cb);else {\n    // Check if we're actually ready to finish, but don't emit yet\n    var finished = needFinish(state) || stream.destroyed;\n\n    if (!finished && !state.corked && !state.bufferProcessing && state.bufferedRequest) {\n      clearBuffer(stream, state);\n    }\n\n    if (sync) {\n      process.nextTick(afterWrite, stream, state, finished, cb);\n    } else {\n      afterWrite(stream, state, finished, cb);\n    }\n  }\n}\n\nfunction afterWrite(stream, state, finished, cb) {\n  if (!finished) onwriteDrain(stream, state);\n  state.pendingcb--;\n  cb();\n  finishMaybe(stream, state);\n} // Must force callback to be called on nextTick, so that we don't\n// emit 'drain' before the write() consumer gets the 'false' return\n// value, and has a chance to attach a 'drain' listener.\n\n\nfunction onwriteDrain(stream, state) {\n  if (state.length === 0 && state.needDrain) {\n    state.needDrain = false;\n    stream.emit('drain');\n  }\n} // if there's something in the buffer waiting, then process it\n\n\nfunction clearBuffer(stream, state) {\n  state.bufferProcessing = true;\n  var entry = state.bufferedRequest;\n\n  if (stream._writev && entry && entry.next) {\n    // Fast case, write everything using _writev()\n    var l = state.bufferedRequestCount;\n    var buffer = new Array(l);\n    var holder = state.corkedRequestsFree;\n    holder.entry = entry;\n    var count = 0;\n    var allBuffers = true;\n\n    while (entry) {\n      buffer[count] = entry;\n      if (!entry.isBuf) allBuffers = false;\n      entry = entry.next;\n      count += 1;\n    }\n\n    buffer.allBuffers = allBuffers;\n    doWrite(stream, state, true, state.length, buffer, '', holder.finish); // doWrite is almost always async, defer these to save a bit of time\n    // as the hot path ends with doWrite\n\n    state.pendingcb++;\n    state.lastBufferedRequest = null;\n\n    if (holder.next) {\n      state.corkedRequestsFree = holder.next;\n      holder.next = null;\n    } else {\n      state.corkedRequestsFree = new CorkedRequest(state);\n    }\n\n    state.bufferedRequestCount = 0;\n  } else {\n    // Slow case, write chunks one-by-one\n    while (entry) {\n      var chunk = entry.chunk;\n      var encoding = entry.encoding;\n      var cb = entry.callback;\n      var len = state.objectMode ? 1 : chunk.length;\n      doWrite(stream, state, false, len, chunk, encoding, cb);\n      entry = entry.next;\n      state.bufferedRequestCount--; // if we didn't call the onwrite immediately, then\n      // it means that we need to wait until it does.\n      // also, that means that the chunk and cb are currently\n      // being processed, so move the buffer counter past them.\n\n      if (state.writing) {\n        break;\n      }\n    }\n\n    if (entry === null) state.lastBufferedRequest = null;\n  }\n\n  state.bufferedRequest = entry;\n  state.bufferProcessing = false;\n}\n\nWritable.prototype._write = function (chunk, encoding, cb) {\n  cb(new ERR_METHOD_NOT_IMPLEMENTED('_write()'));\n};\n\nWritable.prototype._writev = null;\n\nWritable.prototype.end = function (chunk, encoding, cb) {\n  var state = this._writableState;\n\n  if (typeof chunk === 'function') {\n    cb = chunk;\n    chunk = null;\n    encoding = null;\n  } else if (typeof encoding === 'function') {\n    cb = encoding;\n    encoding = null;\n  }\n\n  if (chunk !== null && chunk !== undefined) this.write(chunk, encoding); // .end() fully uncorks\n\n  if (state.corked) {\n    state.corked = 1;\n    this.uncork();\n  } // ignore unnecessary end() calls.\n\n\n  if (!state.ending) endWritable(this, state, cb);\n  return this;\n};\n\nObject.defineProperty(Writable.prototype, 'writableLength', {\n  // making it explicit this property is not enumerable\n  // because otherwise some prototype manipulation in\n  // userland will fail\n  enumerable: false,\n  get: function get() {\n    return this._writableState.length;\n  }\n});\n\nfunction needFinish(state) {\n  return state.ending && state.length === 0 && state.bufferedRequest === null && !state.finished && !state.writing;\n}\n\nfunction callFinal(stream, state) {\n  stream._final(function (err) {\n    state.pendingcb--;\n\n    if (err) {\n      errorOrDestroy(stream, err);\n    }\n\n    state.prefinished = true;\n    stream.emit('prefinish');\n    finishMaybe(stream, state);\n  });\n}\n\nfunction prefinish(stream, state) {\n  if (!state.prefinished && !state.finalCalled) {\n    if (typeof stream._final === 'function' && !state.destroyed) {\n      state.pendingcb++;\n      state.finalCalled = true;\n      process.nextTick(callFinal, stream, state);\n    } else {\n      state.prefinished = true;\n      stream.emit('prefinish');\n    }\n  }\n}\n\nfunction finishMaybe(stream, state) {\n  var need = needFinish(state);\n\n  if (need) {\n    prefinish(stream, state);\n\n    if (state.pendingcb === 0) {\n      state.finished = true;\n      stream.emit('finish');\n\n      if (state.autoDestroy) {\n        // In case of duplex streams we need a way to detect\n        // if the readable side is ready for autoDestroy as well\n        var rState = stream._readableState;\n\n        if (!rState || rState.autoDestroy && rState.endEmitted) {\n          stream.destroy();\n        }\n      }\n    }\n  }\n\n  return need;\n}\n\nfunction endWritable(stream, state, cb) {\n  state.ending = true;\n  finishMaybe(stream, state);\n\n  if (cb) {\n    if (state.finished) process.nextTick(cb);else stream.once('finish', cb);\n  }\n\n  state.ended = true;\n  stream.writable = false;\n}\n\nfunction onCorkedFinish(corkReq, state, err) {\n  var entry = corkReq.entry;\n  corkReq.entry = null;\n\n  while (entry) {\n    var cb = entry.callback;\n    state.pendingcb--;\n    cb(err);\n    entry = entry.next;\n  } // reuse the free corkReq.\n\n\n  state.corkedRequestsFree.next = corkReq;\n}\n\nObject.defineProperty(Writable.prototype, 'destroyed', {\n  // making it explicit this property is not enumerable\n  // because otherwise some prototype manipulation in\n  // userland will fail\n  enumerable: false,\n  get: function get() {\n    if (this._writableState === undefined) {\n      return false;\n    }\n\n    return this._writableState.destroyed;\n  },\n  set: function set(value) {\n    // we ignore the value if the stream\n    // has not been initialized yet\n    if (!this._writableState) {\n      return;\n    } // backward compatibility, the user is explicitly\n    // managing destroyed\n\n\n    this._writableState.destroyed = value;\n  }\n});\nWritable.prototype.destroy = destroyImpl.destroy;\nWritable.prototype._undestroy = destroyImpl.undestroy;\n\nWritable.prototype._destroy = function (err, cb) {\n  cb(err);\n};","// Copyright Joyent, Inc. and other Node contributors.\n//\n// Permission is hereby granted, free of charge, to any person obtaining a\n// copy of this software and associated documentation files (the\n// \"Software\"), to deal in the Software without restriction, including\n// without limitation the rights to use, copy, modify, merge, publish,\n// distribute, sublicense, and/or sell copies of the Software, and to permit\n// persons to whom the Software is furnished to do so, subject to the\n// following conditions:\n//\n// The above copyright notice and this permission notice shall be included\n// in all copies or substantial portions of the Software.\n//\n// THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS\n// OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n// MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN\n// NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n// DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR\n// OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE\n// USE OR OTHER DEALINGS IN THE SOFTWARE.\n// a transform stream is a readable/writable stream where you do\n// something with the data.  Sometimes it's called a \"filter\",\n// but that's not a great name for it, since that implies a thing where\n// some bits pass through, and others are simply ignored.  (That would\n// be a valid example of a transform, of course.)\n//\n// While the output is causally related to the input, it's not a\n// necessarily symmetric or synchronous transformation.  For example,\n// a zlib stream might take multiple plain-text writes(), and then\n// emit a single compressed chunk some time in the future.\n//\n// Here's how this works:\n//\n// The Transform stream has all the aspects of the readable and writable\n// stream classes.  When you write(chunk), that calls _write(chunk,cb)\n// internally, and returns false if there's a lot of pending writes\n// buffered up.  When you call read(), that calls _read(n) until\n// there's enough pending readable data buffered up.\n//\n// In a transform stream, the written data is placed in a buffer.  When\n// _read(n) is called, it transforms the queued up data, calling the\n// buffered _write cb's as it consumes chunks.  If consuming a single\n// written chunk would result in multiple output chunks, then the first\n// outputted bit calls the readcb, and subsequent chunks just go into\n// the read buffer, and will cause it to emit 'readable' if necessary.\n//\n// This way, back-pressure is actually determined by the reading side,\n// since _read has to be called to start processing a new chunk.  However,\n// a pathological inflate type of transform can cause excessive buffering\n// here.  For example, imagine a stream where every byte of input is\n// interpreted as an integer from 0-255, and then results in that many\n// bytes of output.  Writing the 4 bytes {ff,ff,ff,ff} would result in\n// 1kb of data being output.  In this case, you could write a very small\n// amount of input, and end up with a very large amount of output.  In\n// such a pathological inflating mechanism, there'd be no way to tell\n// the system to stop doing the transform.  A single 4MB write could\n// cause the system to run out of memory.\n//\n// However, even in such a pathological case, only a single written chunk\n// would be consumed, and then the rest would wait (un-transformed) until\n// the results of the previous transformed chunk were consumed.\n'use strict';\n\nmodule.exports = Transform;\n\nvar _require$codes = require('../errors').codes,\n    ERR_METHOD_NOT_IMPLEMENTED = _require$codes.ERR_METHOD_NOT_IMPLEMENTED,\n    ERR_MULTIPLE_CALLBACK = _require$codes.ERR_MULTIPLE_CALLBACK,\n    ERR_TRANSFORM_ALREADY_TRANSFORMING = _require$codes.ERR_TRANSFORM_ALREADY_TRANSFORMING,\n    ERR_TRANSFORM_WITH_LENGTH_0 = _require$codes.ERR_TRANSFORM_WITH_LENGTH_0;\n\nvar Duplex = require('./_stream_duplex');\n\nrequire('inherits')(Transform, Duplex);\n\nfunction afterTransform(er, data) {\n  var ts = this._transformState;\n  ts.transforming = false;\n  var cb = ts.writecb;\n\n  if (cb === null) {\n    return this.emit('error', new ERR_MULTIPLE_CALLBACK());\n  }\n\n  ts.writechunk = null;\n  ts.writecb = null;\n  if (data != null) // single equals check for both `null` and `undefined`\n    this.push(data);\n  cb(er);\n  var rs = this._readableState;\n  rs.reading = false;\n\n  if (rs.needReadable || rs.length < rs.highWaterMark) {\n    this._read(rs.highWaterMark);\n  }\n}\n\nfunction Transform(options) {\n  if (!(this instanceof Transform)) return new Transform(options);\n  Duplex.call(this, options);\n  this._transformState = {\n    afterTransform: afterTransform.bind(this),\n    needTransform: false,\n    transforming: false,\n    writecb: null,\n    writechunk: null,\n    writeencoding: null\n  }; // start out asking for a readable event once data is transformed.\n\n  this._readableState.needReadable = true; // we have implemented the _read method, and done the other things\n  // that Readable wants before the first _read call, so unset the\n  // sync guard flag.\n\n  this._readableState.sync = false;\n\n  if (options) {\n    if (typeof options.transform === 'function') this._transform = options.transform;\n    if (typeof options.flush === 'function') this._flush = options.flush;\n  } // When the writable side finishes, then flush out anything remaining.\n\n\n  this.on('prefinish', prefinish);\n}\n\nfunction prefinish() {\n  var _this = this;\n\n  if (typeof this._flush === 'function' && !this._readableState.destroyed) {\n    this._flush(function (er, data) {\n      done(_this, er, data);\n    });\n  } else {\n    done(this, null, null);\n  }\n}\n\nTransform.prototype.push = function (chunk, encoding) {\n  this._transformState.needTransform = false;\n  return Duplex.prototype.push.call(this, chunk, encoding);\n}; // This is the part where you do stuff!\n// override this function in implementation classes.\n// 'chunk' is an input chunk.\n//\n// Call `push(newChunk)` to pass along transformed output\n// to the readable side.  You may call 'push' zero or more times.\n//\n// Call `cb(err)` when you are done with this chunk.  If you pass\n// an error, then that'll put the hurt on the whole operation.  If you\n// never call cb(), then you'll never get another chunk.\n\n\nTransform.prototype._transform = function (chunk, encoding, cb) {\n  cb(new ERR_METHOD_NOT_IMPLEMENTED('_transform()'));\n};\n\nTransform.prototype._write = function (chunk, encoding, cb) {\n  var ts = this._transformState;\n  ts.writecb = cb;\n  ts.writechunk = chunk;\n  ts.writeencoding = encoding;\n\n  if (!ts.transforming) {\n    var rs = this._readableState;\n    if (ts.needTransform || rs.needReadable || rs.length < rs.highWaterMark) this._read(rs.highWaterMark);\n  }\n}; // Doesn't matter what the args are here.\n// _transform does all the work.\n// That we got here means that the readable side wants more data.\n\n\nTransform.prototype._read = function (n) {\n  var ts = this._transformState;\n\n  if (ts.writechunk !== null && !ts.transforming) {\n    ts.transforming = true;\n\n    this._transform(ts.writechunk, ts.writeencoding, ts.afterTransform);\n  } else {\n    // mark that we need a transform, so that any data that comes in\n    // will get processed, now that we've asked for it.\n    ts.needTransform = true;\n  }\n};\n\nTransform.prototype._destroy = function (err, cb) {\n  Duplex.prototype._destroy.call(this, err, function (err2) {\n    cb(err2);\n  });\n};\n\nfunction done(stream, er, data) {\n  if (er) return stream.emit('error', er);\n  if (data != null) // single equals check for both `null` and `undefined`\n    stream.push(data); // TODO(BridgeAR): Write a test for these two error cases\n  // if there's nothing in the write buffer, then that means\n  // that nothing more will ever be provided\n\n  if (stream._writableState.length) throw new ERR_TRANSFORM_WITH_LENGTH_0();\n  if (stream._transformState.transforming) throw new ERR_TRANSFORM_ALREADY_TRANSFORMING();\n  return stream.push(null);\n}","/**\n * A JavaScript implementation of the Secure Hash Algorithm, SHA-256, as defined\n * in FIPS 180-2\n * Version 2.2-beta Copyright Angel Marin, Paul Johnston 2000 - 2009.\n * Other contributors: Greg Holt, Andrew Kepert, Ydnar, Lostinet\n *\n */\n\nvar inherits = require('inherits')\nvar Hash = require('./hash')\nvar Buffer = require('safe-buffer').Buffer\n\nvar K = [\n  0x428A2F98, 0x71374491, 0xB5C0FBCF, 0xE9B5DBA5,\n  0x3956C25B, 0x59F111F1, 0x923F82A4, 0xAB1C5ED5,\n  0xD807AA98, 0x12835B01, 0x243185BE, 0x550C7DC3,\n  0x72BE5D74, 0x80DEB1FE, 0x9BDC06A7, 0xC19BF174,\n  0xE49B69C1, 0xEFBE4786, 0x0FC19DC6, 0x240CA1CC,\n  0x2DE92C6F, 0x4A7484AA, 0x5CB0A9DC, 0x76F988DA,\n  0x983E5152, 0xA831C66D, 0xB00327C8, 0xBF597FC7,\n  0xC6E00BF3, 0xD5A79147, 0x06CA6351, 0x14292967,\n  0x27B70A85, 0x2E1B2138, 0x4D2C6DFC, 0x53380D13,\n  0x650A7354, 0x766A0ABB, 0x81C2C92E, 0x92722C85,\n  0xA2BFE8A1, 0xA81A664B, 0xC24B8B70, 0xC76C51A3,\n  0xD192E819, 0xD6990624, 0xF40E3585, 0x106AA070,\n  0x19A4C116, 0x1E376C08, 0x2748774C, 0x34B0BCB5,\n  0x391C0CB3, 0x4ED8AA4A, 0x5B9CCA4F, 0x682E6FF3,\n  0x748F82EE, 0x78A5636F, 0x84C87814, 0x8CC70208,\n  0x90BEFFFA, 0xA4506CEB, 0xBEF9A3F7, 0xC67178F2\n]\n\nvar W = new Array(64)\n\nfunction Sha256 () {\n  this.init()\n\n  this._w = W // new Array(64)\n\n  Hash.call(this, 64, 56)\n}\n\ninherits(Sha256, Hash)\n\nSha256.prototype.init = function () {\n  this._a = 0x6a09e667\n  this._b = 0xbb67ae85\n  this._c = 0x3c6ef372\n  this._d = 0xa54ff53a\n  this._e = 0x510e527f\n  this._f = 0x9b05688c\n  this._g = 0x1f83d9ab\n  this._h = 0x5be0cd19\n\n  return this\n}\n\nfunction ch (x, y, z) {\n  return z ^ (x & (y ^ z))\n}\n\nfunction maj (x, y, z) {\n  return (x & y) | (z & (x | y))\n}\n\nfunction sigma0 (x) {\n  return (x >>> 2 | x << 30) ^ (x >>> 13 | x << 19) ^ (x >>> 22 | x << 10)\n}\n\nfunction sigma1 (x) {\n  return (x >>> 6 | x << 26) ^ (x >>> 11 | x << 21) ^ (x >>> 25 | x << 7)\n}\n\nfunction gamma0 (x) {\n  return (x >>> 7 | x << 25) ^ (x >>> 18 | x << 14) ^ (x >>> 3)\n}\n\nfunction gamma1 (x) {\n  return (x >>> 17 | x << 15) ^ (x >>> 19 | x << 13) ^ (x >>> 10)\n}\n\nSha256.prototype._update = function (M) {\n  var W = this._w\n\n  var a = this._a | 0\n  var b = this._b | 0\n  var c = this._c | 0\n  var d = this._d | 0\n  var e = this._e | 0\n  var f = this._f | 0\n  var g = this._g | 0\n  var h = this._h | 0\n\n  for (var i = 0; i < 16; ++i) W[i] = M.readInt32BE(i * 4)\n  for (; i < 64; ++i) W[i] = (gamma1(W[i - 2]) + W[i - 7] + gamma0(W[i - 15]) + W[i - 16]) | 0\n\n  for (var j = 0; j < 64; ++j) {\n    var T1 = (h + sigma1(e) + ch(e, f, g) + K[j] + W[j]) | 0\n    var T2 = (sigma0(a) + maj(a, b, c)) | 0\n\n    h = g\n    g = f\n    f = e\n    e = (d + T1) | 0\n    d = c\n    c = b\n    b = a\n    a = (T1 + T2) | 0\n  }\n\n  this._a = (a + this._a) | 0\n  this._b = (b + this._b) | 0\n  this._c = (c + this._c) | 0\n  this._d = (d + this._d) | 0\n  this._e = (e + this._e) | 0\n  this._f = (f + this._f) | 0\n  this._g = (g + this._g) | 0\n  this._h = (h + this._h) | 0\n}\n\nSha256.prototype._hash = function () {\n  var H = Buffer.allocUnsafe(32)\n\n  H.writeInt32BE(this._a, 0)\n  H.writeInt32BE(this._b, 4)\n  H.writeInt32BE(this._c, 8)\n  H.writeInt32BE(this._d, 12)\n  H.writeInt32BE(this._e, 16)\n  H.writeInt32BE(this._f, 20)\n  H.writeInt32BE(this._g, 24)\n  H.writeInt32BE(this._h, 28)\n\n  return H\n}\n\nmodule.exports = Sha256\n","var inherits = require('inherits')\nvar Hash = require('./hash')\nvar Buffer = require('safe-buffer').Buffer\n\nvar K = [\n  0x428a2f98, 0xd728ae22, 0x71374491, 0x23ef65cd,\n  0xb5c0fbcf, 0xec4d3b2f, 0xe9b5dba5, 0x8189dbbc,\n  0x3956c25b, 0xf348b538, 0x59f111f1, 0xb605d019,\n  0x923f82a4, 0xaf194f9b, 0xab1c5ed5, 0xda6d8118,\n  0xd807aa98, 0xa3030242, 0x12835b01, 0x45706fbe,\n  0x243185be, 0x4ee4b28c, 0x550c7dc3, 0xd5ffb4e2,\n  0x72be5d74, 0xf27b896f, 0x80deb1fe, 0x3b1696b1,\n  0x9bdc06a7, 0x25c71235, 0xc19bf174, 0xcf692694,\n  0xe49b69c1, 0x9ef14ad2, 0xefbe4786, 0x384f25e3,\n  0x0fc19dc6, 0x8b8cd5b5, 0x240ca1cc, 0x77ac9c65,\n  0x2de92c6f, 0x592b0275, 0x4a7484aa, 0x6ea6e483,\n  0x5cb0a9dc, 0xbd41fbd4, 0x76f988da, 0x831153b5,\n  0x983e5152, 0xee66dfab, 0xa831c66d, 0x2db43210,\n  0xb00327c8, 0x98fb213f, 0xbf597fc7, 0xbeef0ee4,\n  0xc6e00bf3, 0x3da88fc2, 0xd5a79147, 0x930aa725,\n  0x06ca6351, 0xe003826f, 0x14292967, 0x0a0e6e70,\n  0x27b70a85, 0x46d22ffc, 0x2e1b2138, 0x5c26c926,\n  0x4d2c6dfc, 0x5ac42aed, 0x53380d13, 0x9d95b3df,\n  0x650a7354, 0x8baf63de, 0x766a0abb, 0x3c77b2a8,\n  0x81c2c92e, 0x47edaee6, 0x92722c85, 0x1482353b,\n  0xa2bfe8a1, 0x4cf10364, 0xa81a664b, 0xbc423001,\n  0xc24b8b70, 0xd0f89791, 0xc76c51a3, 0x0654be30,\n  0xd192e819, 0xd6ef5218, 0xd6990624, 0x5565a910,\n  0xf40e3585, 0x5771202a, 0x106aa070, 0x32bbd1b8,\n  0x19a4c116, 0xb8d2d0c8, 0x1e376c08, 0x5141ab53,\n  0x2748774c, 0xdf8eeb99, 0x34b0bcb5, 0xe19b48a8,\n  0x391c0cb3, 0xc5c95a63, 0x4ed8aa4a, 0xe3418acb,\n  0x5b9cca4f, 0x7763e373, 0x682e6ff3, 0xd6b2b8a3,\n  0x748f82ee, 0x5defb2fc, 0x78a5636f, 0x43172f60,\n  0x84c87814, 0xa1f0ab72, 0x8cc70208, 0x1a6439ec,\n  0x90befffa, 0x23631e28, 0xa4506ceb, 0xde82bde9,\n  0xbef9a3f7, 0xb2c67915, 0xc67178f2, 0xe372532b,\n  0xca273ece, 0xea26619c, 0xd186b8c7, 0x21c0c207,\n  0xeada7dd6, 0xcde0eb1e, 0xf57d4f7f, 0xee6ed178,\n  0x06f067aa, 0x72176fba, 0x0a637dc5, 0xa2c898a6,\n  0x113f9804, 0xbef90dae, 0x1b710b35, 0x131c471b,\n  0x28db77f5, 0x23047d84, 0x32caab7b, 0x40c72493,\n  0x3c9ebe0a, 0x15c9bebc, 0x431d67c4, 0x9c100d4c,\n  0x4cc5d4be, 0xcb3e42b6, 0x597f299c, 0xfc657e2a,\n  0x5fcb6fab, 0x3ad6faec, 0x6c44198c, 0x4a475817\n]\n\nvar W = new Array(160)\n\nfunction Sha512 () {\n  this.init()\n  this._w = W\n\n  Hash.call(this, 128, 112)\n}\n\ninherits(Sha512, Hash)\n\nSha512.prototype.init = function () {\n  this._ah = 0x6a09e667\n  this._bh = 0xbb67ae85\n  this._ch = 0x3c6ef372\n  this._dh = 0xa54ff53a\n  this._eh = 0x510e527f\n  this._fh = 0x9b05688c\n  this._gh = 0x1f83d9ab\n  this._hh = 0x5be0cd19\n\n  this._al = 0xf3bcc908\n  this._bl = 0x84caa73b\n  this._cl = 0xfe94f82b\n  this._dl = 0x5f1d36f1\n  this._el = 0xade682d1\n  this._fl = 0x2b3e6c1f\n  this._gl = 0xfb41bd6b\n  this._hl = 0x137e2179\n\n  return this\n}\n\nfunction Ch (x, y, z) {\n  return z ^ (x & (y ^ z))\n}\n\nfunction maj (x, y, z) {\n  return (x & y) | (z & (x | y))\n}\n\nfunction sigma0 (x, xl) {\n  return (x >>> 28 | xl << 4) ^ (xl >>> 2 | x << 30) ^ (xl >>> 7 | x << 25)\n}\n\nfunction sigma1 (x, xl) {\n  return (x >>> 14 | xl << 18) ^ (x >>> 18 | xl << 14) ^ (xl >>> 9 | x << 23)\n}\n\nfunction Gamma0 (x, xl) {\n  return (x >>> 1 | xl << 31) ^ (x >>> 8 | xl << 24) ^ (x >>> 7)\n}\n\nfunction Gamma0l (x, xl) {\n  return (x >>> 1 | xl << 31) ^ (x >>> 8 | xl << 24) ^ (x >>> 7 | xl << 25)\n}\n\nfunction Gamma1 (x, xl) {\n  return (x >>> 19 | xl << 13) ^ (xl >>> 29 | x << 3) ^ (x >>> 6)\n}\n\nfunction Gamma1l (x, xl) {\n  return (x >>> 19 | xl << 13) ^ (xl >>> 29 | x << 3) ^ (x >>> 6 | xl << 26)\n}\n\nfunction getCarry (a, b) {\n  return (a >>> 0) < (b >>> 0) ? 1 : 0\n}\n\nSha512.prototype._update = function (M) {\n  var W = this._w\n\n  var ah = this._ah | 0\n  var bh = this._bh | 0\n  var ch = this._ch | 0\n  var dh = this._dh | 0\n  var eh = this._eh | 0\n  var fh = this._fh | 0\n  var gh = this._gh | 0\n  var hh = this._hh | 0\n\n  var al = this._al | 0\n  var bl = this._bl | 0\n  var cl = this._cl | 0\n  var dl = this._dl | 0\n  var el = this._el | 0\n  var fl = this._fl | 0\n  var gl = this._gl | 0\n  var hl = this._hl | 0\n\n  for (var i = 0; i < 32; i += 2) {\n    W[i] = M.readInt32BE(i * 4)\n    W[i + 1] = M.readInt32BE(i * 4 + 4)\n  }\n  for (; i < 160; i += 2) {\n    var xh = W[i - 15 * 2]\n    var xl = W[i - 15 * 2 + 1]\n    var gamma0 = Gamma0(xh, xl)\n    var gamma0l = Gamma0l(xl, xh)\n\n    xh = W[i - 2 * 2]\n    xl = W[i - 2 * 2 + 1]\n    var gamma1 = Gamma1(xh, xl)\n    var gamma1l = Gamma1l(xl, xh)\n\n    // W[i] = gamma0 + W[i - 7] + gamma1 + W[i - 16]\n    var Wi7h = W[i - 7 * 2]\n    var Wi7l = W[i - 7 * 2 + 1]\n\n    var Wi16h = W[i - 16 * 2]\n    var Wi16l = W[i - 16 * 2 + 1]\n\n    var Wil = (gamma0l + Wi7l) | 0\n    var Wih = (gamma0 + Wi7h + getCarry(Wil, gamma0l)) | 0\n    Wil = (Wil + gamma1l) | 0\n    Wih = (Wih + gamma1 + getCarry(Wil, gamma1l)) | 0\n    Wil = (Wil + Wi16l) | 0\n    Wih = (Wih + Wi16h + getCarry(Wil, Wi16l)) | 0\n\n    W[i] = Wih\n    W[i + 1] = Wil\n  }\n\n  for (var j = 0; j < 160; j += 2) {\n    Wih = W[j]\n    Wil = W[j + 1]\n\n    var majh = maj(ah, bh, ch)\n    var majl = maj(al, bl, cl)\n\n    var sigma0h = sigma0(ah, al)\n    var sigma0l = sigma0(al, ah)\n    var sigma1h = sigma1(eh, el)\n    var sigma1l = sigma1(el, eh)\n\n    // t1 = h + sigma1 + ch + K[j] + W[j]\n    var Kih = K[j]\n    var Kil = K[j + 1]\n\n    var chh = Ch(eh, fh, gh)\n    var chl = Ch(el, fl, gl)\n\n    var t1l = (hl + sigma1l) | 0\n    var t1h = (hh + sigma1h + getCarry(t1l, hl)) | 0\n    t1l = (t1l + chl) | 0\n    t1h = (t1h + chh + getCarry(t1l, chl)) | 0\n    t1l = (t1l + Kil) | 0\n    t1h = (t1h + Kih + getCarry(t1l, Kil)) | 0\n    t1l = (t1l + Wil) | 0\n    t1h = (t1h + Wih + getCarry(t1l, Wil)) | 0\n\n    // t2 = sigma0 + maj\n    var t2l = (sigma0l + majl) | 0\n    var t2h = (sigma0h + majh + getCarry(t2l, sigma0l)) | 0\n\n    hh = gh\n    hl = gl\n    gh = fh\n    gl = fl\n    fh = eh\n    fl = el\n    el = (dl + t1l) | 0\n    eh = (dh + t1h + getCarry(el, dl)) | 0\n    dh = ch\n    dl = cl\n    ch = bh\n    cl = bl\n    bh = ah\n    bl = al\n    al = (t1l + t2l) | 0\n    ah = (t1h + t2h + getCarry(al, t1l)) | 0\n  }\n\n  this._al = (this._al + al) | 0\n  this._bl = (this._bl + bl) | 0\n  this._cl = (this._cl + cl) | 0\n  this._dl = (this._dl + dl) | 0\n  this._el = (this._el + el) | 0\n  this._fl = (this._fl + fl) | 0\n  this._gl = (this._gl + gl) | 0\n  this._hl = (this._hl + hl) | 0\n\n  this._ah = (this._ah + ah + getCarry(this._al, al)) | 0\n  this._bh = (this._bh + bh + getCarry(this._bl, bl)) | 0\n  this._ch = (this._ch + ch + getCarry(this._cl, cl)) | 0\n  this._dh = (this._dh + dh + getCarry(this._dl, dl)) | 0\n  this._eh = (this._eh + eh + getCarry(this._el, el)) | 0\n  this._fh = (this._fh + fh + getCarry(this._fl, fl)) | 0\n  this._gh = (this._gh + gh + getCarry(this._gl, gl)) | 0\n  this._hh = (this._hh + hh + getCarry(this._hl, hl)) | 0\n}\n\nSha512.prototype._hash = function () {\n  var H = Buffer.allocUnsafe(64)\n\n  function writeInt64BE (h, l, offset) {\n    H.writeInt32BE(h, offset)\n    H.writeInt32BE(l, offset + 4)\n  }\n\n  writeInt64BE(this._ah, this._al, 0)\n  writeInt64BE(this._bh, this._bl, 8)\n  writeInt64BE(this._ch, this._cl, 16)\n  writeInt64BE(this._dh, this._dl, 24)\n  writeInt64BE(this._eh, this._el, 32)\n  writeInt64BE(this._fh, this._fl, 40)\n  writeInt64BE(this._gh, this._gl, 48)\n  writeInt64BE(this._hh, this._hl, 56)\n\n  return H\n}\n\nmodule.exports = Sha512\n","'use strict'\nconst EthBlockHeader = require('ethereumjs-block/header')\nconst multicodec = require('multicodec')\nconst cidFromHash = require('../util/cidFromHash')\nconst createResolver = require('../util/createResolver')\n\nconst deserialize = (serialized) => {\n  const ethObj = new EthBlockHeader(serialized)\n\n  const deserialized = {\n    authorAddress: ethObj.coinbase,\n    bloom: ethObj.bloom,\n    difficulty: ethObj.difficulty,\n    extraData: ethObj.extraData,\n    gasLimit: ethObj.gasLimit,\n    gasUsed: ethObj.gasUsed,\n    mixHash: ethObj.mixHash,\n    nonce: ethObj.nonce,\n    number: ethObj.number,\n    ommerHash: ethObj.uncleHash,\n    ommers: cidFromHash(multicodec.ETH_BLOCK_LIST, ethObj.uncleHash),\n    parent: cidFromHash(multicodec.ETH_BLOCK, ethObj.parentHash),\n    parentHash: ethObj.parentHash,\n    state: cidFromHash(multicodec.ETH_STATE_TRIE, ethObj.stateRoot),\n    stateRoot: ethObj.stateRoot,\n    timestamp: ethObj.timestamp,\n    transactions: cidFromHash(multicodec.ETH_TX_TRIE, ethObj.transactionsTrie),\n    transactionReceipts: cidFromHash(\n      multicodec.ETH_TX_RECEIPT_TRIE, ethObj.receiptTrie),\n    transactionReceiptTrieRoot: ethObj.receiptTrie,\n    transactionTrieRoot: ethObj.transactionsTrie,\n    _ethObj: ethObj\n  }\n\n  Object.defineProperty(deserialized, '_ethObj', { enumerable: false })\n\n  return deserialized\n}\n\nmodule.exports = createResolver(multicodec.ETH_BLOCK, deserialize)\n","'use strict'\nconst EthTx = require('ethereumjs-tx').Transaction\nconst createResolver = require('../util/createResolver')\nconst multicodec = require('multicodec')\n\nconst deserialize = (serialized) => {\n  const ethObj = new EthTx(serialized)\n\n  const deserialized = {\n    data: ethObj.data,\n    fromAddress: ethObj.from,\n    gasLimit: ethObj.gasLimit,\n    gasPrice: ethObj.gasPrice,\n    isContractPublish: ethObj.toCreationAddress(),\n    nonce: ethObj.nonce,\n    r: ethObj.r,\n    s: ethObj.s,\n    signature: [ethObj.v, ethObj.r, ethObj.s],\n    toAddress: ethObj.to,\n    v: ethObj.v,\n    value: ethObj.value,\n    _ethObj: ethObj\n  }\n\n  Object.defineProperty(deserialized, '_ethObj', { enumerable: false })\n\n  return deserialized\n}\n\nmodule.exports = createResolver(multicodec.ETH_TX, deserialize)\n","\"use strict\";\nvar __assign = (this && this.__assign) || function () {\n    __assign = Object.assign || function(t) {\n        for (var s, i = 1, n = arguments.length; i < n; i++) {\n            s = arguments[i];\n            for (var p in s) if (Object.prototype.hasOwnProperty.call(s, p))\n                t[p] = s[p];\n        }\n        return t;\n    };\n    return __assign.apply(this, arguments);\n};\nObject.defineProperty(exports, \"__esModule\", { value: true });\nvar ethereumjs_util_1 = require(\"ethereumjs-util\");\nvar ethereumjs_common_1 = require(\"ethereumjs-common\");\nvar buffer_1 = require(\"buffer\");\n// secp256k1n/2\nvar N_DIV_2 = new ethereumjs_util_1.BN('7fffffffffffffffffffffffffffffff5d576e7357a4501ddfe92f46681b20a0', 16);\n/**\n * An Ethereum transaction.\n */\nvar Transaction = /** @class */ (function () {\n    /**\n     * Creates a new transaction from an object with its fields' values.\n     *\n     * @param data - A transaction can be initialized with its rlp representation, an array containing\n     * the value of its fields in order, or an object containing them by name.\n     *\n     * @param opts - The transaction's options, used to indicate the chain and hardfork the\n     * transactions belongs to.\n     *\n     * @note Transaction objects implement EIP155 by default. To disable it, use the constructor's\n     * second parameter to set a chain and hardfork before EIP155 activation (i.e. before Spurious\n     * Dragon.)\n     *\n     * @example\n     * ```js\n     * const txData = {\n     *   nonce: '0x00',\n     *   gasPrice: '0x09184e72a000',\n     *   gasLimit: '0x2710',\n     *   to: '0x0000000000000000000000000000000000000000',\n     *   value: '0x00',\n     *   data: '0x7f7465737432000000000000000000000000000000000000000000000000000000600057',\n     *   v: '0x1c',\n     *   r: '0x5e1d3a76fbf824220eafc8c79ad578ad2b67d01b0c2425eb1f1347e8f50882ab',\n     *   s: '0x5bd428537f05f9830e93792f90ea6a3e2d1ee84952dd96edbae9f658f831ab13'\n     * };\n     * const tx = new Transaction(txData);\n     * ```\n     */\n    function Transaction(data, opts) {\n        if (data === void 0) { data = {}; }\n        if (opts === void 0) { opts = {}; }\n        // instantiate Common class instance based on passed options\n        if (opts.common) {\n            if (opts.chain || opts.hardfork) {\n                throw new Error('Instantiation with both opts.common, and opts.chain and opts.hardfork parameter not allowed!');\n            }\n            this._common = opts.common;\n        }\n        else {\n            var chain = opts.chain ? opts.chain : 'mainnet';\n            var hardfork = opts.hardfork ? opts.hardfork : 'petersburg';\n            this._common = new ethereumjs_common_1.default(chain, hardfork);\n        }\n        // Define Properties\n        var fields = [\n            {\n                name: 'nonce',\n                length: 32,\n                allowLess: true,\n                default: new buffer_1.Buffer([]),\n            },\n            {\n                name: 'gasPrice',\n                length: 32,\n                allowLess: true,\n                default: new buffer_1.Buffer([]),\n            },\n            {\n                name: 'gasLimit',\n                alias: 'gas',\n                length: 32,\n                allowLess: true,\n                default: new buffer_1.Buffer([]),\n            },\n            {\n                name: 'to',\n                allowZero: true,\n                length: 20,\n                default: new buffer_1.Buffer([]),\n            },\n            {\n                name: 'value',\n                length: 32,\n                allowLess: true,\n                default: new buffer_1.Buffer([]),\n            },\n            {\n                name: 'data',\n                alias: 'input',\n                allowZero: true,\n                default: new buffer_1.Buffer([]),\n            },\n            {\n                name: 'v',\n                allowZero: true,\n                default: new buffer_1.Buffer([]),\n            },\n            {\n                name: 'r',\n                length: 32,\n                allowZero: true,\n                allowLess: true,\n                default: new buffer_1.Buffer([]),\n            },\n            {\n                name: 's',\n                length: 32,\n                allowZero: true,\n                allowLess: true,\n                default: new buffer_1.Buffer([]),\n            },\n        ];\n        // attached serialize\n        ethereumjs_util_1.defineProperties(this, fields, data);\n        /**\n         * @property {Buffer} from (read only) sender address of this transaction, mathematically derived from other parameters.\n         * @name from\n         * @memberof Transaction\n         */\n        Object.defineProperty(this, 'from', {\n            enumerable: true,\n            configurable: true,\n            get: this.getSenderAddress.bind(this),\n        });\n        this._validateV(this.v);\n        this._overrideVSetterWithValidation();\n    }\n    /**\n     * If the tx's `to` is to the creation address\n     */\n    Transaction.prototype.toCreationAddress = function () {\n        return this.to.toString('hex') === '';\n    };\n    /**\n     * Computes a sha3-256 hash of the serialized tx\n     * @param includeSignature - Whether or not to include the signature\n     */\n    Transaction.prototype.hash = function (includeSignature) {\n        if (includeSignature === void 0) { includeSignature = true; }\n        var items;\n        if (includeSignature) {\n            items = this.raw;\n        }\n        else {\n            if (this._implementsEIP155()) {\n                items = this.raw.slice(0, 6).concat([\n                    ethereumjs_util_1.toBuffer(this.getChainId()),\n                    // TODO: stripping zeros should probably be a responsibility of the rlp module\n                    ethereumjs_util_1.stripZeros(ethereumjs_util_1.toBuffer(0)),\n                    ethereumjs_util_1.stripZeros(ethereumjs_util_1.toBuffer(0)),\n                ]);\n            }\n            else {\n                items = this.raw.slice(0, 6);\n            }\n        }\n        // create hash\n        return ethereumjs_util_1.rlphash(items);\n    };\n    /**\n     * returns chain ID\n     */\n    Transaction.prototype.getChainId = function () {\n        return this._common.chainId();\n    };\n    /**\n     * returns the sender's address\n     */\n    Transaction.prototype.getSenderAddress = function () {\n        if (this._from) {\n            return this._from;\n        }\n        var pubkey = this.getSenderPublicKey();\n        this._from = ethereumjs_util_1.publicToAddress(pubkey);\n        return this._from;\n    };\n    /**\n     * returns the public key of the sender\n     */\n    Transaction.prototype.getSenderPublicKey = function () {\n        if (!this.verifySignature()) {\n            throw new Error('Invalid Signature');\n        }\n        // If the signature was verified successfully the _senderPubKey field is defined\n        return this._senderPubKey;\n    };\n    /**\n     * Determines if the signature is valid\n     */\n    Transaction.prototype.verifySignature = function () {\n        var msgHash = this.hash(false);\n        // All transaction signatures whose s-value is greater than secp256k1n/2 are considered invalid.\n        if (this._common.gteHardfork('homestead') && new ethereumjs_util_1.BN(this.s).cmp(N_DIV_2) === 1) {\n            return false;\n        }\n        try {\n            var v = ethereumjs_util_1.bufferToInt(this.v);\n            var useChainIdWhileRecoveringPubKey = v >= this.getChainId() * 2 + 35 && this._common.gteHardfork('spuriousDragon');\n            this._senderPubKey = ethereumjs_util_1.ecrecover(msgHash, v, this.r, this.s, useChainIdWhileRecoveringPubKey ? this.getChainId() : undefined);\n        }\n        catch (e) {\n            return false;\n        }\n        return !!this._senderPubKey;\n    };\n    /**\n     * sign a transaction with a given private key\n     * @param privateKey - Must be 32 bytes in length\n     */\n    Transaction.prototype.sign = function (privateKey) {\n        // We clear any previous signature before signing it. Otherwise, _implementsEIP155's can give\n        // different results if this tx was already signed.\n        this.v = new buffer_1.Buffer([]);\n        this.s = new buffer_1.Buffer([]);\n        this.r = new buffer_1.Buffer([]);\n        var msgHash = this.hash(false);\n        var sig = ethereumjs_util_1.ecsign(msgHash, privateKey);\n        if (this._implementsEIP155()) {\n            sig.v += this.getChainId() * 2 + 8;\n        }\n        Object.assign(this, sig);\n    };\n    /**\n     * The amount of gas paid for the data in this tx\n     */\n    Transaction.prototype.getDataFee = function () {\n        var data = this.raw[5];\n        var cost = new ethereumjs_util_1.BN(0);\n        for (var i = 0; i < data.length; i++) {\n            data[i] === 0\n                ? cost.iaddn(this._common.param('gasPrices', 'txDataZero'))\n                : cost.iaddn(this._common.param('gasPrices', 'txDataNonZero'));\n        }\n        return cost;\n    };\n    /**\n     * the minimum amount of gas the tx must have (DataFee + TxFee + Creation Fee)\n     */\n    Transaction.prototype.getBaseFee = function () {\n        var fee = this.getDataFee().iaddn(this._common.param('gasPrices', 'tx'));\n        if (this._common.gteHardfork('homestead') && this.toCreationAddress()) {\n            fee.iaddn(this._common.param('gasPrices', 'txCreation'));\n        }\n        return fee;\n    };\n    /**\n     * the up front amount that an account must have for this transaction to be valid\n     */\n    Transaction.prototype.getUpfrontCost = function () {\n        return new ethereumjs_util_1.BN(this.gasLimit).imul(new ethereumjs_util_1.BN(this.gasPrice)).iadd(new ethereumjs_util_1.BN(this.value));\n    };\n    Transaction.prototype.validate = function (stringError) {\n        if (stringError === void 0) { stringError = false; }\n        var errors = [];\n        if (!this.verifySignature()) {\n            errors.push('Invalid Signature');\n        }\n        if (this.getBaseFee().cmp(new ethereumjs_util_1.BN(this.gasLimit)) > 0) {\n            errors.push([\"gas limit is too low. Need at least \" + this.getBaseFee()]);\n        }\n        if (stringError === false) {\n            return errors.length === 0;\n        }\n        else {\n            return errors.join(' ');\n        }\n    };\n    /**\n     * Returns the rlp encoding of the transaction\n     */\n    Transaction.prototype.serialize = function () {\n        // Note: This never gets executed, defineProperties overwrites it.\n        return ethereumjs_util_1.rlp.encode(this.raw);\n    };\n    /**\n     * Returns the transaction in JSON format\n     * @see {@link https://github.com/ethereumjs/ethereumjs-util/blob/master/docs/index.md#defineproperties|ethereumjs-util}\n     */\n    Transaction.prototype.toJSON = function (labels) {\n        if (labels === void 0) { labels = false; }\n        // Note: This never gets executed, defineProperties overwrites it.\n        return {};\n    };\n    Transaction.prototype._validateV = function (v) {\n        if (v === undefined || v.length === 0) {\n            return;\n        }\n        if (!this._common.gteHardfork('spuriousDragon')) {\n            return;\n        }\n        var vInt = ethereumjs_util_1.bufferToInt(v);\n        if (vInt === 27 || vInt === 28) {\n            return;\n        }\n        var isValidEIP155V = vInt === this.getChainId() * 2 + 35 || vInt === this.getChainId() * 2 + 36;\n        if (!isValidEIP155V) {\n            throw new Error(\"Incompatible EIP155-based V \" + vInt + \" and chain id \" + this.getChainId() + \". See the second parameter of the Transaction constructor to set the chain id.\");\n        }\n    };\n    Transaction.prototype._isSigned = function () {\n        return this.v.length > 0 && this.r.length > 0 && this.s.length > 0;\n    };\n    Transaction.prototype._overrideVSetterWithValidation = function () {\n        var _this = this;\n        var vDescriptor = Object.getOwnPropertyDescriptor(this, 'v');\n        Object.defineProperty(this, 'v', __assign({}, vDescriptor, { set: function (v) {\n                if (v !== undefined) {\n                    _this._validateV(ethereumjs_util_1.toBuffer(v));\n                }\n                vDescriptor.set(v);\n            } }));\n    };\n    Transaction.prototype._implementsEIP155 = function () {\n        var onEIP155BlockOrLater = this._common.gteHardfork('spuriousDragon');\n        if (!this._isSigned()) {\n            // We sign with EIP155 all unsigned transactions after spuriousDragon\n            return onEIP155BlockOrLater;\n        }\n        // EIP155 spec:\n        // If block.number >= 2,675,000 and v = CHAIN_ID * 2 + 35 or v = CHAIN_ID * 2 + 36, then when computing\n        // the hash of a transaction for purposes of signing or recovering, instead of hashing only the first six\n        // elements (i.e. nonce, gasprice, startgas, to, value, data), hash nine elements, with v replaced by\n        // CHAIN_ID, r = 0 and s = 0.\n        var v = ethereumjs_util_1.bufferToInt(this.v);\n        var vAndChainIdMeetEIP155Conditions = v === this.getChainId() * 2 + 35 || v === this.getChainId() * 2 + 36;\n        return vAndChainIdMeetEIP155Conditions && onEIP155BlockOrLater;\n    };\n    return Transaction;\n}());\nexports.default = Transaction;\n//# sourceMappingURL=transaction.js.map","'use strict';\n\nObject.defineProperty(exports, '__esModule', { value: true });\n\nvar is = require('./is.js');\nvar token = require('./token.js');\nvar bl = require('./bl.js');\nvar common = require('./common.js');\nvar jump = require('./jump.js');\nvar byteUtils = require('./byte-utils.js');\nvar _0uint = require('./0uint.js');\nvar _1negint = require('./1negint.js');\nvar _2bytes = require('./2bytes.js');\nvar _3string = require('./3string.js');\nvar _4array = require('./4array.js');\nvar _5map = require('./5map.js');\nvar _6tag = require('./6tag.js');\nvar _7float = require('./7float.js');\n\nconst defaultEncodeOptions = {\n  float64: false,\n  mapSorter,\n  quickEncodeToken: jump.quickEncodeToken\n};\nfunction makeCborEncoders() {\n  const encoders = [];\n  encoders[token.Type.uint.major] = _0uint.encodeUint;\n  encoders[token.Type.negint.major] = _1negint.encodeNegint;\n  encoders[token.Type.bytes.major] = _2bytes.encodeBytes;\n  encoders[token.Type.string.major] = _3string.encodeString;\n  encoders[token.Type.array.major] = _4array.encodeArray;\n  encoders[token.Type.map.major] = _5map.encodeMap;\n  encoders[token.Type.tag.major] = _6tag.encodeTag;\n  encoders[token.Type.float.major] = _7float.encodeFloat;\n  return encoders;\n}\nconst cborEncoders = makeCborEncoders();\nconst buf = new bl.Bl();\nclass Ref {\n  constructor(obj, parent) {\n    this.obj = obj;\n    this.parent = parent;\n  }\n  includes(obj) {\n    let p = this;\n    do {\n      if (p.obj === obj) {\n        return true;\n      }\n    } while (p = p.parent);\n    return false;\n  }\n  static createCheck(stack, obj) {\n    if (stack && stack.includes(obj)) {\n      throw new Error(`${ common.encodeErrPrefix } object contains circular references`);\n    }\n    return new Ref(obj, stack);\n  }\n}\nconst simpleTokens = {\n  null: new token.Token(token.Type.null, null),\n  undefined: new token.Token(token.Type.undefined, undefined),\n  true: new token.Token(token.Type.true, true),\n  false: new token.Token(token.Type.false, false),\n  emptyArray: new token.Token(token.Type.array, 0),\n  emptyMap: new token.Token(token.Type.map, 0)\n};\nconst typeEncoders = {\n  number(obj, _typ, _options, _refStack) {\n    if (!Number.isInteger(obj) || !Number.isSafeInteger(obj)) {\n      return new token.Token(token.Type.float, obj);\n    } else if (obj >= 0) {\n      return new token.Token(token.Type.uint, obj);\n    } else {\n      return new token.Token(token.Type.negint, obj);\n    }\n  },\n  bigint(obj, _typ, _options, _refStack) {\n    if (obj >= BigInt(0)) {\n      return new token.Token(token.Type.uint, obj);\n    } else {\n      return new token.Token(token.Type.negint, obj);\n    }\n  },\n  Uint8Array(obj, _typ, _options, _refStack) {\n    return new token.Token(token.Type.bytes, obj);\n  },\n  string(obj, _typ, _options, _refStack) {\n    return new token.Token(token.Type.string, obj);\n  },\n  boolean(obj, _typ, _options, _refStack) {\n    return obj ? simpleTokens.true : simpleTokens.false;\n  },\n  null(_obj, _typ, _options, _refStack) {\n    return simpleTokens.null;\n  },\n  undefined(_obj, _typ, _options, _refStack) {\n    return simpleTokens.undefined;\n  },\n  ArrayBuffer(obj, _typ, _options, _refStack) {\n    return new token.Token(token.Type.bytes, new Uint8Array(obj));\n  },\n  DataView(obj, _typ, _options, _refStack) {\n    return new token.Token(token.Type.bytes, new Uint8Array(obj.buffer, obj.byteOffset, obj.byteLength));\n  },\n  Array(obj, _typ, options, refStack) {\n    if (!obj.length) {\n      if (options.addBreakTokens === true) {\n        return [\n          simpleTokens.emptyArray,\n          new token.Token(token.Type.break)\n        ];\n      }\n      return simpleTokens.emptyArray;\n    }\n    refStack = Ref.createCheck(refStack, obj);\n    const entries = [];\n    let i = 0;\n    for (const e of obj) {\n      entries[i++] = objectToTokens(e, options, refStack);\n    }\n    if (options.addBreakTokens) {\n      return [\n        new token.Token(token.Type.array, obj.length),\n        entries,\n        new token.Token(token.Type.break)\n      ];\n    }\n    return [\n      new token.Token(token.Type.array, obj.length),\n      entries\n    ];\n  },\n  Object(obj, typ, options, refStack) {\n    const isMap = typ !== 'Object';\n    const keys = isMap ? obj.keys() : Object.keys(obj);\n    const length = isMap ? obj.size : keys.length;\n    if (!length) {\n      if (options.addBreakTokens === true) {\n        return [\n          simpleTokens.emptyMap,\n          new token.Token(token.Type.break)\n        ];\n      }\n      return simpleTokens.emptyMap;\n    }\n    refStack = Ref.createCheck(refStack, obj);\n    const entries = [];\n    let i = 0;\n    for (const key of keys) {\n      entries[i++] = [\n        objectToTokens(key, options, refStack),\n        objectToTokens(isMap ? obj.get(key) : obj[key], options, refStack)\n      ];\n    }\n    sortMapEntries(entries, options);\n    if (options.addBreakTokens) {\n      return [\n        new token.Token(token.Type.map, length),\n        entries,\n        new token.Token(token.Type.break)\n      ];\n    }\n    return [\n      new token.Token(token.Type.map, length),\n      entries\n    ];\n  }\n};\ntypeEncoders.Map = typeEncoders.Object;\ntypeEncoders.Buffer = typeEncoders.Uint8Array;\nfor (const typ of 'Uint8Clamped Uint16 Uint32 Int8 Int16 Int32 BigUint64 BigInt64 Float32 Float64'.split(' ')) {\n  typeEncoders[`${ typ }Array`] = typeEncoders.DataView;\n}\nfunction objectToTokens(obj, options = {}, refStack) {\n  const typ = is.is(obj);\n  const customTypeEncoder = options && options.typeEncoders && options.typeEncoders[typ] || typeEncoders[typ];\n  if (typeof customTypeEncoder === 'function') {\n    const tokens = customTypeEncoder(obj, typ, options, refStack);\n    if (tokens != null) {\n      return tokens;\n    }\n  }\n  const typeEncoder = typeEncoders[typ];\n  if (!typeEncoder) {\n    throw new Error(`${ common.encodeErrPrefix } unsupported type: ${ typ }`);\n  }\n  return typeEncoder(obj, typ, options, refStack);\n}\nfunction sortMapEntries(entries, options) {\n  if (options.mapSorter) {\n    entries.sort(options.mapSorter);\n  }\n}\nfunction mapSorter(e1, e2) {\n  const keyToken1 = Array.isArray(e1[0]) ? e1[0][0] : e1[0];\n  const keyToken2 = Array.isArray(e2[0]) ? e2[0][0] : e2[0];\n  if (keyToken1.type !== keyToken2.type) {\n    return keyToken1.type.compare(keyToken2.type);\n  }\n  const major = keyToken1.type.major;\n  const tcmp = cborEncoders[major].compareTokens(keyToken1, keyToken2);\n  if (tcmp === 0) {\n    console.warn('WARNING: complex key types used, CBOR key sorting guarantees are gone');\n  }\n  return tcmp;\n}\nfunction tokensToEncoded(buf, tokens, encoders, options) {\n  if (Array.isArray(tokens)) {\n    for (const token of tokens) {\n      tokensToEncoded(buf, token, encoders, options);\n    }\n  } else {\n    encoders[tokens.type.major](buf, tokens, options);\n  }\n}\nfunction encodeCustom(data, encoders, options) {\n  const tokens = objectToTokens(data, options);\n  if (!Array.isArray(tokens) && options.quickEncodeToken) {\n    const quickBytes = options.quickEncodeToken(tokens);\n    if (quickBytes) {\n      return quickBytes;\n    }\n    const encoder = encoders[tokens.type.major];\n    if (encoder.encodedSize) {\n      const size = encoder.encodedSize(tokens, options);\n      const buf = new bl.Bl(size);\n      encoder(buf, tokens, options);\n      if (buf.chunks.length !== 1) {\n        throw new Error(`Unexpected error: pre-calculated length for ${ tokens } was wrong`);\n      }\n      return byteUtils.asU8A(buf.chunks[0]);\n    }\n  }\n  buf.reset();\n  tokensToEncoded(buf, tokens, encoders, options);\n  return buf.toBytes(true);\n}\nfunction encode(data, options) {\n  options = Object.assign({}, defaultEncodeOptions, options);\n  return encodeCustom(data, cborEncoders, options);\n}\n\nexports.Ref = Ref;\nexports.encode = encode;\nexports.encodeCustom = encodeCustom;\nexports.makeCborEncoders = makeCborEncoders;\nexports.objectToTokens = objectToTokens;\n","'use strict';\n\nObject.defineProperty(exports, '__esModule', { value: true });\n\nvar token = require('./token.js');\nvar _0uint = require('./0uint.js');\nvar _1negint = require('./1negint.js');\nvar _2bytes = require('./2bytes.js');\nvar _3string = require('./3string.js');\nvar _4array = require('./4array.js');\nvar _5map = require('./5map.js');\nvar _6tag = require('./6tag.js');\nvar _7float = require('./7float.js');\nvar common = require('./common.js');\nvar byteUtils = require('./byte-utils.js');\n\nfunction invalidMinor(data, pos, minor) {\n  throw new Error(`${ common.decodeErrPrefix } encountered invalid minor (${ minor }) for major ${ data[pos] >>> 5 }`);\n}\nfunction errorer(msg) {\n  return () => {\n    throw new Error(`${ common.decodeErrPrefix } ${ msg }`);\n  };\n}\nconst jump = [];\nfor (let i = 0; i <= 23; i++) {\n  jump[i] = invalidMinor;\n}\njump[24] = _0uint.decodeUint8;\njump[25] = _0uint.decodeUint16;\njump[26] = _0uint.decodeUint32;\njump[27] = _0uint.decodeUint64;\njump[28] = invalidMinor;\njump[29] = invalidMinor;\njump[30] = invalidMinor;\njump[31] = invalidMinor;\nfor (let i = 32; i <= 55; i++) {\n  jump[i] = invalidMinor;\n}\njump[56] = _1negint.decodeNegint8;\njump[57] = _1negint.decodeNegint16;\njump[58] = _1negint.decodeNegint32;\njump[59] = _1negint.decodeNegint64;\njump[60] = invalidMinor;\njump[61] = invalidMinor;\njump[62] = invalidMinor;\njump[63] = invalidMinor;\nfor (let i = 64; i <= 87; i++) {\n  jump[i] = _2bytes.decodeBytesCompact;\n}\njump[88] = _2bytes.decodeBytes8;\njump[89] = _2bytes.decodeBytes16;\njump[90] = _2bytes.decodeBytes32;\njump[91] = _2bytes.decodeBytes64;\njump[92] = invalidMinor;\njump[93] = invalidMinor;\njump[94] = invalidMinor;\njump[95] = errorer('indefinite length bytes/strings are not supported');\nfor (let i = 96; i <= 119; i++) {\n  jump[i] = _3string.decodeStringCompact;\n}\njump[120] = _3string.decodeString8;\njump[121] = _3string.decodeString16;\njump[122] = _3string.decodeString32;\njump[123] = _3string.decodeString64;\njump[124] = invalidMinor;\njump[125] = invalidMinor;\njump[126] = invalidMinor;\njump[127] = errorer('indefinite length bytes/strings are not supported');\nfor (let i = 128; i <= 151; i++) {\n  jump[i] = _4array.decodeArrayCompact;\n}\njump[152] = _4array.decodeArray8;\njump[153] = _4array.decodeArray16;\njump[154] = _4array.decodeArray32;\njump[155] = _4array.decodeArray64;\njump[156] = invalidMinor;\njump[157] = invalidMinor;\njump[158] = invalidMinor;\njump[159] = _4array.decodeArrayIndefinite;\nfor (let i = 160; i <= 183; i++) {\n  jump[i] = _5map.decodeMapCompact;\n}\njump[184] = _5map.decodeMap8;\njump[185] = _5map.decodeMap16;\njump[186] = _5map.decodeMap32;\njump[187] = _5map.decodeMap64;\njump[188] = invalidMinor;\njump[189] = invalidMinor;\njump[190] = invalidMinor;\njump[191] = _5map.decodeMapIndefinite;\nfor (let i = 192; i <= 215; i++) {\n  jump[i] = _6tag.decodeTagCompact;\n}\njump[216] = _6tag.decodeTag8;\njump[217] = _6tag.decodeTag16;\njump[218] = _6tag.decodeTag32;\njump[219] = _6tag.decodeTag64;\njump[220] = invalidMinor;\njump[221] = invalidMinor;\njump[222] = invalidMinor;\njump[223] = invalidMinor;\nfor (let i = 224; i <= 243; i++) {\n  jump[i] = errorer('simple values are not supported');\n}\njump[244] = invalidMinor;\njump[245] = invalidMinor;\njump[246] = invalidMinor;\njump[247] = _7float.decodeUndefined;\njump[248] = errorer('simple values are not supported');\njump[249] = _7float.decodeFloat16;\njump[250] = _7float.decodeFloat32;\njump[251] = _7float.decodeFloat64;\njump[252] = invalidMinor;\njump[253] = invalidMinor;\njump[254] = invalidMinor;\njump[255] = _7float.decodeBreak;\nconst quick = [];\nfor (let i = 0; i < 24; i++) {\n  quick[i] = new token.Token(token.Type.uint, i, 1);\n}\nfor (let i = -1; i >= -24; i--) {\n  quick[31 - i] = new token.Token(token.Type.negint, i, 1);\n}\nquick[64] = new token.Token(token.Type.bytes, new Uint8Array(0), 1);\nquick[96] = new token.Token(token.Type.string, '', 1);\nquick[128] = new token.Token(token.Type.array, 0, 1);\nquick[160] = new token.Token(token.Type.map, 0, 1);\nquick[244] = new token.Token(token.Type.false, false, 1);\nquick[245] = new token.Token(token.Type.true, true, 1);\nquick[246] = new token.Token(token.Type.null, null, 1);\nfunction quickEncodeToken(token$1) {\n  switch (token$1.type) {\n  case token.Type.false:\n    return byteUtils.fromArray([244]);\n  case token.Type.true:\n    return byteUtils.fromArray([245]);\n  case token.Type.null:\n    return byteUtils.fromArray([246]);\n  case token.Type.bytes:\n    if (!token$1.value.length) {\n      return byteUtils.fromArray([64]);\n    }\n    return;\n  case token.Type.string:\n    if (token$1.value === '') {\n      return byteUtils.fromArray([96]);\n    }\n    return;\n  case token.Type.array:\n    if (token$1.value === 0) {\n      return byteUtils.fromArray([128]);\n    }\n    return;\n  case token.Type.map:\n    if (token$1.value === 0) {\n      return byteUtils.fromArray([160]);\n    }\n    return;\n  case token.Type.uint:\n    if (token$1.value < 24) {\n      return byteUtils.fromArray([Number(token$1.value)]);\n    }\n    return;\n  case token.Type.negint:\n    if (token$1.value >= -24) {\n      return byteUtils.fromArray([31 - Number(token$1.value)]);\n    }\n  }\n}\n\nexports.jump = jump;\nexports.quick = quick;\nexports.quickEncodeToken = quickEncodeToken;\n","'use strict';\n\nObject.defineProperty(exports, '__esModule', { value: true });\n\nvar token = require('./token.js');\nvar _0uint = require('./0uint.js');\nvar common = require('./common.js');\n\nfunction decodeNegint8(data, pos, _minor, options) {\n  return new token.Token(token.Type.negint, -1 - _0uint.readUint8(data, pos + 1, options), 2);\n}\nfunction decodeNegint16(data, pos, _minor, options) {\n  return new token.Token(token.Type.negint, -1 - _0uint.readUint16(data, pos + 1, options), 3);\n}\nfunction decodeNegint32(data, pos, _minor, options) {\n  return new token.Token(token.Type.negint, -1 - _0uint.readUint32(data, pos + 1, options), 5);\n}\nconst neg1b = BigInt(-1);\nconst pos1b = BigInt(1);\nfunction decodeNegint64(data, pos, _minor, options) {\n  const int = _0uint.readUint64(data, pos + 1, options);\n  if (typeof int !== 'bigint') {\n    const value = -1 - int;\n    if (value >= Number.MIN_SAFE_INTEGER) {\n      return new token.Token(token.Type.negint, value, 9);\n    }\n  }\n  if (options.allowBigInt !== true) {\n    throw new Error(`${ common.decodeErrPrefix } integers outside of the safe integer range are not supported`);\n  }\n  return new token.Token(token.Type.negint, neg1b - BigInt(int), 9);\n}\nfunction encodeNegint(buf, token) {\n  const negint = token.value;\n  const unsigned = typeof negint === 'bigint' ? negint * neg1b - pos1b : negint * -1 - 1;\n  _0uint.encodeUintValue(buf, token.type.majorEncoded, unsigned);\n}\nencodeNegint.encodedSize = function encodedSize(token) {\n  const negint = token.value;\n  const unsigned = typeof negint === 'bigint' ? negint * neg1b - pos1b : negint * -1 - 1;\n  if (unsigned < _0uint.uintBoundaries[0]) {\n    return 1;\n  }\n  if (unsigned < _0uint.uintBoundaries[1]) {\n    return 2;\n  }\n  if (unsigned < _0uint.uintBoundaries[2]) {\n    return 3;\n  }\n  if (unsigned < _0uint.uintBoundaries[3]) {\n    return 5;\n  }\n  return 9;\n};\nencodeNegint.compareTokens = function compareTokens(tok1, tok2) {\n  return tok1.value < tok2.value ? 1 : tok1.value > tok2.value ? -1 : 0;\n};\n\nexports.decodeNegint16 = decodeNegint16;\nexports.decodeNegint32 = decodeNegint32;\nexports.decodeNegint64 = decodeNegint64;\nexports.decodeNegint8 = decodeNegint8;\nexports.encodeNegint = encodeNegint;\n","'use strict';\n\nObject.defineProperty(exports, '__esModule', { value: true });\n\nvar token = require('./token.js');\nvar common = require('./common.js');\nvar _0uint = require('./0uint.js');\nvar _2bytes = require('./2bytes.js');\nvar byteUtils = require('./byte-utils.js');\n\nfunction toToken(data, pos, prefix, length, options) {\n  const totLength = prefix + length;\n  common.assertEnoughData(data, pos, totLength);\n  const tok = new token.Token(token.Type.string, byteUtils.toString(data, pos + prefix, pos + totLength), totLength);\n  if (options.retainStringBytes === true) {\n    tok.byteValue = byteUtils.slice(data, pos + prefix, pos + totLength);\n  }\n  return tok;\n}\nfunction decodeStringCompact(data, pos, minor, options) {\n  return toToken(data, pos, 1, minor, options);\n}\nfunction decodeString8(data, pos, _minor, options) {\n  return toToken(data, pos, 2, _0uint.readUint8(data, pos + 1, options), options);\n}\nfunction decodeString16(data, pos, _minor, options) {\n  return toToken(data, pos, 3, _0uint.readUint16(data, pos + 1, options), options);\n}\nfunction decodeString32(data, pos, _minor, options) {\n  return toToken(data, pos, 5, _0uint.readUint32(data, pos + 1, options), options);\n}\nfunction decodeString64(data, pos, _minor, options) {\n  const l = _0uint.readUint64(data, pos + 1, options);\n  if (typeof l === 'bigint') {\n    throw new Error(`${ common.decodeErrPrefix } 64-bit integer string lengths not supported`);\n  }\n  return toToken(data, pos, 9, l, options);\n}\nconst encodeString = _2bytes.encodeBytes;\n\nexports.decodeString16 = decodeString16;\nexports.decodeString32 = decodeString32;\nexports.decodeString64 = decodeString64;\nexports.decodeString8 = decodeString8;\nexports.decodeStringCompact = decodeStringCompact;\nexports.encodeString = encodeString;\n","'use strict';\n\nObject.defineProperty(exports, '__esModule', { value: true });\n\nvar token = require('./token.js');\nvar _0uint = require('./0uint.js');\nvar common = require('./common.js');\n\nfunction toToken(_data, _pos, prefix, length) {\n  return new token.Token(token.Type.array, length, prefix);\n}\nfunction decodeArrayCompact(data, pos, minor, _options) {\n  return toToken(data, pos, 1, minor);\n}\nfunction decodeArray8(data, pos, _minor, options) {\n  return toToken(data, pos, 2, _0uint.readUint8(data, pos + 1, options));\n}\nfunction decodeArray16(data, pos, _minor, options) {\n  return toToken(data, pos, 3, _0uint.readUint16(data, pos + 1, options));\n}\nfunction decodeArray32(data, pos, _minor, options) {\n  return toToken(data, pos, 5, _0uint.readUint32(data, pos + 1, options));\n}\nfunction decodeArray64(data, pos, _minor, options) {\n  const l = _0uint.readUint64(data, pos + 1, options);\n  if (typeof l === 'bigint') {\n    throw new Error(`${ common.decodeErrPrefix } 64-bit integer array lengths not supported`);\n  }\n  return toToken(data, pos, 9, l);\n}\nfunction decodeArrayIndefinite(data, pos, _minor, options) {\n  if (options.allowIndefinite === false) {\n    throw new Error(`${ common.decodeErrPrefix } indefinite length items not allowed`);\n  }\n  return toToken(data, pos, 1, Infinity);\n}\nfunction encodeArray(buf, token$1) {\n  _0uint.encodeUintValue(buf, token.Type.array.majorEncoded, token$1.value);\n}\nencodeArray.compareTokens = _0uint.encodeUint.compareTokens;\nencodeArray.encodedSize = function encodedSize(token) {\n  return _0uint.encodeUintValue.encodedSize(token.value);\n};\n\nexports.decodeArray16 = decodeArray16;\nexports.decodeArray32 = decodeArray32;\nexports.decodeArray64 = decodeArray64;\nexports.decodeArray8 = decodeArray8;\nexports.decodeArrayCompact = decodeArrayCompact;\nexports.decodeArrayIndefinite = decodeArrayIndefinite;\nexports.encodeArray = encodeArray;\n","'use strict';\n\nObject.defineProperty(exports, '__esModule', { value: true });\n\nvar token = require('./token.js');\nvar _0uint = require('./0uint.js');\nvar common = require('./common.js');\n\nfunction toToken(_data, _pos, prefix, length) {\n  return new token.Token(token.Type.map, length, prefix);\n}\nfunction decodeMapCompact(data, pos, minor, _options) {\n  return toToken(data, pos, 1, minor);\n}\nfunction decodeMap8(data, pos, _minor, options) {\n  return toToken(data, pos, 2, _0uint.readUint8(data, pos + 1, options));\n}\nfunction decodeMap16(data, pos, _minor, options) {\n  return toToken(data, pos, 3, _0uint.readUint16(data, pos + 1, options));\n}\nfunction decodeMap32(data, pos, _minor, options) {\n  return toToken(data, pos, 5, _0uint.readUint32(data, pos + 1, options));\n}\nfunction decodeMap64(data, pos, _minor, options) {\n  const l = _0uint.readUint64(data, pos + 1, options);\n  if (typeof l === 'bigint') {\n    throw new Error(`${ common.decodeErrPrefix } 64-bit integer map lengths not supported`);\n  }\n  return toToken(data, pos, 9, l);\n}\nfunction decodeMapIndefinite(data, pos, _minor, options) {\n  if (options.allowIndefinite === false) {\n    throw new Error(`${ common.decodeErrPrefix } indefinite length items not allowed`);\n  }\n  return toToken(data, pos, 1, Infinity);\n}\nfunction encodeMap(buf, token$1) {\n  _0uint.encodeUintValue(buf, token.Type.map.majorEncoded, token$1.value);\n}\nencodeMap.compareTokens = _0uint.encodeUint.compareTokens;\nencodeMap.encodedSize = function encodedSize(token) {\n  return _0uint.encodeUintValue.encodedSize(token.value);\n};\n\nexports.decodeMap16 = decodeMap16;\nexports.decodeMap32 = decodeMap32;\nexports.decodeMap64 = decodeMap64;\nexports.decodeMap8 = decodeMap8;\nexports.decodeMapCompact = decodeMapCompact;\nexports.decodeMapIndefinite = decodeMapIndefinite;\nexports.encodeMap = encodeMap;\n","'use strict';\n\nObject.defineProperty(exports, '__esModule', { value: true });\n\nvar token = require('./token.js');\nvar _0uint = require('./0uint.js');\n\nfunction decodeTagCompact(_data, _pos, minor, _options) {\n  return new token.Token(token.Type.tag, minor, 1);\n}\nfunction decodeTag8(data, pos, _minor, options) {\n  return new token.Token(token.Type.tag, _0uint.readUint8(data, pos + 1, options), 2);\n}\nfunction decodeTag16(data, pos, _minor, options) {\n  return new token.Token(token.Type.tag, _0uint.readUint16(data, pos + 1, options), 3);\n}\nfunction decodeTag32(data, pos, _minor, options) {\n  return new token.Token(token.Type.tag, _0uint.readUint32(data, pos + 1, options), 5);\n}\nfunction decodeTag64(data, pos, _minor, options) {\n  return new token.Token(token.Type.tag, _0uint.readUint64(data, pos + 1, options), 9);\n}\nfunction encodeTag(buf, token$1) {\n  _0uint.encodeUintValue(buf, token.Type.tag.majorEncoded, token$1.value);\n}\nencodeTag.compareTokens = _0uint.encodeUint.compareTokens;\nencodeTag.encodedSize = function encodedSize(token) {\n  return _0uint.encodeUintValue.encodedSize(token.value);\n};\n\nexports.decodeTag16 = decodeTag16;\nexports.decodeTag32 = decodeTag32;\nexports.decodeTag64 = decodeTag64;\nexports.decodeTag8 = decodeTag8;\nexports.decodeTagCompact = decodeTagCompact;\nexports.encodeTag = encodeTag;\n","'use strict';\n\nObject.defineProperty(exports, '__esModule', { value: true });\n\nvar token = require('./token.js');\nvar common = require('./common.js');\nvar _0uint = require('./0uint.js');\n\nconst MINOR_FALSE = 20;\nconst MINOR_TRUE = 21;\nconst MINOR_NULL = 22;\nconst MINOR_UNDEFINED = 23;\nfunction decodeUndefined(_data, _pos, _minor, options) {\n  if (options.allowUndefined === false) {\n    throw new Error(`${ common.decodeErrPrefix } undefined values are not supported`);\n  } else if (options.coerceUndefinedToNull === true) {\n    return new token.Token(token.Type.null, null, 1);\n  }\n  return new token.Token(token.Type.undefined, undefined, 1);\n}\nfunction decodeBreak(_data, _pos, _minor, options) {\n  if (options.allowIndefinite === false) {\n    throw new Error(`${ common.decodeErrPrefix } indefinite length items not allowed`);\n  }\n  return new token.Token(token.Type.break, undefined, 1);\n}\nfunction createToken(value, bytes, options) {\n  if (options) {\n    if (options.allowNaN === false && Number.isNaN(value)) {\n      throw new Error(`${ common.decodeErrPrefix } NaN values are not supported`);\n    }\n    if (options.allowInfinity === false && (value === Infinity || value === -Infinity)) {\n      throw new Error(`${ common.decodeErrPrefix } Infinity values are not supported`);\n    }\n  }\n  return new token.Token(token.Type.float, value, bytes);\n}\nfunction decodeFloat16(data, pos, _minor, options) {\n  return createToken(readFloat16(data, pos + 1), 3, options);\n}\nfunction decodeFloat32(data, pos, _minor, options) {\n  return createToken(readFloat32(data, pos + 1), 5, options);\n}\nfunction decodeFloat64(data, pos, _minor, options) {\n  return createToken(readFloat64(data, pos + 1), 9, options);\n}\nfunction encodeFloat(buf, token$1, options) {\n  const float = token$1.value;\n  if (float === false) {\n    buf.push([token.Type.float.majorEncoded | MINOR_FALSE]);\n  } else if (float === true) {\n    buf.push([token.Type.float.majorEncoded | MINOR_TRUE]);\n  } else if (float === null) {\n    buf.push([token.Type.float.majorEncoded | MINOR_NULL]);\n  } else if (float === undefined) {\n    buf.push([token.Type.float.majorEncoded | MINOR_UNDEFINED]);\n  } else {\n    let decoded;\n    let success = false;\n    if (!options || options.float64 !== true) {\n      encodeFloat16(float);\n      decoded = readFloat16(ui8a, 1);\n      if (float === decoded || Number.isNaN(float)) {\n        ui8a[0] = 249;\n        buf.push(ui8a.slice(0, 3));\n        success = true;\n      } else {\n        encodeFloat32(float);\n        decoded = readFloat32(ui8a, 1);\n        if (float === decoded) {\n          ui8a[0] = 250;\n          buf.push(ui8a.slice(0, 5));\n          success = true;\n        }\n      }\n    }\n    if (!success) {\n      encodeFloat64(float);\n      decoded = readFloat64(ui8a, 1);\n      ui8a[0] = 251;\n      buf.push(ui8a.slice(0, 9));\n    }\n  }\n}\nencodeFloat.encodedSize = function encodedSize(token, options) {\n  const float = token.value;\n  if (float === false || float === true || float === null || float === undefined) {\n    return 1;\n  }\n  if (!options || options.float64 !== true) {\n    encodeFloat16(float);\n    let decoded = readFloat16(ui8a, 1);\n    if (float === decoded || Number.isNaN(float)) {\n      return 3;\n    }\n    encodeFloat32(float);\n    decoded = readFloat32(ui8a, 1);\n    if (float === decoded) {\n      return 5;\n    }\n  }\n  return 9;\n};\nconst buffer = new ArrayBuffer(9);\nconst dataView = new DataView(buffer, 1);\nconst ui8a = new Uint8Array(buffer, 0);\nfunction encodeFloat16(inp) {\n  if (inp === Infinity) {\n    dataView.setUint16(0, 31744, false);\n  } else if (inp === -Infinity) {\n    dataView.setUint16(0, 64512, false);\n  } else if (Number.isNaN(inp)) {\n    dataView.setUint16(0, 32256, false);\n  } else {\n    dataView.setFloat32(0, inp);\n    const valu32 = dataView.getUint32(0);\n    const exponent = (valu32 & 2139095040) >> 23;\n    const mantissa = valu32 & 8388607;\n    if (exponent === 255) {\n      dataView.setUint16(0, 31744, false);\n    } else if (exponent === 0) {\n      dataView.setUint16(0, (inp & 2147483648) >> 16 | mantissa >> 13, false);\n    } else {\n      const logicalExponent = exponent - 127;\n      if (logicalExponent < -24) {\n        dataView.setUint16(0, 0);\n      } else if (logicalExponent < -14) {\n        dataView.setUint16(0, (valu32 & 2147483648) >> 16 | 1 << 24 + logicalExponent, false);\n      } else {\n        dataView.setUint16(0, (valu32 & 2147483648) >> 16 | logicalExponent + 15 << 10 | mantissa >> 13, false);\n      }\n    }\n  }\n}\nfunction readFloat16(ui8a, pos) {\n  if (ui8a.length - pos < 2) {\n    throw new Error(`${ common.decodeErrPrefix } not enough data for float16`);\n  }\n  const half = (ui8a[pos] << 8) + ui8a[pos + 1];\n  if (half === 31744) {\n    return Infinity;\n  }\n  if (half === 64512) {\n    return -Infinity;\n  }\n  if (half === 32256) {\n    return NaN;\n  }\n  const exp = half >> 10 & 31;\n  const mant = half & 1023;\n  let val;\n  if (exp === 0) {\n    val = mant * 2 ** -24;\n  } else if (exp !== 31) {\n    val = (mant + 1024) * 2 ** (exp - 25);\n  } else {\n    val = mant === 0 ? Infinity : NaN;\n  }\n  return half & 32768 ? -val : val;\n}\nfunction encodeFloat32(inp) {\n  dataView.setFloat32(0, inp, false);\n}\nfunction readFloat32(ui8a, pos) {\n  if (ui8a.length - pos < 4) {\n    throw new Error(`${ common.decodeErrPrefix } not enough data for float32`);\n  }\n  const offset = (ui8a.byteOffset || 0) + pos;\n  return new DataView(ui8a.buffer, offset, 4).getFloat32(0, false);\n}\nfunction encodeFloat64(inp) {\n  dataView.setFloat64(0, inp, false);\n}\nfunction readFloat64(ui8a, pos) {\n  if (ui8a.length - pos < 8) {\n    throw new Error(`${ common.decodeErrPrefix } not enough data for float64`);\n  }\n  const offset = (ui8a.byteOffset || 0) + pos;\n  return new DataView(ui8a.buffer, offset, 8).getFloat64(0, false);\n}\nencodeFloat.compareTokens = _0uint.encodeUint.compareTokens;\n\nexports.decodeBreak = decodeBreak;\nexports.decodeFloat16 = decodeFloat16;\nexports.decodeFloat32 = decodeFloat32;\nexports.decodeFloat64 = decodeFloat64;\nexports.decodeUndefined = decodeUndefined;\nexports.encodeFloat = encodeFloat;\n","'use strict';\n\nObject.defineProperty(exports, '__esModule', { value: true });\n\nvar common = require('./common.js');\nvar token = require('./token.js');\nvar jump = require('./jump.js');\n\nconst defaultDecodeOptions = {\n  strict: false,\n  allowIndefinite: true,\n  allowUndefined: true,\n  allowBigInt: true\n};\nclass Tokeniser {\n  constructor(data, options = {}) {\n    this.pos = 0;\n    this.data = data;\n    this.options = options;\n  }\n  done() {\n    return this.pos >= this.data.length;\n  }\n  next() {\n    const byt = this.data[this.pos];\n    let token = jump.quick[byt];\n    if (token === undefined) {\n      const decoder = jump.jump[byt];\n      if (!decoder) {\n        throw new Error(`${ common.decodeErrPrefix } no decoder for major type ${ byt >>> 5 } (byte 0x${ byt.toString(16).padStart(2, '0') })`);\n      }\n      const minor = byt & 31;\n      token = decoder(this.data, this.pos, minor, this.options);\n    }\n    this.pos += token.encodedLength;\n    return token;\n  }\n}\nconst DONE = Symbol.for('DONE');\nconst BREAK = Symbol.for('BREAK');\nfunction tokenToArray(token, tokeniser, options) {\n  const arr = [];\n  for (let i = 0; i < token.value; i++) {\n    const value = tokensToObject(tokeniser, options);\n    if (value === BREAK) {\n      if (token.value === Infinity) {\n        break;\n      }\n      throw new Error(`${ common.decodeErrPrefix } got unexpected break to lengthed array`);\n    }\n    if (value === DONE) {\n      throw new Error(`${ common.decodeErrPrefix } found array but not enough entries (got ${ i }, expected ${ token.value })`);\n    }\n    arr[i] = value;\n  }\n  return arr;\n}\nfunction tokenToMap(token, tokeniser, options) {\n  const useMaps = options.useMaps === true;\n  const obj = useMaps ? undefined : {};\n  const m = useMaps ? new Map() : undefined;\n  for (let i = 0; i < token.value; i++) {\n    const key = tokensToObject(tokeniser, options);\n    if (key === BREAK) {\n      if (token.value === Infinity) {\n        break;\n      }\n      throw new Error(`${ common.decodeErrPrefix } got unexpected break to lengthed map`);\n    }\n    if (key === DONE) {\n      throw new Error(`${ common.decodeErrPrefix } found map but not enough entries (got ${ i } [no key], expected ${ token.value })`);\n    }\n    if (useMaps !== true && typeof key !== 'string') {\n      throw new Error(`${ common.decodeErrPrefix } non-string keys not supported (got ${ typeof key })`);\n    }\n    const value = tokensToObject(tokeniser, options);\n    if (value === DONE) {\n      throw new Error(`${ common.decodeErrPrefix } found map but not enough entries (got ${ i } [no value], expected ${ token.value })`);\n    }\n    if (useMaps) {\n      m.set(key, value);\n    } else {\n      obj[key] = value;\n    }\n  }\n  return useMaps ? m : obj;\n}\nfunction tokensToObject(tokeniser, options) {\n  if (tokeniser.done()) {\n    return DONE;\n  }\n  const token$1 = tokeniser.next();\n  if (token$1.type === token.Type.break) {\n    return BREAK;\n  }\n  if (token$1.type.terminal) {\n    return token$1.value;\n  }\n  if (token$1.type === token.Type.array) {\n    return tokenToArray(token$1, tokeniser, options);\n  }\n  if (token$1.type === token.Type.map) {\n    return tokenToMap(token$1, tokeniser, options);\n  }\n  if (token$1.type === token.Type.tag) {\n    if (options.tags && typeof options.tags[token$1.value] === 'function') {\n      const tagged = tokensToObject(tokeniser, options);\n      return options.tags[token$1.value](tagged);\n    }\n    throw new Error(`${ common.decodeErrPrefix } tag not supported (${ token$1.value })`);\n  }\n  throw new Error('unsupported');\n}\nfunction decode(data, options) {\n  if (!(data instanceof Uint8Array)) {\n    throw new Error(`${ common.decodeErrPrefix } data to decode must be a Uint8Array`);\n  }\n  options = Object.assign({}, defaultDecodeOptions, options);\n  const tokeniser = options.tokenizer || new Tokeniser(data, options);\n  const decoded = tokensToObject(tokeniser, options);\n  if (decoded === DONE) {\n    throw new Error(`${ common.decodeErrPrefix } did not find any content to decode`);\n  }\n  if (decoded === BREAK) {\n    throw new Error(`${ common.decodeErrPrefix } got unexpected break`);\n  }\n  if (!tokeniser.done()) {\n    throw new Error(`${ common.decodeErrPrefix } too many terminals, data makes no sense`);\n  }\n  return decoded;\n}\n\nexports.Tokeniser = Tokeniser;\nexports.decode = decode;\nexports.tokensToObject = tokensToObject;\n","'use strict'\n\nconst util = require('./util.js')\nconst resolver = require('./resolver.js')\n\n/**\n * @typedef {import('interface-ipld-format').Format<object>} ObjectFormat\n */\n\n/**\n * @type {ObjectFormat}\n */\nmodule.exports = {\n  util,\n  resolver,\n  codec: util.codec,\n  defaultHashAlg: util.defaultHashAlg\n}\n","'use strict'\n\nconst baseX = require('@multiformats/base-x')\nconst Base = require('./base.js')\nconst { rfc4648 } = require('./rfc4648')\nconst { decodeText, encodeText } = require('./util')\n\n/** @typedef {import('./types').CodecFactory} CodecFactory */\n/** @typedef {import('./types').Codec} Codec */\n/** @typedef {import('./types').BaseName} BaseName */\n/** @typedef {import('./types').BaseCode} BaseCode */\n\n/** @type {CodecFactory} */\nconst identity = () => {\n  return {\n    encode: decodeText,\n    decode: encodeText\n  }\n}\n\n/**\n *\n * name, code, implementation, alphabet\n *\n * @type {Array<[BaseName, BaseCode, CodecFactory, string]>}\n */\nconst constants = [\n  ['identity', '\\x00', identity, ''],\n  ['base2', '0', rfc4648(1), '01'],\n  ['base8', '7', rfc4648(3), '01234567'],\n  ['base10', '9', baseX, '0123456789'],\n  ['base16', 'f', rfc4648(4), '0123456789abcdef'],\n  ['base16upper', 'F', rfc4648(4), '0123456789ABCDEF'],\n  ['base32hex', 'v', rfc4648(5), '0123456789abcdefghijklmnopqrstuv'],\n  ['base32hexupper', 'V', rfc4648(5), '0123456789ABCDEFGHIJKLMNOPQRSTUV'],\n  ['base32hexpad', 't', rfc4648(5), '0123456789abcdefghijklmnopqrstuv='],\n  ['base32hexpadupper', 'T', rfc4648(5), '0123456789ABCDEFGHIJKLMNOPQRSTUV='],\n  ['base32', 'b', rfc4648(5), 'abcdefghijklmnopqrstuvwxyz234567'],\n  ['base32upper', 'B', rfc4648(5), 'ABCDEFGHIJKLMNOPQRSTUVWXYZ234567'],\n  ['base32pad', 'c', rfc4648(5), 'abcdefghijklmnopqrstuvwxyz234567='],\n  ['base32padupper', 'C', rfc4648(5), 'ABCDEFGHIJKLMNOPQRSTUVWXYZ234567='],\n  ['base32z', 'h', rfc4648(5), 'ybndrfg8ejkmcpqxot1uwisza345h769'],\n  ['base36', 'k', baseX, '0123456789abcdefghijklmnopqrstuvwxyz'],\n  ['base36upper', 'K', baseX, '0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ'],\n  ['base58btc', 'z', baseX, '123456789ABCDEFGHJKLMNPQRSTUVWXYZabcdefghijkmnopqrstuvwxyz'],\n  ['base58flickr', 'Z', baseX, '123456789abcdefghijkmnopqrstuvwxyzABCDEFGHJKLMNPQRSTUVWXYZ'],\n  ['base64', 'm', rfc4648(6), 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/'],\n  ['base64pad', 'M', rfc4648(6), 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/='],\n  ['base64url', 'u', rfc4648(6), 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789-_'],\n  ['base64urlpad', 'U', rfc4648(6), 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789-_=']\n]\n\n/** @type {Record<BaseName,Base>} */\nconst names = constants.reduce((prev, tupple) => {\n  prev[tupple[0]] = new Base(tupple[0], tupple[1], tupple[2], tupple[3])\n  return prev\n}, /** @type {Record<BaseName,Base>} */({}))\n\n/** @type {Record<BaseCode,Base>} */\nconst codes = constants.reduce((prev, tupple) => {\n  prev[tupple[1]] = names[tupple[0]]\n  return prev\n}, /** @type {Record<BaseCode,Base>} */({}))\n\nmodule.exports = {\n  names,\n  codes\n}\n","'use strict'\n\nconst { encodeText } = require('./util')\n\n/** @typedef {import('./types').CodecFactory} CodecFactory */\n/** @typedef {import(\"./types\").BaseName} BaseName */\n/** @typedef {import(\"./types\").BaseCode} BaseCode */\n\n/**\n * Class to encode/decode in the supported Bases\n *\n */\nclass Base {\n  /**\n   * @param {BaseName} name\n   * @param {BaseCode} code\n   * @param {CodecFactory} factory\n   * @param {string} alphabet\n   */\n  constructor (name, code, factory, alphabet) {\n    this.name = name\n    this.code = code\n    this.codeBuf = encodeText(this.code)\n    this.alphabet = alphabet\n    this.codec = factory(alphabet)\n  }\n\n  /**\n   * @param {Uint8Array} buf\n   * @returns {string}\n   */\n  encode (buf) {\n    return this.codec.encode(buf)\n  }\n\n  /**\n   * @param {string} string\n   * @returns {Uint8Array}\n   */\n  decode (string) {\n    for (const char of string) {\n      if (this.alphabet && this.alphabet.indexOf(char) < 0) {\n        throw new Error(`invalid character '${char}' in '${string}'`)\n      }\n    }\n    return this.codec.decode(string)\n  }\n}\n\nmodule.exports = Base\n","'use strict'\n\n/** @typedef {import('./types').CodecFactory} CodecFactory */\n\n/**\n * @param {string} string\n * @param {string} alphabet\n * @param {number} bitsPerChar\n * @returns {Uint8Array}\n */\nconst decode = (string, alphabet, bitsPerChar) => {\n  // Build the character lookup table:\n  /** @type {Record<string, number>} */\n  const codes = {}\n  for (let i = 0; i < alphabet.length; ++i) {\n    codes[alphabet[i]] = i\n  }\n\n  // Count the padding bytes:\n  let end = string.length\n  while (string[end - 1] === '=') {\n    --end\n  }\n\n  // Allocate the output:\n  const out = new Uint8Array((end * bitsPerChar / 8) | 0)\n\n  // Parse the data:\n  let bits = 0 // Number of bits currently in the buffer\n  let buffer = 0 // Bits waiting to be written out, MSB first\n  let written = 0 // Next byte to write\n  for (let i = 0; i < end; ++i) {\n    // Read one character from the string:\n    const value = codes[string[i]]\n    if (value === undefined) {\n      throw new SyntaxError('Invalid character ' + string[i])\n    }\n\n    // Append the bits to the buffer:\n    buffer = (buffer << bitsPerChar) | value\n    bits += bitsPerChar\n\n    // Write out some bits if the buffer has a byte's worth:\n    if (bits >= 8) {\n      bits -= 8\n      out[written++] = 0xff & (buffer >> bits)\n    }\n  }\n\n  // Verify that we have received just enough bits:\n  if (bits >= bitsPerChar || 0xff & (buffer << (8 - bits))) {\n    throw new SyntaxError('Unexpected end of data')\n  }\n\n  return out\n}\n\n/**\n * @param {Uint8Array} data\n * @param {string} alphabet\n * @param {number} bitsPerChar\n * @returns {string}\n */\nconst encode = (data, alphabet, bitsPerChar) => {\n  const pad = alphabet[alphabet.length - 1] === '='\n  const mask = (1 << bitsPerChar) - 1\n  let out = ''\n\n  let bits = 0 // Number of bits currently in the buffer\n  let buffer = 0 // Bits waiting to be written out, MSB first\n  for (let i = 0; i < data.length; ++i) {\n    // Slurp data into the buffer:\n    buffer = (buffer << 8) | data[i]\n    bits += 8\n\n    // Write out as much as we can:\n    while (bits > bitsPerChar) {\n      bits -= bitsPerChar\n      out += alphabet[mask & (buffer >> bits)]\n    }\n  }\n\n  // Partial character:\n  if (bits) {\n    out += alphabet[mask & (buffer << (bitsPerChar - bits))]\n  }\n\n  // Add padding characters until we hit a byte boundary:\n  if (pad) {\n    while ((out.length * bitsPerChar) & 7) {\n      out += '='\n    }\n  }\n\n  return out\n}\n\n/**\n * RFC4648 Factory\n *\n * @param {number} bitsPerChar\n * @returns {CodecFactory}\n */\nconst rfc4648 = (bitsPerChar) => (alphabet) => {\n  return {\n    /**\n     * @param {Uint8Array} input\n     * @returns {string}\n     */\n    encode (input) {\n      return encode(input, alphabet, bitsPerChar)\n    },\n    /**\n     * @param {string} input\n     * @returns {Uint8Array}\n     */\n    decode (input) {\n      return decode(input, alphabet, bitsPerChar)\n    }\n  }\n}\n\nmodule.exports = { rfc4648 }\n","'use strict'\n\n/** @typedef {import('./generated-types').ConstantCodeMap} ConstantCodeMap */\n/** @typedef {import('./generated-types').NameUint8ArrayMap} NameUint8ArrayMap */\n/** @typedef {import('./generated-types').CodeNameMap} CodeNameMap */\n/** @typedef {import('./generated-types').CodecName} CodecName */\n/** @typedef {import('./generated-types').CodecConstant} CodecConstant */\n\nconst { baseTable } = require('./generated-table')\nconst varintEncode = require('./util').varintEncode\n\nconst nameToVarint = /** @type {NameUint8ArrayMap} */ ({})\nconst constantToCode = /** @type {ConstantCodeMap} */({})\nconst codeToName = /** @type {CodeNameMap} */({})\n\n// eslint-disable-next-line guard-for-in\nfor (const name in baseTable) {\n  const codecName = /** @type {CodecName} */(name)\n  const code = baseTable[codecName]\n  nameToVarint[codecName] = varintEncode(code)\n\n  const constant = /** @type {CodecConstant} */(codecName.toUpperCase().replace(/-/g, '_'))\n  constantToCode[constant] = code\n\n  if (!codeToName[code]) {\n    codeToName[code] = codecName\n  }\n}\n\nObject.freeze(nameToVarint)\nObject.freeze(constantToCode)\nObject.freeze(codeToName)\nconst nameToCode = Object.freeze(baseTable)\nmodule.exports = {\n  nameToVarint,\n  constantToCode,\n  nameToCode,\n  codeToName\n}\n","// DO NOT CHANGE THIS FILE. IT IS GENERATED BY tools/update-table.js\n/* eslint quote-props: off */\n'use strict'\n\n/**\n * @type {import('./generated-types').NameCodeMap}\n */\nconst baseTable = Object.freeze({\n  'identity': 0x00,\n  'cidv1': 0x01,\n  'cidv2': 0x02,\n  'cidv3': 0x03,\n  'ip4': 0x04,\n  'tcp': 0x06,\n  'sha1': 0x11,\n  'sha2-256': 0x12,\n  'sha2-512': 0x13,\n  'sha3-512': 0x14,\n  'sha3-384': 0x15,\n  'sha3-256': 0x16,\n  'sha3-224': 0x17,\n  'shake-128': 0x18,\n  'shake-256': 0x19,\n  'keccak-224': 0x1a,\n  'keccak-256': 0x1b,\n  'keccak-384': 0x1c,\n  'keccak-512': 0x1d,\n  'blake3': 0x1e,\n  'dccp': 0x21,\n  'murmur3-128': 0x22,\n  'murmur3-32': 0x23,\n  'ip6': 0x29,\n  'ip6zone': 0x2a,\n  'path': 0x2f,\n  'multicodec': 0x30,\n  'multihash': 0x31,\n  'multiaddr': 0x32,\n  'multibase': 0x33,\n  'dns': 0x35,\n  'dns4': 0x36,\n  'dns6': 0x37,\n  'dnsaddr': 0x38,\n  'protobuf': 0x50,\n  'cbor': 0x51,\n  'raw': 0x55,\n  'dbl-sha2-256': 0x56,\n  'rlp': 0x60,\n  'bencode': 0x63,\n  'dag-pb': 0x70,\n  'dag-cbor': 0x71,\n  'libp2p-key': 0x72,\n  'git-raw': 0x78,\n  'torrent-info': 0x7b,\n  'torrent-file': 0x7c,\n  'leofcoin-block': 0x81,\n  'leofcoin-tx': 0x82,\n  'leofcoin-pr': 0x83,\n  'sctp': 0x84,\n  'dag-jose': 0x85,\n  'dag-cose': 0x86,\n  'eth-block': 0x90,\n  'eth-block-list': 0x91,\n  'eth-tx-trie': 0x92,\n  'eth-tx': 0x93,\n  'eth-tx-receipt-trie': 0x94,\n  'eth-tx-receipt': 0x95,\n  'eth-state-trie': 0x96,\n  'eth-account-snapshot': 0x97,\n  'eth-storage-trie': 0x98,\n  'bitcoin-block': 0xb0,\n  'bitcoin-tx': 0xb1,\n  'bitcoin-witness-commitment': 0xb2,\n  'zcash-block': 0xc0,\n  'zcash-tx': 0xc1,\n  'docid': 0xce,\n  'stellar-block': 0xd0,\n  'stellar-tx': 0xd1,\n  'md4': 0xd4,\n  'md5': 0xd5,\n  'bmt': 0xd6,\n  'decred-block': 0xe0,\n  'decred-tx': 0xe1,\n  'ipld-ns': 0xe2,\n  'ipfs-ns': 0xe3,\n  'swarm-ns': 0xe4,\n  'ipns-ns': 0xe5,\n  'zeronet': 0xe6,\n  'secp256k1-pub': 0xe7,\n  'bls12_381-g1-pub': 0xea,\n  'bls12_381-g2-pub': 0xeb,\n  'x25519-pub': 0xec,\n  'ed25519-pub': 0xed,\n  'bls12_381-g1g2-pub': 0xee,\n  'dash-block': 0xf0,\n  'dash-tx': 0xf1,\n  'swarm-manifest': 0xfa,\n  'swarm-feed': 0xfb,\n  'udp': 0x0111,\n  'p2p-webrtc-star': 0x0113,\n  'p2p-webrtc-direct': 0x0114,\n  'p2p-stardust': 0x0115,\n  'p2p-circuit': 0x0122,\n  'dag-json': 0x0129,\n  'udt': 0x012d,\n  'utp': 0x012e,\n  'unix': 0x0190,\n  'thread': 0x0196,\n  'p2p': 0x01a5,\n  'ipfs': 0x01a5,\n  'https': 0x01bb,\n  'onion': 0x01bc,\n  'onion3': 0x01bd,\n  'garlic64': 0x01be,\n  'garlic32': 0x01bf,\n  'tls': 0x01c0,\n  'quic': 0x01cc,\n  'ws': 0x01dd,\n  'wss': 0x01de,\n  'p2p-websocket-star': 0x01df,\n  'http': 0x01e0,\n  'json': 0x0200,\n  'messagepack': 0x0201,\n  'libp2p-peer-record': 0x0301,\n  'sha2-256-trunc254-padded': 0x1012,\n  'ripemd-128': 0x1052,\n  'ripemd-160': 0x1053,\n  'ripemd-256': 0x1054,\n  'ripemd-320': 0x1055,\n  'x11': 0x1100,\n  'p256-pub': 0x1200,\n  'p384-pub': 0x1201,\n  'p521-pub': 0x1202,\n  'ed448-pub': 0x1203,\n  'x448-pub': 0x1204,\n  'ed25519-priv': 0x1300,\n  'kangarootwelve': 0x1d01,\n  'sm3-256': 0x534d,\n  'blake2b-8': 0xb201,\n  'blake2b-16': 0xb202,\n  'blake2b-24': 0xb203,\n  'blake2b-32': 0xb204,\n  'blake2b-40': 0xb205,\n  'blake2b-48': 0xb206,\n  'blake2b-56': 0xb207,\n  'blake2b-64': 0xb208,\n  'blake2b-72': 0xb209,\n  'blake2b-80': 0xb20a,\n  'blake2b-88': 0xb20b,\n  'blake2b-96': 0xb20c,\n  'blake2b-104': 0xb20d,\n  'blake2b-112': 0xb20e,\n  'blake2b-120': 0xb20f,\n  'blake2b-128': 0xb210,\n  'blake2b-136': 0xb211,\n  'blake2b-144': 0xb212,\n  'blake2b-152': 0xb213,\n  'blake2b-160': 0xb214,\n  'blake2b-168': 0xb215,\n  'blake2b-176': 0xb216,\n  'blake2b-184': 0xb217,\n  'blake2b-192': 0xb218,\n  'blake2b-200': 0xb219,\n  'blake2b-208': 0xb21a,\n  'blake2b-216': 0xb21b,\n  'blake2b-224': 0xb21c,\n  'blake2b-232': 0xb21d,\n  'blake2b-240': 0xb21e,\n  'blake2b-248': 0xb21f,\n  'blake2b-256': 0xb220,\n  'blake2b-264': 0xb221,\n  'blake2b-272': 0xb222,\n  'blake2b-280': 0xb223,\n  'blake2b-288': 0xb224,\n  'blake2b-296': 0xb225,\n  'blake2b-304': 0xb226,\n  'blake2b-312': 0xb227,\n  'blake2b-320': 0xb228,\n  'blake2b-328': 0xb229,\n  'blake2b-336': 0xb22a,\n  'blake2b-344': 0xb22b,\n  'blake2b-352': 0xb22c,\n  'blake2b-360': 0xb22d,\n  'blake2b-368': 0xb22e,\n  'blake2b-376': 0xb22f,\n  'blake2b-384': 0xb230,\n  'blake2b-392': 0xb231,\n  'blake2b-400': 0xb232,\n  'blake2b-408': 0xb233,\n  'blake2b-416': 0xb234,\n  'blake2b-424': 0xb235,\n  'blake2b-432': 0xb236,\n  'blake2b-440': 0xb237,\n  'blake2b-448': 0xb238,\n  'blake2b-456': 0xb239,\n  'blake2b-464': 0xb23a,\n  'blake2b-472': 0xb23b,\n  'blake2b-480': 0xb23c,\n  'blake2b-488': 0xb23d,\n  'blake2b-496': 0xb23e,\n  'blake2b-504': 0xb23f,\n  'blake2b-512': 0xb240,\n  'blake2s-8': 0xb241,\n  'blake2s-16': 0xb242,\n  'blake2s-24': 0xb243,\n  'blake2s-32': 0xb244,\n  'blake2s-40': 0xb245,\n  'blake2s-48': 0xb246,\n  'blake2s-56': 0xb247,\n  'blake2s-64': 0xb248,\n  'blake2s-72': 0xb249,\n  'blake2s-80': 0xb24a,\n  'blake2s-88': 0xb24b,\n  'blake2s-96': 0xb24c,\n  'blake2s-104': 0xb24d,\n  'blake2s-112': 0xb24e,\n  'blake2s-120': 0xb24f,\n  'blake2s-128': 0xb250,\n  'blake2s-136': 0xb251,\n  'blake2s-144': 0xb252,\n  'blake2s-152': 0xb253,\n  'blake2s-160': 0xb254,\n  'blake2s-168': 0xb255,\n  'blake2s-176': 0xb256,\n  'blake2s-184': 0xb257,\n  'blake2s-192': 0xb258,\n  'blake2s-200': 0xb259,\n  'blake2s-208': 0xb25a,\n  'blake2s-216': 0xb25b,\n  'blake2s-224': 0xb25c,\n  'blake2s-232': 0xb25d,\n  'blake2s-240': 0xb25e,\n  'blake2s-248': 0xb25f,\n  'blake2s-256': 0xb260,\n  'skein256-8': 0xb301,\n  'skein256-16': 0xb302,\n  'skein256-24': 0xb303,\n  'skein256-32': 0xb304,\n  'skein256-40': 0xb305,\n  'skein256-48': 0xb306,\n  'skein256-56': 0xb307,\n  'skein256-64': 0xb308,\n  'skein256-72': 0xb309,\n  'skein256-80': 0xb30a,\n  'skein256-88': 0xb30b,\n  'skein256-96': 0xb30c,\n  'skein256-104': 0xb30d,\n  'skein256-112': 0xb30e,\n  'skein256-120': 0xb30f,\n  'skein256-128': 0xb310,\n  'skein256-136': 0xb311,\n  'skein256-144': 0xb312,\n  'skein256-152': 0xb313,\n  'skein256-160': 0xb314,\n  'skein256-168': 0xb315,\n  'skein256-176': 0xb316,\n  'skein256-184': 0xb317,\n  'skein256-192': 0xb318,\n  'skein256-200': 0xb319,\n  'skein256-208': 0xb31a,\n  'skein256-216': 0xb31b,\n  'skein256-224': 0xb31c,\n  'skein256-232': 0xb31d,\n  'skein256-240': 0xb31e,\n  'skein256-248': 0xb31f,\n  'skein256-256': 0xb320,\n  'skein512-8': 0xb321,\n  'skein512-16': 0xb322,\n  'skein512-24': 0xb323,\n  'skein512-32': 0xb324,\n  'skein512-40': 0xb325,\n  'skein512-48': 0xb326,\n  'skein512-56': 0xb327,\n  'skein512-64': 0xb328,\n  'skein512-72': 0xb329,\n  'skein512-80': 0xb32a,\n  'skein512-88': 0xb32b,\n  'skein512-96': 0xb32c,\n  'skein512-104': 0xb32d,\n  'skein512-112': 0xb32e,\n  'skein512-120': 0xb32f,\n  'skein512-128': 0xb330,\n  'skein512-136': 0xb331,\n  'skein512-144': 0xb332,\n  'skein512-152': 0xb333,\n  'skein512-160': 0xb334,\n  'skein512-168': 0xb335,\n  'skein512-176': 0xb336,\n  'skein512-184': 0xb337,\n  'skein512-192': 0xb338,\n  'skein512-200': 0xb339,\n  'skein512-208': 0xb33a,\n  'skein512-216': 0xb33b,\n  'skein512-224': 0xb33c,\n  'skein512-232': 0xb33d,\n  'skein512-240': 0xb33e,\n  'skein512-248': 0xb33f,\n  'skein512-256': 0xb340,\n  'skein512-264': 0xb341,\n  'skein512-272': 0xb342,\n  'skein512-280': 0xb343,\n  'skein512-288': 0xb344,\n  'skein512-296': 0xb345,\n  'skein512-304': 0xb346,\n  'skein512-312': 0xb347,\n  'skein512-320': 0xb348,\n  'skein512-328': 0xb349,\n  'skein512-336': 0xb34a,\n  'skein512-344': 0xb34b,\n  'skein512-352': 0xb34c,\n  'skein512-360': 0xb34d,\n  'skein512-368': 0xb34e,\n  'skein512-376': 0xb34f,\n  'skein512-384': 0xb350,\n  'skein512-392': 0xb351,\n  'skein512-400': 0xb352,\n  'skein512-408': 0xb353,\n  'skein512-416': 0xb354,\n  'skein512-424': 0xb355,\n  'skein512-432': 0xb356,\n  'skein512-440': 0xb357,\n  'skein512-448': 0xb358,\n  'skein512-456': 0xb359,\n  'skein512-464': 0xb35a,\n  'skein512-472': 0xb35b,\n  'skein512-480': 0xb35c,\n  'skein512-488': 0xb35d,\n  'skein512-496': 0xb35e,\n  'skein512-504': 0xb35f,\n  'skein512-512': 0xb360,\n  'skein1024-8': 0xb361,\n  'skein1024-16': 0xb362,\n  'skein1024-24': 0xb363,\n  'skein1024-32': 0xb364,\n  'skein1024-40': 0xb365,\n  'skein1024-48': 0xb366,\n  'skein1024-56': 0xb367,\n  'skein1024-64': 0xb368,\n  'skein1024-72': 0xb369,\n  'skein1024-80': 0xb36a,\n  'skein1024-88': 0xb36b,\n  'skein1024-96': 0xb36c,\n  'skein1024-104': 0xb36d,\n  'skein1024-112': 0xb36e,\n  'skein1024-120': 0xb36f,\n  'skein1024-128': 0xb370,\n  'skein1024-136': 0xb371,\n  'skein1024-144': 0xb372,\n  'skein1024-152': 0xb373,\n  'skein1024-160': 0xb374,\n  'skein1024-168': 0xb375,\n  'skein1024-176': 0xb376,\n  'skein1024-184': 0xb377,\n  'skein1024-192': 0xb378,\n  'skein1024-200': 0xb379,\n  'skein1024-208': 0xb37a,\n  'skein1024-216': 0xb37b,\n  'skein1024-224': 0xb37c,\n  'skein1024-232': 0xb37d,\n  'skein1024-240': 0xb37e,\n  'skein1024-248': 0xb37f,\n  'skein1024-256': 0xb380,\n  'skein1024-264': 0xb381,\n  'skein1024-272': 0xb382,\n  'skein1024-280': 0xb383,\n  'skein1024-288': 0xb384,\n  'skein1024-296': 0xb385,\n  'skein1024-304': 0xb386,\n  'skein1024-312': 0xb387,\n  'skein1024-320': 0xb388,\n  'skein1024-328': 0xb389,\n  'skein1024-336': 0xb38a,\n  'skein1024-344': 0xb38b,\n  'skein1024-352': 0xb38c,\n  'skein1024-360': 0xb38d,\n  'skein1024-368': 0xb38e,\n  'skein1024-376': 0xb38f,\n  'skein1024-384': 0xb390,\n  'skein1024-392': 0xb391,\n  'skein1024-400': 0xb392,\n  'skein1024-408': 0xb393,\n  'skein1024-416': 0xb394,\n  'skein1024-424': 0xb395,\n  'skein1024-432': 0xb396,\n  'skein1024-440': 0xb397,\n  'skein1024-448': 0xb398,\n  'skein1024-456': 0xb399,\n  'skein1024-464': 0xb39a,\n  'skein1024-472': 0xb39b,\n  'skein1024-480': 0xb39c,\n  'skein1024-488': 0xb39d,\n  'skein1024-496': 0xb39e,\n  'skein1024-504': 0xb39f,\n  'skein1024-512': 0xb3a0,\n  'skein1024-520': 0xb3a1,\n  'skein1024-528': 0xb3a2,\n  'skein1024-536': 0xb3a3,\n  'skein1024-544': 0xb3a4,\n  'skein1024-552': 0xb3a5,\n  'skein1024-560': 0xb3a6,\n  'skein1024-568': 0xb3a7,\n  'skein1024-576': 0xb3a8,\n  'skein1024-584': 0xb3a9,\n  'skein1024-592': 0xb3aa,\n  'skein1024-600': 0xb3ab,\n  'skein1024-608': 0xb3ac,\n  'skein1024-616': 0xb3ad,\n  'skein1024-624': 0xb3ae,\n  'skein1024-632': 0xb3af,\n  'skein1024-640': 0xb3b0,\n  'skein1024-648': 0xb3b1,\n  'skein1024-656': 0xb3b2,\n  'skein1024-664': 0xb3b3,\n  'skein1024-672': 0xb3b4,\n  'skein1024-680': 0xb3b5,\n  'skein1024-688': 0xb3b6,\n  'skein1024-696': 0xb3b7,\n  'skein1024-704': 0xb3b8,\n  'skein1024-712': 0xb3b9,\n  'skein1024-720': 0xb3ba,\n  'skein1024-728': 0xb3bb,\n  'skein1024-736': 0xb3bc,\n  'skein1024-744': 0xb3bd,\n  'skein1024-752': 0xb3be,\n  'skein1024-760': 0xb3bf,\n  'skein1024-768': 0xb3c0,\n  'skein1024-776': 0xb3c1,\n  'skein1024-784': 0xb3c2,\n  'skein1024-792': 0xb3c3,\n  'skein1024-800': 0xb3c4,\n  'skein1024-808': 0xb3c5,\n  'skein1024-816': 0xb3c6,\n  'skein1024-824': 0xb3c7,\n  'skein1024-832': 0xb3c8,\n  'skein1024-840': 0xb3c9,\n  'skein1024-848': 0xb3ca,\n  'skein1024-856': 0xb3cb,\n  'skein1024-864': 0xb3cc,\n  'skein1024-872': 0xb3cd,\n  'skein1024-880': 0xb3ce,\n  'skein1024-888': 0xb3cf,\n  'skein1024-896': 0xb3d0,\n  'skein1024-904': 0xb3d1,\n  'skein1024-912': 0xb3d2,\n  'skein1024-920': 0xb3d3,\n  'skein1024-928': 0xb3d4,\n  'skein1024-936': 0xb3d5,\n  'skein1024-944': 0xb3d6,\n  'skein1024-952': 0xb3d7,\n  'skein1024-960': 0xb3d8,\n  'skein1024-968': 0xb3d9,\n  'skein1024-976': 0xb3da,\n  'skein1024-984': 0xb3db,\n  'skein1024-992': 0xb3dc,\n  'skein1024-1000': 0xb3dd,\n  'skein1024-1008': 0xb3de,\n  'skein1024-1016': 0xb3df,\n  'skein1024-1024': 0xb3e0,\n  'poseidon-bls12_381-a2-fc1': 0xb401,\n  'poseidon-bls12_381-a2-fc1-sc': 0xb402,\n  'zeroxcert-imprint-256': 0xce11,\n  'fil-commitment-unsealed': 0xf101,\n  'fil-commitment-sealed': 0xf102,\n  'holochain-adr-v0': 0x807124,\n  'holochain-adr-v1': 0x817124,\n  'holochain-key-v0': 0x947124,\n  'holochain-key-v1': 0x957124,\n  'holochain-sig-v0': 0xa27124,\n  'holochain-sig-v1': 0xa37124,\n  'skynet-ns': 0xb19910\n})\n\nmodule.exports = { baseTable }\n","'use strict';\n\n/**\n * @typedef {{ [key: string]: any }} Extensions\n * @typedef {Error} Err\n * @property {string} message\n */\n\n/**\n *\n * @param {Error} obj\n * @param {Extensions} props\n * @returns {Error & Extensions}\n */\nfunction assign(obj, props) {\n    for (const key in props) {\n        Object.defineProperty(obj, key, {\n            value: props[key],\n            enumerable: true,\n            configurable: true,\n        });\n    }\n\n    return obj;\n}\n\n/**\n *\n * @param {any} err - An Error\n * @param {string|Extensions} code - A string code or props to set on the error\n * @param {Extensions} [props] - Props to set on the error\n * @returns {Error & Extensions}\n */\nfunction createError(err, code, props) {\n    if (!err || typeof err === 'string') {\n        throw new TypeError('Please pass an Error to err-code');\n    }\n\n    if (!props) {\n        props = {};\n    }\n\n    if (typeof code === 'object') {\n        props = code;\n        code = '';\n    }\n\n    if (code) {\n        props.code = code;\n    }\n\n    try {\n        return assign(err, props);\n    } catch (_) {\n        props.message = err.message;\n        props.stack = err.stack;\n\n        const ErrClass = function () {};\n\n        ErrClass.prototype = Object.create(Object.getPrototypeOf(err));\n\n        // @ts-ignore\n        const output = assign(new ErrClass(), props);\n\n        return output;\n    }\n}\n\nmodule.exports = createError;\n","/* eslint quote-props: off */\n'use strict'\n\n/**\n * Names for all available hashes\n *\n * @typedef { \"identity\" | \"sha1\" | \"sha2-256\" | \"sha2-512\" | \"sha3-512\" | \"sha3-384\" | \"sha3-256\" | \"sha3-224\" | \"shake-128\" | \"shake-256\" | \"keccak-224\" | \"keccak-256\" | \"keccak-384\" | \"keccak-512\" | \"blake3\" | \"murmur3-128\" | \"murmur3-32\" | \"dbl-sha2-256\" | \"md4\" | \"md5\" | \"bmt\" | \"sha2-256-trunc254-padded\" | \"ripemd-128\" | \"ripemd-160\" | \"ripemd-256\" | \"ripemd-320\" | \"x11\" | \"kangarootwelve\" | \"sm3-256\" | \"blake2b-8\" | \"blake2b-16\" | \"blake2b-24\" | \"blake2b-32\" | \"blake2b-40\" | \"blake2b-48\" | \"blake2b-56\" | \"blake2b-64\" | \"blake2b-72\" | \"blake2b-80\" | \"blake2b-88\" | \"blake2b-96\" | \"blake2b-104\" | \"blake2b-112\" | \"blake2b-120\" | \"blake2b-128\" | \"blake2b-136\" | \"blake2b-144\" | \"blake2b-152\" | \"blake2b-160\" | \"blake2b-168\" | \"blake2b-176\" | \"blake2b-184\" | \"blake2b-192\" | \"blake2b-200\" | \"blake2b-208\" | \"blake2b-216\" | \"blake2b-224\" | \"blake2b-232\" | \"blake2b-240\" | \"blake2b-248\" | \"blake2b-256\" | \"blake2b-264\" | \"blake2b-272\" | \"blake2b-280\" | \"blake2b-288\" | \"blake2b-296\" | \"blake2b-304\" | \"blake2b-312\" | \"blake2b-320\" | \"blake2b-328\" | \"blake2b-336\" | \"blake2b-344\" | \"blake2b-352\" | \"blake2b-360\" | \"blake2b-368\" | \"blake2b-376\" | \"blake2b-384\" | \"blake2b-392\" | \"blake2b-400\" | \"blake2b-408\" | \"blake2b-416\" | \"blake2b-424\" | \"blake2b-432\" | \"blake2b-440\" | \"blake2b-448\" | \"blake2b-456\" | \"blake2b-464\" | \"blake2b-472\" | \"blake2b-480\" | \"blake2b-488\" | \"blake2b-496\" | \"blake2b-504\" | \"blake2b-512\" | \"blake2s-8\" | \"blake2s-16\" | \"blake2s-24\" | \"blake2s-32\" | \"blake2s-40\" | \"blake2s-48\" | \"blake2s-56\" | \"blake2s-64\" | \"blake2s-72\" | \"blake2s-80\" | \"blake2s-88\" | \"blake2s-96\" | \"blake2s-104\" | \"blake2s-112\" | \"blake2s-120\" | \"blake2s-128\" | \"blake2s-136\" | \"blake2s-144\" | \"blake2s-152\" | \"blake2s-160\" | \"blake2s-168\" | \"blake2s-176\" | \"blake2s-184\" | \"blake2s-192\" | \"blake2s-200\" | \"blake2s-208\" | \"blake2s-216\" | \"blake2s-224\" | \"blake2s-232\" | \"blake2s-240\" | \"blake2s-248\" | \"blake2s-256\" | \"skein256-8\" | \"skein256-16\" | \"skein256-24\" | \"skein256-32\" | \"skein256-40\" | \"skein256-48\" | \"skein256-56\" | \"skein256-64\" | \"skein256-72\" | \"skein256-80\" | \"skein256-88\" | \"skein256-96\" | \"skein256-104\" | \"skein256-112\" | \"skein256-120\" | \"skein256-128\" | \"skein256-136\" | \"skein256-144\" | \"skein256-152\" | \"skein256-160\" | \"skein256-168\" | \"skein256-176\" | \"skein256-184\" | \"skein256-192\" | \"skein256-200\" | \"skein256-208\" | \"skein256-216\" | \"skein256-224\" | \"skein256-232\" | \"skein256-240\" | \"skein256-248\" | \"skein256-256\" | \"skein512-8\" | \"skein512-16\" | \"skein512-24\" | \"skein512-32\" | \"skein512-40\" | \"skein512-48\" | \"skein512-56\" | \"skein512-64\" | \"skein512-72\" | \"skein512-80\" | \"skein512-88\" | \"skein512-96\" | \"skein512-104\" | \"skein512-112\" | \"skein512-120\" | \"skein512-128\" | \"skein512-136\" | \"skein512-144\" | \"skein512-152\" | \"skein512-160\" | \"skein512-168\" | \"skein512-176\" | \"skein512-184\" | \"skein512-192\" | \"skein512-200\" | \"skein512-208\" | \"skein512-216\" | \"skein512-224\" | \"skein512-232\" | \"skein512-240\" | \"skein512-248\" | \"skein512-256\" | \"skein512-264\" | \"skein512-272\" | \"skein512-280\" | \"skein512-288\" | \"skein512-296\" | \"skein512-304\" | \"skein512-312\" | \"skein512-320\" | \"skein512-328\" | \"skein512-336\" | \"skein512-344\" | \"skein512-352\" | \"skein512-360\" | \"skein512-368\" | \"skein512-376\" | \"skein512-384\" | \"skein512-392\" | \"skein512-400\" | \"skein512-408\" | \"skein512-416\" | \"skein512-424\" | \"skein512-432\" | \"skein512-440\" | \"skein512-448\" | \"skein512-456\" | \"skein512-464\" | \"skein512-472\" | \"skein512-480\" | \"skein512-488\" | \"skein512-496\" | \"skein512-504\" | \"skein512-512\" | \"skein1024-8\" | \"skein1024-16\" | \"skein1024-24\" | \"skein1024-32\" | \"skein1024-40\" | \"skein1024-48\" | \"skein1024-56\" | \"skein1024-64\" | \"skein1024-72\" | \"skein1024-80\" | \"skein1024-88\" | \"skein1024-96\" | \"skein1024-104\" | \"skein1024-112\" | \"skein1024-120\" | \"skein1024-128\" | \"skein1024-136\" | \"skein1024-144\" | \"skein1024-152\" | \"skein1024-160\" | \"skein1024-168\" | \"skein1024-176\" | \"skein1024-184\" | \"skein1024-192\" | \"skein1024-200\" | \"skein1024-208\" | \"skein1024-216\" | \"skein1024-224\" | \"skein1024-232\" | \"skein1024-240\" | \"skein1024-248\" | \"skein1024-256\" | \"skein1024-264\" | \"skein1024-272\" | \"skein1024-280\" | \"skein1024-288\" | \"skein1024-296\" | \"skein1024-304\" | \"skein1024-312\" | \"skein1024-320\" | \"skein1024-328\" | \"skein1024-336\" | \"skein1024-344\" | \"skein1024-352\" | \"skein1024-360\" | \"skein1024-368\" | \"skein1024-376\" | \"skein1024-384\" | \"skein1024-392\" | \"skein1024-400\" | \"skein1024-408\" | \"skein1024-416\" | \"skein1024-424\" | \"skein1024-432\" | \"skein1024-440\" | \"skein1024-448\" | \"skein1024-456\" | \"skein1024-464\" | \"skein1024-472\" | \"skein1024-480\" | \"skein1024-488\" | \"skein1024-496\" | \"skein1024-504\" | \"skein1024-512\" | \"skein1024-520\" | \"skein1024-528\" | \"skein1024-536\" | \"skein1024-544\" | \"skein1024-552\" | \"skein1024-560\" | \"skein1024-568\" | \"skein1024-576\" | \"skein1024-584\" | \"skein1024-592\" | \"skein1024-600\" | \"skein1024-608\" | \"skein1024-616\" | \"skein1024-624\" | \"skein1024-632\" | \"skein1024-640\" | \"skein1024-648\" | \"skein1024-656\" | \"skein1024-664\" | \"skein1024-672\" | \"skein1024-680\" | \"skein1024-688\" | \"skein1024-696\" | \"skein1024-704\" | \"skein1024-712\" | \"skein1024-720\" | \"skein1024-728\" | \"skein1024-736\" | \"skein1024-744\" | \"skein1024-752\" | \"skein1024-760\" | \"skein1024-768\" | \"skein1024-776\" | \"skein1024-784\" | \"skein1024-792\" | \"skein1024-800\" | \"skein1024-808\" | \"skein1024-816\" | \"skein1024-824\" | \"skein1024-832\" | \"skein1024-840\" | \"skein1024-848\" | \"skein1024-856\" | \"skein1024-864\" | \"skein1024-872\" | \"skein1024-880\" | \"skein1024-888\" | \"skein1024-896\" | \"skein1024-904\" | \"skein1024-912\" | \"skein1024-920\" | \"skein1024-928\" | \"skein1024-936\" | \"skein1024-944\" | \"skein1024-952\" | \"skein1024-960\" | \"skein1024-968\" | \"skein1024-976\" | \"skein1024-984\" | \"skein1024-992\" | \"skein1024-1000\" | \"skein1024-1008\" | \"skein1024-1016\" | \"skein1024-1024\" | \"poseidon-bls12_381-a2-fc1\" | \"poseidon-bls12_381-a2-fc1-sc\" } HashName\n */\n/**\n * Codes for all available hashes\n *\n * @typedef { 0x00 | 0x11 | 0x12 | 0x13 | 0x14 | 0x15 | 0x16 | 0x17 | 0x18 | 0x19 | 0x1a | 0x1b | 0x1c | 0x1d | 0x1e | 0x22 | 0x23 | 0x56 | 0xd4 | 0xd5 | 0xd6 | 0x1012 | 0x1052 | 0x1053 | 0x1054 | 0x1055 | 0x1100 | 0x1d01 | 0x534d | 0xb201 | 0xb202 | 0xb203 | 0xb204 | 0xb205 | 0xb206 | 0xb207 | 0xb208 | 0xb209 | 0xb20a | 0xb20b | 0xb20c | 0xb20d | 0xb20e | 0xb20f | 0xb210 | 0xb211 | 0xb212 | 0xb213 | 0xb214 | 0xb215 | 0xb216 | 0xb217 | 0xb218 | 0xb219 | 0xb21a | 0xb21b | 0xb21c | 0xb21d | 0xb21e | 0xb21f | 0xb220 | 0xb221 | 0xb222 | 0xb223 | 0xb224 | 0xb225 | 0xb226 | 0xb227 | 0xb228 | 0xb229 | 0xb22a | 0xb22b | 0xb22c | 0xb22d | 0xb22e | 0xb22f | 0xb230 | 0xb231 | 0xb232 | 0xb233 | 0xb234 | 0xb235 | 0xb236 | 0xb237 | 0xb238 | 0xb239 | 0xb23a | 0xb23b | 0xb23c | 0xb23d | 0xb23e | 0xb23f | 0xb240 | 0xb241 | 0xb242 | 0xb243 | 0xb244 | 0xb245 | 0xb246 | 0xb247 | 0xb248 | 0xb249 | 0xb24a | 0xb24b | 0xb24c | 0xb24d | 0xb24e | 0xb24f | 0xb250 | 0xb251 | 0xb252 | 0xb253 | 0xb254 | 0xb255 | 0xb256 | 0xb257 | 0xb258 | 0xb259 | 0xb25a | 0xb25b | 0xb25c | 0xb25d | 0xb25e | 0xb25f | 0xb260 | 0xb301 | 0xb302 | 0xb303 | 0xb304 | 0xb305 | 0xb306 | 0xb307 | 0xb308 | 0xb309 | 0xb30a | 0xb30b | 0xb30c | 0xb30d | 0xb30e | 0xb30f | 0xb310 | 0xb311 | 0xb312 | 0xb313 | 0xb314 | 0xb315 | 0xb316 | 0xb317 | 0xb318 | 0xb319 | 0xb31a | 0xb31b | 0xb31c | 0xb31d | 0xb31e | 0xb31f | 0xb320 | 0xb321 | 0xb322 | 0xb323 | 0xb324 | 0xb325 | 0xb326 | 0xb327 | 0xb328 | 0xb329 | 0xb32a | 0xb32b | 0xb32c | 0xb32d | 0xb32e | 0xb32f | 0xb330 | 0xb331 | 0xb332 | 0xb333 | 0xb334 | 0xb335 | 0xb336 | 0xb337 | 0xb338 | 0xb339 | 0xb33a | 0xb33b | 0xb33c | 0xb33d | 0xb33e | 0xb33f | 0xb340 | 0xb341 | 0xb342 | 0xb343 | 0xb344 | 0xb345 | 0xb346 | 0xb347 | 0xb348 | 0xb349 | 0xb34a | 0xb34b | 0xb34c | 0xb34d | 0xb34e | 0xb34f | 0xb350 | 0xb351 | 0xb352 | 0xb353 | 0xb354 | 0xb355 | 0xb356 | 0xb357 | 0xb358 | 0xb359 | 0xb35a | 0xb35b | 0xb35c | 0xb35d | 0xb35e | 0xb35f | 0xb360 | 0xb361 | 0xb362 | 0xb363 | 0xb364 | 0xb365 | 0xb366 | 0xb367 | 0xb368 | 0xb369 | 0xb36a | 0xb36b | 0xb36c | 0xb36d | 0xb36e | 0xb36f | 0xb370 | 0xb371 | 0xb372 | 0xb373 | 0xb374 | 0xb375 | 0xb376 | 0xb377 | 0xb378 | 0xb379 | 0xb37a | 0xb37b | 0xb37c | 0xb37d | 0xb37e | 0xb37f | 0xb380 | 0xb381 | 0xb382 | 0xb383 | 0xb384 | 0xb385 | 0xb386 | 0xb387 | 0xb388 | 0xb389 | 0xb38a | 0xb38b | 0xb38c | 0xb38d | 0xb38e | 0xb38f | 0xb390 | 0xb391 | 0xb392 | 0xb393 | 0xb394 | 0xb395 | 0xb396 | 0xb397 | 0xb398 | 0xb399 | 0xb39a | 0xb39b | 0xb39c | 0xb39d | 0xb39e | 0xb39f | 0xb3a0 | 0xb3a1 | 0xb3a2 | 0xb3a3 | 0xb3a4 | 0xb3a5 | 0xb3a6 | 0xb3a7 | 0xb3a8 | 0xb3a9 | 0xb3aa | 0xb3ab | 0xb3ac | 0xb3ad | 0xb3ae | 0xb3af | 0xb3b0 | 0xb3b1 | 0xb3b2 | 0xb3b3 | 0xb3b4 | 0xb3b5 | 0xb3b6 | 0xb3b7 | 0xb3b8 | 0xb3b9 | 0xb3ba | 0xb3bb | 0xb3bc | 0xb3bd | 0xb3be | 0xb3bf | 0xb3c0 | 0xb3c1 | 0xb3c2 | 0xb3c3 | 0xb3c4 | 0xb3c5 | 0xb3c6 | 0xb3c7 | 0xb3c8 | 0xb3c9 | 0xb3ca | 0xb3cb | 0xb3cc | 0xb3cd | 0xb3ce | 0xb3cf | 0xb3d0 | 0xb3d1 | 0xb3d2 | 0xb3d3 | 0xb3d4 | 0xb3d5 | 0xb3d6 | 0xb3d7 | 0xb3d8 | 0xb3d9 | 0xb3da | 0xb3db | 0xb3dc | 0xb3dd | 0xb3de | 0xb3df | 0xb3e0 | 0xb401 | 0xb402 } HashCode\n */\n\n/**\n * @type { Record<HashName,HashCode> }\n */\nconst names = Object.freeze({\n  'identity': 0x00,\n  'sha1': 0x11,\n  'sha2-256': 0x12,\n  'sha2-512': 0x13,\n  'sha3-512': 0x14,\n  'sha3-384': 0x15,\n  'sha3-256': 0x16,\n  'sha3-224': 0x17,\n  'shake-128': 0x18,\n  'shake-256': 0x19,\n  'keccak-224': 0x1a,\n  'keccak-256': 0x1b,\n  'keccak-384': 0x1c,\n  'keccak-512': 0x1d,\n  'blake3': 0x1e,\n  'murmur3-128': 0x22,\n  'murmur3-32': 0x23,\n  'dbl-sha2-256': 0x56,\n  'md4': 0xd4,\n  'md5': 0xd5,\n  'bmt': 0xd6,\n  'sha2-256-trunc254-padded': 0x1012,\n  'ripemd-128': 0x1052,\n  'ripemd-160': 0x1053,\n  'ripemd-256': 0x1054,\n  'ripemd-320': 0x1055,\n  'x11': 0x1100,\n  'kangarootwelve': 0x1d01,\n  'sm3-256': 0x534d,\n  'blake2b-8': 0xb201,\n  'blake2b-16': 0xb202,\n  'blake2b-24': 0xb203,\n  'blake2b-32': 0xb204,\n  'blake2b-40': 0xb205,\n  'blake2b-48': 0xb206,\n  'blake2b-56': 0xb207,\n  'blake2b-64': 0xb208,\n  'blake2b-72': 0xb209,\n  'blake2b-80': 0xb20a,\n  'blake2b-88': 0xb20b,\n  'blake2b-96': 0xb20c,\n  'blake2b-104': 0xb20d,\n  'blake2b-112': 0xb20e,\n  'blake2b-120': 0xb20f,\n  'blake2b-128': 0xb210,\n  'blake2b-136': 0xb211,\n  'blake2b-144': 0xb212,\n  'blake2b-152': 0xb213,\n  'blake2b-160': 0xb214,\n  'blake2b-168': 0xb215,\n  'blake2b-176': 0xb216,\n  'blake2b-184': 0xb217,\n  'blake2b-192': 0xb218,\n  'blake2b-200': 0xb219,\n  'blake2b-208': 0xb21a,\n  'blake2b-216': 0xb21b,\n  'blake2b-224': 0xb21c,\n  'blake2b-232': 0xb21d,\n  'blake2b-240': 0xb21e,\n  'blake2b-248': 0xb21f,\n  'blake2b-256': 0xb220,\n  'blake2b-264': 0xb221,\n  'blake2b-272': 0xb222,\n  'blake2b-280': 0xb223,\n  'blake2b-288': 0xb224,\n  'blake2b-296': 0xb225,\n  'blake2b-304': 0xb226,\n  'blake2b-312': 0xb227,\n  'blake2b-320': 0xb228,\n  'blake2b-328': 0xb229,\n  'blake2b-336': 0xb22a,\n  'blake2b-344': 0xb22b,\n  'blake2b-352': 0xb22c,\n  'blake2b-360': 0xb22d,\n  'blake2b-368': 0xb22e,\n  'blake2b-376': 0xb22f,\n  'blake2b-384': 0xb230,\n  'blake2b-392': 0xb231,\n  'blake2b-400': 0xb232,\n  'blake2b-408': 0xb233,\n  'blake2b-416': 0xb234,\n  'blake2b-424': 0xb235,\n  'blake2b-432': 0xb236,\n  'blake2b-440': 0xb237,\n  'blake2b-448': 0xb238,\n  'blake2b-456': 0xb239,\n  'blake2b-464': 0xb23a,\n  'blake2b-472': 0xb23b,\n  'blake2b-480': 0xb23c,\n  'blake2b-488': 0xb23d,\n  'blake2b-496': 0xb23e,\n  'blake2b-504': 0xb23f,\n  'blake2b-512': 0xb240,\n  'blake2s-8': 0xb241,\n  'blake2s-16': 0xb242,\n  'blake2s-24': 0xb243,\n  'blake2s-32': 0xb244,\n  'blake2s-40': 0xb245,\n  'blake2s-48': 0xb246,\n  'blake2s-56': 0xb247,\n  'blake2s-64': 0xb248,\n  'blake2s-72': 0xb249,\n  'blake2s-80': 0xb24a,\n  'blake2s-88': 0xb24b,\n  'blake2s-96': 0xb24c,\n  'blake2s-104': 0xb24d,\n  'blake2s-112': 0xb24e,\n  'blake2s-120': 0xb24f,\n  'blake2s-128': 0xb250,\n  'blake2s-136': 0xb251,\n  'blake2s-144': 0xb252,\n  'blake2s-152': 0xb253,\n  'blake2s-160': 0xb254,\n  'blake2s-168': 0xb255,\n  'blake2s-176': 0xb256,\n  'blake2s-184': 0xb257,\n  'blake2s-192': 0xb258,\n  'blake2s-200': 0xb259,\n  'blake2s-208': 0xb25a,\n  'blake2s-216': 0xb25b,\n  'blake2s-224': 0xb25c,\n  'blake2s-232': 0xb25d,\n  'blake2s-240': 0xb25e,\n  'blake2s-248': 0xb25f,\n  'blake2s-256': 0xb260,\n  'skein256-8': 0xb301,\n  'skein256-16': 0xb302,\n  'skein256-24': 0xb303,\n  'skein256-32': 0xb304,\n  'skein256-40': 0xb305,\n  'skein256-48': 0xb306,\n  'skein256-56': 0xb307,\n  'skein256-64': 0xb308,\n  'skein256-72': 0xb309,\n  'skein256-80': 0xb30a,\n  'skein256-88': 0xb30b,\n  'skein256-96': 0xb30c,\n  'skein256-104': 0xb30d,\n  'skein256-112': 0xb30e,\n  'skein256-120': 0xb30f,\n  'skein256-128': 0xb310,\n  'skein256-136': 0xb311,\n  'skein256-144': 0xb312,\n  'skein256-152': 0xb313,\n  'skein256-160': 0xb314,\n  'skein256-168': 0xb315,\n  'skein256-176': 0xb316,\n  'skein256-184': 0xb317,\n  'skein256-192': 0xb318,\n  'skein256-200': 0xb319,\n  'skein256-208': 0xb31a,\n  'skein256-216': 0xb31b,\n  'skein256-224': 0xb31c,\n  'skein256-232': 0xb31d,\n  'skein256-240': 0xb31e,\n  'skein256-248': 0xb31f,\n  'skein256-256': 0xb320,\n  'skein512-8': 0xb321,\n  'skein512-16': 0xb322,\n  'skein512-24': 0xb323,\n  'skein512-32': 0xb324,\n  'skein512-40': 0xb325,\n  'skein512-48': 0xb326,\n  'skein512-56': 0xb327,\n  'skein512-64': 0xb328,\n  'skein512-72': 0xb329,\n  'skein512-80': 0xb32a,\n  'skein512-88': 0xb32b,\n  'skein512-96': 0xb32c,\n  'skein512-104': 0xb32d,\n  'skein512-112': 0xb32e,\n  'skein512-120': 0xb32f,\n  'skein512-128': 0xb330,\n  'skein512-136': 0xb331,\n  'skein512-144': 0xb332,\n  'skein512-152': 0xb333,\n  'skein512-160': 0xb334,\n  'skein512-168': 0xb335,\n  'skein512-176': 0xb336,\n  'skein512-184': 0xb337,\n  'skein512-192': 0xb338,\n  'skein512-200': 0xb339,\n  'skein512-208': 0xb33a,\n  'skein512-216': 0xb33b,\n  'skein512-224': 0xb33c,\n  'skein512-232': 0xb33d,\n  'skein512-240': 0xb33e,\n  'skein512-248': 0xb33f,\n  'skein512-256': 0xb340,\n  'skein512-264': 0xb341,\n  'skein512-272': 0xb342,\n  'skein512-280': 0xb343,\n  'skein512-288': 0xb344,\n  'skein512-296': 0xb345,\n  'skein512-304': 0xb346,\n  'skein512-312': 0xb347,\n  'skein512-320': 0xb348,\n  'skein512-328': 0xb349,\n  'skein512-336': 0xb34a,\n  'skein512-344': 0xb34b,\n  'skein512-352': 0xb34c,\n  'skein512-360': 0xb34d,\n  'skein512-368': 0xb34e,\n  'skein512-376': 0xb34f,\n  'skein512-384': 0xb350,\n  'skein512-392': 0xb351,\n  'skein512-400': 0xb352,\n  'skein512-408': 0xb353,\n  'skein512-416': 0xb354,\n  'skein512-424': 0xb355,\n  'skein512-432': 0xb356,\n  'skein512-440': 0xb357,\n  'skein512-448': 0xb358,\n  'skein512-456': 0xb359,\n  'skein512-464': 0xb35a,\n  'skein512-472': 0xb35b,\n  'skein512-480': 0xb35c,\n  'skein512-488': 0xb35d,\n  'skein512-496': 0xb35e,\n  'skein512-504': 0xb35f,\n  'skein512-512': 0xb360,\n  'skein1024-8': 0xb361,\n  'skein1024-16': 0xb362,\n  'skein1024-24': 0xb363,\n  'skein1024-32': 0xb364,\n  'skein1024-40': 0xb365,\n  'skein1024-48': 0xb366,\n  'skein1024-56': 0xb367,\n  'skein1024-64': 0xb368,\n  'skein1024-72': 0xb369,\n  'skein1024-80': 0xb36a,\n  'skein1024-88': 0xb36b,\n  'skein1024-96': 0xb36c,\n  'skein1024-104': 0xb36d,\n  'skein1024-112': 0xb36e,\n  'skein1024-120': 0xb36f,\n  'skein1024-128': 0xb370,\n  'skein1024-136': 0xb371,\n  'skein1024-144': 0xb372,\n  'skein1024-152': 0xb373,\n  'skein1024-160': 0xb374,\n  'skein1024-168': 0xb375,\n  'skein1024-176': 0xb376,\n  'skein1024-184': 0xb377,\n  'skein1024-192': 0xb378,\n  'skein1024-200': 0xb379,\n  'skein1024-208': 0xb37a,\n  'skein1024-216': 0xb37b,\n  'skein1024-224': 0xb37c,\n  'skein1024-232': 0xb37d,\n  'skein1024-240': 0xb37e,\n  'skein1024-248': 0xb37f,\n  'skein1024-256': 0xb380,\n  'skein1024-264': 0xb381,\n  'skein1024-272': 0xb382,\n  'skein1024-280': 0xb383,\n  'skein1024-288': 0xb384,\n  'skein1024-296': 0xb385,\n  'skein1024-304': 0xb386,\n  'skein1024-312': 0xb387,\n  'skein1024-320': 0xb388,\n  'skein1024-328': 0xb389,\n  'skein1024-336': 0xb38a,\n  'skein1024-344': 0xb38b,\n  'skein1024-352': 0xb38c,\n  'skein1024-360': 0xb38d,\n  'skein1024-368': 0xb38e,\n  'skein1024-376': 0xb38f,\n  'skein1024-384': 0xb390,\n  'skein1024-392': 0xb391,\n  'skein1024-400': 0xb392,\n  'skein1024-408': 0xb393,\n  'skein1024-416': 0xb394,\n  'skein1024-424': 0xb395,\n  'skein1024-432': 0xb396,\n  'skein1024-440': 0xb397,\n  'skein1024-448': 0xb398,\n  'skein1024-456': 0xb399,\n  'skein1024-464': 0xb39a,\n  'skein1024-472': 0xb39b,\n  'skein1024-480': 0xb39c,\n  'skein1024-488': 0xb39d,\n  'skein1024-496': 0xb39e,\n  'skein1024-504': 0xb39f,\n  'skein1024-512': 0xb3a0,\n  'skein1024-520': 0xb3a1,\n  'skein1024-528': 0xb3a2,\n  'skein1024-536': 0xb3a3,\n  'skein1024-544': 0xb3a4,\n  'skein1024-552': 0xb3a5,\n  'skein1024-560': 0xb3a6,\n  'skein1024-568': 0xb3a7,\n  'skein1024-576': 0xb3a8,\n  'skein1024-584': 0xb3a9,\n  'skein1024-592': 0xb3aa,\n  'skein1024-600': 0xb3ab,\n  'skein1024-608': 0xb3ac,\n  'skein1024-616': 0xb3ad,\n  'skein1024-624': 0xb3ae,\n  'skein1024-632': 0xb3af,\n  'skein1024-640': 0xb3b0,\n  'skein1024-648': 0xb3b1,\n  'skein1024-656': 0xb3b2,\n  'skein1024-664': 0xb3b3,\n  'skein1024-672': 0xb3b4,\n  'skein1024-680': 0xb3b5,\n  'skein1024-688': 0xb3b6,\n  'skein1024-696': 0xb3b7,\n  'skein1024-704': 0xb3b8,\n  'skein1024-712': 0xb3b9,\n  'skein1024-720': 0xb3ba,\n  'skein1024-728': 0xb3bb,\n  'skein1024-736': 0xb3bc,\n  'skein1024-744': 0xb3bd,\n  'skein1024-752': 0xb3be,\n  'skein1024-760': 0xb3bf,\n  'skein1024-768': 0xb3c0,\n  'skein1024-776': 0xb3c1,\n  'skein1024-784': 0xb3c2,\n  'skein1024-792': 0xb3c3,\n  'skein1024-800': 0xb3c4,\n  'skein1024-808': 0xb3c5,\n  'skein1024-816': 0xb3c6,\n  'skein1024-824': 0xb3c7,\n  'skein1024-832': 0xb3c8,\n  'skein1024-840': 0xb3c9,\n  'skein1024-848': 0xb3ca,\n  'skein1024-856': 0xb3cb,\n  'skein1024-864': 0xb3cc,\n  'skein1024-872': 0xb3cd,\n  'skein1024-880': 0xb3ce,\n  'skein1024-888': 0xb3cf,\n  'skein1024-896': 0xb3d0,\n  'skein1024-904': 0xb3d1,\n  'skein1024-912': 0xb3d2,\n  'skein1024-920': 0xb3d3,\n  'skein1024-928': 0xb3d4,\n  'skein1024-936': 0xb3d5,\n  'skein1024-944': 0xb3d6,\n  'skein1024-952': 0xb3d7,\n  'skein1024-960': 0xb3d8,\n  'skein1024-968': 0xb3d9,\n  'skein1024-976': 0xb3da,\n  'skein1024-984': 0xb3db,\n  'skein1024-992': 0xb3dc,\n  'skein1024-1000': 0xb3dd,\n  'skein1024-1008': 0xb3de,\n  'skein1024-1016': 0xb3df,\n  'skein1024-1024': 0xb3e0,\n  'poseidon-bls12_381-a2-fc1': 0xb401,\n  'poseidon-bls12_381-a2-fc1-sc': 0xb402\n})\n\nmodule.exports = { names }\n","'use strict'\n\nconst sha3 = require('js-sha3')\n// @ts-ignore - no types available\nconst mur = require('murmurhash3js-revisited')\nconst { factory: sha } = require('./sha')\nconst { fromNumberTo32BitBuf } = require('./utils')\nconst uint8ArrayFromString = require('uint8arrays/from-string')\n\n// Note that although this function doesn't do any asynchronous work, we mark\n// the function as async because it must return a Promise to match the API\n// for other functions that do perform asynchronous work (see sha.browser.js)\n// eslint-disable-next-line\n/**\n * @param {string} algorithm\n * @returns {import('./types').Digest}\n */\nconst hash = (algorithm) => async (data) => {\n  switch (algorithm) {\n    case 'sha3-224':\n      return new Uint8Array(sha3.sha3_224.arrayBuffer(data))\n    case 'sha3-256':\n      return new Uint8Array(sha3.sha3_256.arrayBuffer(data))\n    case 'sha3-384':\n      return new Uint8Array(sha3.sha3_384.arrayBuffer(data))\n    case 'sha3-512':\n      return new Uint8Array(sha3.sha3_512.arrayBuffer(data))\n    case 'shake-128':\n      return new Uint8Array(sha3.shake128.create(128).update(data).arrayBuffer())\n    case 'shake-256':\n      return new Uint8Array(sha3.shake256.create(256).update(data).arrayBuffer())\n    case 'keccak-224':\n      return new Uint8Array(sha3.keccak224.arrayBuffer(data))\n    case 'keccak-256':\n      return new Uint8Array(sha3.keccak256.arrayBuffer(data))\n    case 'keccak-384':\n      return new Uint8Array(sha3.keccak384.arrayBuffer(data))\n    case 'keccak-512':\n      return new Uint8Array(sha3.keccak512.arrayBuffer(data))\n    case 'murmur3-128':\n      return uint8ArrayFromString(mur.x64.hash128(data), 'base16')\n    case 'murmur3-32':\n      return fromNumberTo32BitBuf(mur.x86.hash32(data))\n\n    default:\n      throw new TypeError(`${algorithm} is not a supported algorithm`)\n  }\n}\n\n/** @type {import('./types').Digest} */\nconst identity = data => data\n\nmodule.exports = {\n  identity,\n  sha1: sha('sha1'),\n  sha2256: sha('sha2-256'),\n  sha2512: sha('sha2-512'),\n  dblSha2256: sha('dbl-sha2-256'),\n  sha3224: hash('sha3-224'),\n  sha3256: hash('sha3-256'),\n  sha3384: hash('sha3-384'),\n  sha3512: hash('sha3-512'),\n  shake128: hash('shake-128'),\n  shake256: hash('shake-256'),\n  keccak224: hash('keccak-224'),\n  keccak256: hash('keccak-256'),\n  keccak384: hash('keccak-384'),\n  keccak512: hash('keccak-512'),\n  murmur3128: hash('murmur3-128'),\n  murmur332: hash('murmur3-32'),\n  addBlake: require('./blake')\n}\n","/* eslint-disable require-await */\n'use strict'\n\nconst multihash = require('multihashes')\n/**\n * @typedef {import('multihashes').HashName} HashName\n * @typedef {import('./types').Digest} Digest\n */\n\n/**\n * @type {Crypto}\n */\nconst crypto =\n  self.crypto ||\n  /** @type {typeof window.crypto} */\n  // @ts-ignore - unknown property\n  (self.msCrypto)\n\n/**\n *\n * @param {Uint8Array} data\n * @param {HashName} alg\n * @returns {Promise<Uint8Array>}\n */\nconst digest = async (data, alg) => {\n  if (typeof self === 'undefined' || !crypto) {\n    throw new Error(\n      'Please use a browser with webcrypto support and ensure the code has been delivered securely via HTTPS/TLS and run within a Secure Context'\n    )\n  }\n  switch (alg) {\n    case 'sha1':\n      return new Uint8Array(await crypto.subtle.digest({ name: 'SHA-1' }, data))\n    case 'sha2-256':\n      return new Uint8Array(await crypto.subtle.digest({ name: 'SHA-256' }, data))\n    case 'sha2-512':\n      return new Uint8Array(await crypto.subtle.digest({ name: 'SHA-512' }, data))\n    case 'dbl-sha2-256': {\n      const d = await crypto.subtle.digest({ name: 'SHA-256' }, data)\n      return new Uint8Array(await crypto.subtle.digest({ name: 'SHA-256' }, d))\n    }\n    default:\n      throw new Error(`${alg} is not a supported algorithm`)\n  }\n}\n\nmodule.exports = {\n  /**\n   * @param {HashName} alg\n   * @returns {Digest}\n   */\n  factory: (alg) => async (data) => {\n    return digest(data, alg)\n  },\n  digest,\n  /**\n   * @param {Uint8Array} buf\n   * @param {HashName} alg\n   * @param {number} [length]\n   */\n  multihashing: async (buf, alg, length) => {\n    const h = await digest(buf, alg)\n    return multihash.encode(h, alg, length)\n  }\n}\n","'use strict'\n\n/**\n * @param {number} number\n * @returns {Uint8Array}\n */\nconst fromNumberTo32BitBuf = (number) => {\n  const bytes = new Uint8Array(4)\n\n  for (let i = 0; i < 4; i++) {\n    bytes[i] = number & 0xff\n    number = number >> 8\n  }\n\n  return bytes\n}\n\nmodule.exports = {\n  fromNumberTo32BitBuf\n}\n","'use strict'\n\n// @ts-ignore - no types available\nconst blake = require('blakejs')\n\nconst minB = 0xb201\nconst minS = 0xb241\n\nconst blake2b = {\n  init: blake.blake2bInit,\n  update: blake.blake2bUpdate,\n  digest: blake.blake2bFinal\n}\n\nconst blake2s = {\n  init: blake.blake2sInit,\n  update: blake.blake2sUpdate,\n  digest: blake.blake2sFinal\n}\n\n// Note that although this function doesn't do any asynchronous work, we mark\n// the function as async because it must return a Promise to match the API\n// for other functions that do perform asynchronous work (see sha.browser.js)\n// eslint-disable-next-line\n\n/**\n * @param {number} size\n * @param {any} hf\n * @returns {import('./types').Digest}\n */\nconst makeB2Hash = (size, hf) => async (data) => {\n  const ctx = hf.init(size, null)\n  hf.update(ctx, data)\n  return hf.digest(ctx)\n}\n\n/**\n * @param {Record<number, import('./types').Digest>} table\n */\nmodule.exports = (table) => {\n  for (let i = 0; i < 64; i++) {\n    table[minB + i] = makeB2Hash(i + 1, blake2b)\n  }\n  for (let i = 0; i < 32; i++) {\n    table[minS + i] = makeB2Hash(i + 1, blake2s)\n  }\n}\n","'use strict'\n\nconst CID = require('cids')\nconst util = require('./util')\n\n/**\n * Resolves a path within a CBOR block.\n *\n * Returns the value or a link and the partial mising path. This way the\n * IPLD Resolver can fetch the link and continue to resolve.\n *\n * @param {Uint8Array} binaryBlob - Binary representation of a CBOR block\n * @param {string} [path='/'] - Path that should be resolved\n */\nexports.resolve = (binaryBlob, path = '') => {\n  let node = util.deserialize(binaryBlob)\n\n  const parts = path.split('/').filter(Boolean)\n  while (parts.length) {\n    const key = parts.shift()\n    if (!key || !(key in node)) {\n      throw new Error(`Object has no property '${key}'`)\n    }\n\n    node = node[key]\n    if (CID.isCID(node)) {\n      return {\n        value: node,\n        remainderPath: parts.join('/')\n      }\n    }\n  }\n\n  return {\n    value: node,\n    remainderPath: ''\n  }\n}\n\n/**\n * @param {any} node\n * @param {string} [path]\n * @returns {Generator<string, void, undefined>}\n */\nconst traverse = function * (node, path) {\n  // Traverse only objects and arrays\n  if (node instanceof Uint8Array || CID.isCID(node) || typeof node === 'string' || node === null) {\n    return\n  }\n\n  for (const item of Object.keys(node)) {\n    const nextpath = path === undefined ? item : path + '/' + item\n    yield nextpath\n    yield * traverse(node[item], nextpath)\n  }\n\n  // to stop eslint and tsc fighting\n  return undefined\n}\n\n/**\n * Return all available paths of a block.\n *\n * @generator\n * @param {Uint8Array} binaryBlob - Binary representation of a CBOR block\n * @yields {string} - A single path\n */\nexports.tree = function * (binaryBlob) {\n  const node = util.deserialize(binaryBlob)\n\n  yield * traverse(node)\n}\n","'use strict'\n\nconst resolver = require('./resolver')\nconst util = require('./util')\nconst DAGNodeClass = require('./dag-node/dagNode')\nconst DAGLinkClass = require('./dag-link/dagLink')\n\n/**\n * @typedef {import('./types').DAGLinkLike} DAGLinkLike\n * @typedef {import('./types').DAGNodeLike} DAGNodeLike\n * @typedef {import('./dag-node/dagNode')} DAGNode\n * @typedef {import('./dag-link/dagLink')} DAGLink\n */\n\n/**\n * @type {import('./types').DAGNodeFormat}\n */\nconst format = {\n  DAGNode: DAGNodeClass,\n  DAGLink: DAGLinkClass,\n\n  /**\n   * Functions to fulfil IPLD Format interface\n   * https://github.com/ipld/interface-ipld-format\n   */\n  resolver,\n  util,\n  codec: util.codec,\n  defaultHashAlg: util.defaultHashAlg\n}\n\nmodule.exports = format\n","'use strict'\n\nconst CID = require('cids')\n\nconst util = require('./util')\n\n/**\n * Resolves a path within a PB block.\n *\n * If the path resolves half-way to a link, then the `remainderPath` is the part\n * after the link that can be used for further resolving\n *\n * Returns the value or a link and the partial missing path. This way the\n * IPLD Resolver can fetch the link and continue to resolve.\n *\n * @param {Uint8Array} binaryBlob - Binary representation of a PB block\n * @param {string} [path='/'] - Path that should be resolved\n */\nexports.resolve = (binaryBlob, path = '/') => {\n  let node = util.deserialize(binaryBlob)\n\n  const parts = path.split('/').filter(Boolean)\n  while (parts.length) {\n    const key = parts.shift()\n    // @ts-ignore\n    if (node[key] === undefined) {\n      // There might be a matching named link\n      for (const link of node.Links) {\n        if (link.Name === key) {\n          return {\n            value: link.Hash,\n            remainderPath: parts.join('/')\n          }\n        }\n      }\n\n      // There wasn't even a matching named link\n      throw new Error(`Object has no property '${key}'`)\n    }\n\n    // @ts-ignore\n    node = node[key]\n    if (CID.isCID(node)) {\n      return {\n        value: node,\n        remainderPath: parts.join('/')\n      }\n    }\n  }\n\n  return {\n    value: node,\n    remainderPath: ''\n  }\n}\n\n/**\n * Return all available paths of a block.\n *\n * @generator\n * @param {Uint8Array} binaryBlob - Binary representation of a PB block\n * @yields {string} - A single path\n */\nexports.tree = function * (binaryBlob) {\n  const node = util.deserialize(binaryBlob)\n\n  // There is always a `Data` and `Links` property\n  yield 'Data'\n  yield 'Links'\n  for (let ii = 0; ii < node.Links.length; ii++) {\n    yield `Links/${ii}`\n    yield `Links/${ii}/Name`\n    yield `Links/${ii}/Tsize`\n    yield `Links/${ii}/Hash`\n  }\n}\n","'use strict'\n\n/**\n * Can be used with Array.sort to sort and array with Uint8Array entries\n *\n * @param {Uint8Array} a\n * @param {Uint8Array} b\n */\nfunction compare (a, b) {\n  for (let i = 0; i < a.byteLength; i++) {\n    if (a[i] < b[i]) {\n      return -1\n    }\n\n    if (a[i] > b[i]) {\n      return 1\n    }\n  }\n\n  if (a.byteLength > b.byteLength) {\n    return 1\n  }\n\n  if (a.byteLength < b.byteLength) {\n    return -1\n  }\n\n  return 0\n}\n\nmodule.exports = compare\n","'use strict'\n\nconst DAGLink = require('../dag-link/dagLink')\nconst genCid = require('../genCid')\n\n/**\n * toDAGLink converts a DAGNode to a DAGLink\n *\n * @typedef {import('../genCid').GenCIDOptions} GenCIDOptions\n *\n * @typedef {object} ToDagLinkExtraOptions\n * @property {string} [name]\n *\n * @typedef {GenCIDOptions & ToDagLinkExtraOptions} ToDagLinkOptions\n *\n * @param {import('./dagNode')} node\n * @param {ToDagLinkOptions} options\n */\nconst toDAGLink = async (node, options = {}) => {\n  const buf = node.serialize()\n  const nodeCid = await genCid.cid(buf, options)\n  return new DAGLink(options.name || '', node.size, nodeCid)\n}\n\nmodule.exports = toDAGLink\n","'use strict'\n\nconst sortLinks = require('./sortLinks')\nconst DAGLink = require('../dag-link/dagLink')\n\n/**\n * @typedef {import('./dagNode')} DAGNode\n * @typedef {import('../types')} DAGLinkLike\n */\n\n/**\n * @param {*} link\n * @returns {DAGLink}\n */\nconst asDAGLink = (link) => {\n  if (link instanceof DAGLink) {\n    // It's a DAGLink instance\n    // no need to do anything\n    return link\n  }\n\n  // DAGNode.isDagNode() would be more appropriate here, but it can't be used\n  // as it would lead to circular dependencies as `addLink` is called from\n  // within the DAGNode object.\n  if (!('cid' in link ||\n        'hash' in link ||\n        'Hash' in link ||\n        'multihash' in link)) {\n    throw new Error('Link must be a DAGLink or DAGLink-like. Convert the DAGNode into a DAGLink via `node.toDAGLink()`.')\n  }\n\n  // It's a Object with name, multihash/hash/cid and size\n  // @ts-ignore\n  return new DAGLink(link.Name || link.name, link.Tsize || link.size, link.Hash || link.multihash || link.hash || link.cid)\n}\n\n/**\n * @param {DAGNode} node\n * @param {DAGLink | DAGLinkLike} link\n */\nconst addLink = (node, link) => {\n  const dagLink = asDAGLink(link)\n  node.Links.push(dagLink)\n  sortLinks(node.Links)\n}\n\nmodule.exports = addLink\n","'use strict'\n\nconst CID = require('cids')\nconst uint8ArrayEquals = require('uint8arrays/equals')\n\n/**\n * @typedef {import('../dag-link/dagLink')} DAGLink\n */\n\n/**\n *\n * @param {import('./dagNode')} dagNode\n * @param {string | CID | Uint8Array | DAGLink} nameOrCid\n */\nconst rmLink = (dagNode, nameOrCid) => {\n  let predicate = null\n\n  // It's a name\n  if (typeof nameOrCid === 'string') {\n    predicate = (/** @type {DAGLink} */ link) => link.Name === nameOrCid\n  } else if (nameOrCid instanceof Uint8Array) {\n    predicate = (/** @type {DAGLink} */ link) => uint8ArrayEquals(link.Hash.bytes, nameOrCid)\n  } else if (CID.isCID(nameOrCid)) {\n    predicate = (/** @type {DAGLink} */ link) => uint8ArrayEquals(link.Hash.bytes, nameOrCid.bytes)\n  }\n\n  if (predicate) {\n    const links = dagNode.Links\n    let index = 0\n    while (index < links.length) {\n      const link = links[index]\n      if (predicate(link)) {\n        links.splice(index, 1)\n      } else {\n        index++\n      }\n    }\n  } else {\n    throw new Error('second arg needs to be a name or CID')\n  }\n}\n\nmodule.exports = rmLink\n","'use strict'\nconst CID = require('cids')\nconst multihashing = require('multihashing-async')\nconst { multihash } = multihashing\nconst multicodec = require('multicodec')\n\n/**\n * @typedef {import('cids').CIDVersion} CIDVersion\n * @typedef {import('multihashing-async').multihash.HashCode} HashCode\n * @typedef {import('interface-ipld-format').Format<Uint8Array>} RawFormat\n */\n\n/**\n * Binary resolver\n *\n * @type {RawFormat}\n */\nmodule.exports = {\n  codec: multicodec.RAW,\n  defaultHashAlg: multihash.names['sha2-256'],\n  resolver: {\n    /**\n     * Resolves a path within a Raw block.\n     *\n     * Always returns the raw data as value without any remainderPath.\n     *\n     * @param {Uint8Array} binaryBlob - Binary representation of a PB block\n     * @param {string} [path='/'] - Path that should be resolved.  Must be '/' or an exception is thrown\n     */\n    resolve: (binaryBlob, path) => {\n      if (path !== '/') {\n        throw new Error('Only the root path / may be resolved')\n      }\n\n      return {\n        value: binaryBlob,\n        remainderPath: ''\n      }\n    },\n    /**\n     * Return all available paths of a block.\n     *\n     * @generator\n     * @param {Uint8Array} binaryBlob - The raw data\n     */\n    async * tree (binaryBlob) {\n\n    }\n  },\n  util: {\n    /**\n     * @param {Uint8Array} data\n     */\n    deserialize: (data) => {\n      return data\n    },\n    /**\n     * @param {Uint8Array} data\n     */\n    serialize: (data) => {\n      return data\n    },\n    /**\n     * Calculate the CID of the binary blob.\n     *\n     * @param {Uint8Array} binaryBlob - Encoded IPLD Node\n     * @param {Object} [userOptions] - Options to create the CID\n     * @param {CIDVersion} [userOptions.cidVersion=1] - CID version number\n     * @param {HashCode} [userOptions.hashAlg=multihash.names['sha2-256']] - Defaults to the defaultHashAlg of the format\n     */\n    cid: async (binaryBlob, userOptions = {}) => {\n      const options = {\n        cidVersion: userOptions.cidVersion == null ? 1 : userOptions.cidVersion,\n        hashAlg: userOptions.hashAlg == null ? module.exports.defaultHashAlg : userOptions.hashAlg\n      }\n\n      const hashName = multihash.codes[options.hashAlg]\n      const hash = await multihashing(binaryBlob, hashName)\n      const codecName = multicodec.getNameFromCode(module.exports.codec)\n      const cid = new CID(options.cidVersion, codecName, hash)\n\n      return cid\n    }\n  }\n}\n","'use strict'\n\nconst errcode = require('err-code')\nconst multihash = require('multihashes')\nconst crypto = require('./crypto')\nconst equals = require('uint8arrays/equals')\n\n/**\n * @typedef {import(\"./types\").Digest} Digest\n * @typedef {import(\"multihashes\").HashName} HashName\n */\n\n/**\n * Hash the given `bytes` using the algorithm specified by `alg`.\n *\n * @param {Uint8Array} bytes - The value to hash.\n * @param {HashName} alg - The algorithm to use eg 'sha1'\n * @param {number} [length] - Optionally trim the result to this length.\n * @returns {Promise<Uint8Array>}\n */\nasync function Multihashing (bytes, alg, length) {\n  const digest = await Multihashing.digest(bytes, alg, length)\n  return multihash.encode(digest, alg, length)\n}\n\n/**\n * Expose multihash itself, to avoid silly double requires.\n */\nMultihashing.multihash = multihash\n\n/**\n * @param {Uint8Array} bytes - The value to hash.\n * @param {HashName} alg - The algorithm to use eg 'sha1'\n * @param {number} [length] - Optionally trim the result to this length.\n * @returns {Promise<Uint8Array>}\n */\nMultihashing.digest = async (bytes, alg, length) => {\n  const hash = Multihashing.createHash(alg)\n  const digest = await hash(bytes)\n  return length ? digest.slice(0, length) : digest\n}\n\n/**\n * Creates a function that hashes with the given algorithm\n *\n * @param {HashName} alg - The algorithm to use eg 'sha1'\n * @returns {Digest} - The hash function corresponding to `alg`\n */\nMultihashing.createHash = function (alg) {\n  if (!alg) {\n    const e = errcode(new Error('hash algorithm must be specified'), 'ERR_HASH_ALGORITHM_NOT_SPECIFIED')\n    throw e\n  }\n\n  const code = multihash.coerceCode(alg)\n  if (!Multihashing.functions[code]) {\n    throw errcode(new Error(`multihash function '${alg}' not yet supported`), 'ERR_HASH_ALGORITHM_NOT_SUPPORTED')\n  }\n\n  return Multihashing.functions[code]\n}\n\n/**\n * Mapping of multihash codes to their hashing functions.\n *\n * @type {Record<number, Digest>}\n */\n// @ts-ignore - most of those functions aren't typed\nMultihashing.functions = {\n  // identity\n  0x00: crypto.identity,\n  // sha1\n  0x11: crypto.sha1,\n  // sha2-256\n  0x12: crypto.sha2256,\n  // sha2-512\n  0x13: crypto.sha2512,\n  // sha3-512\n  0x14: crypto.sha3512,\n  // sha3-384\n  0x15: crypto.sha3384,\n  // sha3-256\n  0x16: crypto.sha3256,\n  // sha3-224\n  0x17: crypto.sha3224,\n  // shake-128\n  0x18: crypto.shake128,\n  // shake-256\n  0x19: crypto.shake256,\n  // keccak-224\n  0x1A: crypto.keccak224,\n  // keccak-256\n  0x1B: crypto.keccak256,\n  // keccak-384\n  0x1C: crypto.keccak384,\n  // keccak-512\n  0x1D: crypto.keccak512,\n  // murmur3-128\n  0x22: crypto.murmur3128,\n  // murmur3-32\n  0x23: crypto.murmur332,\n  // dbl-sha2-256\n  0x56: crypto.dblSha2256\n}\n\n// add blake functions\ncrypto.addBlake(Multihashing.functions)\n\n/**\n * @param {Uint8Array} bytes\n * @param {Uint8Array} hash\n * @returns {Promise<boolean>}\n */\nMultihashing.validate = async (bytes, hash) => {\n  const newHash = await Multihashing(bytes, multihash.decode(hash).name)\n\n  return equals(hash, newHash)\n}\n\nmodule.exports = Multihashing\n","'use strict';\n\n/**\n * @typedef {{ [key: string]: any }} Extensions\n * @typedef {Error} Err\n * @property {string} message\n */\n\n/**\n *\n * @param {Error} obj\n * @param {Extensions} props\n * @returns {Error & Extensions}\n */\nfunction assign(obj, props) {\n    for (const key in props) {\n        Object.defineProperty(obj, key, {\n            value: props[key],\n            enumerable: true,\n            configurable: true,\n        });\n    }\n\n    return obj;\n}\n\n/**\n *\n * @param {any} err - An Error\n * @param {string|Extensions} code - A string code or props to set on the error\n * @param {Extensions} [props] - Props to set on the error\n * @returns {Error & Extensions}\n */\nfunction createError(err, code, props) {\n    if (!err || typeof err === 'string') {\n        throw new TypeError('Please pass an Error to err-code');\n    }\n\n    if (!props) {\n        props = {};\n    }\n\n    if (typeof code === 'object') {\n        props = code;\n        code = '';\n    }\n\n    if (code) {\n        props.code = code;\n    }\n\n    try {\n        return assign(err, props);\n    } catch (_) {\n        props.message = err.message;\n        props.stack = err.stack;\n\n        const ErrClass = function () {};\n\n        ErrClass.prototype = Object.create(Object.getPrototypeOf(err));\n\n        // @ts-ignore\n        const output = assign(new ErrClass(), props);\n\n        return output;\n    }\n}\n\nmodule.exports = createError;\n","'use strict'\n\nconst baseX = require('@multiformats/base-x')\nconst Base = require('./base.js')\nconst { rfc4648 } = re